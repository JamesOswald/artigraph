{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"artigraph Declarative Data Production Artigraph is a tool to improve the authorship, management, and quality of data. It emphasizes that the core deliverable of a data pipeline or workflow is the data, not the tasks. Artigraph is hosted by the LF AI and Data Foundation as a Sandbox project. Installation Artigraph can be installed from PyPI on python 3.9+ with pip install arti . Example This sample from the spend example highlights computing the total amount spent from a series of purchase transactions: from pathlib import Path from typing import Annotated from arti import Annotation , Artifact , Graph , producer from arti.formats.json import JSON from arti.storage.local import LocalFile from arti.types import Collection , Date , Float64 , Int64 , Struct from arti.versions import SemVer DIR = Path ( __file__ ) . parent class Vendor ( Annotation ): name : str class Transactions ( Artifact ): \"\"\"Transactions partitioned by day.\"\"\" type = Collection ( element = Struct ( fields = { \"id\" : Int64 (), \"date\" : Date (), \"amount\" : Float64 ()}), partition_by = ( \"date\" ,), ) class TotalSpend ( Artifact ): \"\"\"Aggregate spend over all time.\"\"\" type = Float64 () format = JSON () storage = LocalFile () @producer ( version = SemVer ( major = 1 , minor = 0 , patch = 0 )) def aggregate_transactions ( transactions : Annotated [ list [ dict ], Transactions ] ) -> Annotated [ float , TotalSpend ]: return sum ( txn [ \"amount\" ] for txn in transactions ) with Graph ( name = \"test-graph\" ) as g : g . artifacts . vendor . transactions = Transactions ( annotations = [ Vendor ( name = \"Acme\" )], format = JSON (), storage = LocalFile ( path = str ( DIR / \"transactions\" / \" {date.iso} .json\" )), ) g . artifacts . spend = aggregate_transactions ( transactions = g . artifacts . vendor . transactions ) The full example can be run easily with docker run --rm artigraph/example-spend : INFO : root : Writing mock Transactions data : INFO : root : /usr/src/app/transactions/ 2021 - 10 - 01 . json : [{ 'id' : 1 , 'amount' : 9.95 }, { 'id' : 2 , 'amount' : 7.5 }] INFO : root : /usr/src/app/transactions/ 2021 - 10 - 02 . json : [{ 'id' : 3 , 'amount' : 5.0 }, { 'id' : 4 , 'amount' : 12.0 }, { 'id' : 4 , 'amount' : 7.55 }] INFO : root : Building aggregate_transactions ( transactions = Transactions ( format = JSON (), storage = LocalFile ( path = '/usr/src/app/transactions/{date.iso}.json' ), annotations =( Vendor ( name = 'Acme' ),)))... INFO : root : Build finished . INFO : root : Final Spend data : INFO : root : /tmp/test-graph/spend/7564053533177891797/s pend . json : 42.0 Community Everyone is welcome to join the community - learn more in out support and contributing pages! Presentations 2022-01-27: Requesting Sandbox Incubation with LF AI & Data ( deck , presentation )","title":"Home"},{"location":"#artigraph","text":"Declarative Data Production Artigraph is a tool to improve the authorship, management, and quality of data. It emphasizes that the core deliverable of a data pipeline or workflow is the data, not the tasks. Artigraph is hosted by the LF AI and Data Foundation as a Sandbox project.","title":"artigraph"},{"location":"#installation","text":"Artigraph can be installed from PyPI on python 3.9+ with pip install arti .","title":"Installation"},{"location":"#example","text":"This sample from the spend example highlights computing the total amount spent from a series of purchase transactions: from pathlib import Path from typing import Annotated from arti import Annotation , Artifact , Graph , producer from arti.formats.json import JSON from arti.storage.local import LocalFile from arti.types import Collection , Date , Float64 , Int64 , Struct from arti.versions import SemVer DIR = Path ( __file__ ) . parent class Vendor ( Annotation ): name : str class Transactions ( Artifact ): \"\"\"Transactions partitioned by day.\"\"\" type = Collection ( element = Struct ( fields = { \"id\" : Int64 (), \"date\" : Date (), \"amount\" : Float64 ()}), partition_by = ( \"date\" ,), ) class TotalSpend ( Artifact ): \"\"\"Aggregate spend over all time.\"\"\" type = Float64 () format = JSON () storage = LocalFile () @producer ( version = SemVer ( major = 1 , minor = 0 , patch = 0 )) def aggregate_transactions ( transactions : Annotated [ list [ dict ], Transactions ] ) -> Annotated [ float , TotalSpend ]: return sum ( txn [ \"amount\" ] for txn in transactions ) with Graph ( name = \"test-graph\" ) as g : g . artifacts . vendor . transactions = Transactions ( annotations = [ Vendor ( name = \"Acme\" )], format = JSON (), storage = LocalFile ( path = str ( DIR / \"transactions\" / \" {date.iso} .json\" )), ) g . artifacts . spend = aggregate_transactions ( transactions = g . artifacts . vendor . transactions ) The full example can be run easily with docker run --rm artigraph/example-spend : INFO : root : Writing mock Transactions data : INFO : root : /usr/src/app/transactions/ 2021 - 10 - 01 . json : [{ 'id' : 1 , 'amount' : 9.95 }, { 'id' : 2 , 'amount' : 7.5 }] INFO : root : /usr/src/app/transactions/ 2021 - 10 - 02 . json : [{ 'id' : 3 , 'amount' : 5.0 }, { 'id' : 4 , 'amount' : 12.0 }, { 'id' : 4 , 'amount' : 7.55 }] INFO : root : Building aggregate_transactions ( transactions = Transactions ( format = JSON (), storage = LocalFile ( path = '/usr/src/app/transactions/{date.iso}.json' ), annotations =( Vendor ( name = 'Acme' ),)))... INFO : root : Build finished . INFO : root : Final Spend data : INFO : root : /tmp/test-graph/spend/7564053533177891797/s pend . json : 42.0","title":"Example"},{"location":"#community","text":"Everyone is welcome to join the community - learn more in out support and contributing pages!","title":"Community"},{"location":"#presentations","text":"2022-01-27: Requesting Sandbox Incubation with LF AI & Data ( deck , presentation )","title":"Presentations"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] Added Changed Removed v0.0.1 Added Add project skeleton by @JacobHayes in https://github.com/artigraph/artigraph/pull/1 CI Tweaks by @JacobHayes in https://github.com/artigraph/artigraph/pull/6 Apply isort/black formatting in pre-commit (pytest only checks) by @JacobHayes in https://github.com/artigraph/artigraph/pull/13 Add dev tool to automate worktree creation by @JacobHayes in https://github.com/artigraph/artigraph/pull/14 Add start to a set of the core Artigraph interfaces! by @JacobHayes in https://github.com/artigraph/artigraph/pull/5 Add internal typing stubs by @JacobHayes in https://github.com/artigraph/artigraph/pull/26 Add base Fingerprint class by @JacobHayes in https://github.com/artigraph/artigraph/pull/35 Add base Version classes by @JacobHayes in https://github.com/artigraph/artigraph/pull/36 Migrate ancillary classes to Pydantic Models by @JacobHayes in https://github.com/artigraph/artigraph/pull/60 Initial example of Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/58 Introduce python view, pickle format, local storage by @mikss in https://github.com/artigraph/artigraph/pull/62 move read/write outside of View by @mikss in https://github.com/artigraph/artigraph/pull/65 Int as a view, not Python by @mikss in https://github.com/artigraph/artigraph/pull/66 Support registration priority and use for Views by @JacobHayes in https://github.com/artigraph/artigraph/pull/69 Rename non-standard dunder attributes to sunder by @JacobHayes in https://github.com/artigraph/artigraph/pull/70 Prep for pydantic Artifacts/Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/71 Use pydantic for Artifacts by @JacobHayes in https://github.com/artigraph/artigraph/pull/72 Expand Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/74 Use pydantic for Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/73 Add ObjectBox and frozen Type.metadata by @JacobHayes in https://github.com/artigraph/artigraph/pull/81 Add Type.nullable and Enum(Type) by @JacobHayes in https://github.com/artigraph/artigraph/pull/82 Add pydantic and initial sgqlc typesystems by @joycex99 in https://github.com/artigraph/artigraph/pull/75 Replace Type.metadata with TypeSystem hints by @JacobHayes in https://github.com/artigraph/artigraph/pull/90 Support setting sgqlc schema by @JacobHayes in https://github.com/artigraph/artigraph/pull/98 Refactor PartitionKeys and add StoragePartition by @JacobHayes in https://github.com/artigraph/artigraph/pull/99 CI Multiple Python Versions and py 3.10 support by @JacobHayes in https://github.com/artigraph/artigraph/pull/116 Add Graph.build and lots of other changes by @JacobHayes in https://github.com/artigraph/artigraph/pull/133 Fix typo in Collection.__init__ by @bnaul in https://github.com/artigraph/artigraph/pull/143 Open Source by @JacobHayes in https://github.com/artigraph/artigraph/pull/148 Bump pydantic to 1.9+ by @JacobHayes in https://github.com/artigraph/artigraph/pull/153 Changed Removed","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"[Unreleased]"},{"location":"CHANGELOG/#added","text":"","title":"Added"},{"location":"CHANGELOG/#changed","text":"","title":"Changed"},{"location":"CHANGELOG/#removed","text":"","title":"Removed"},{"location":"CHANGELOG/#v001","text":"","title":"v0.0.1"},{"location":"CHANGELOG/#added_1","text":"Add project skeleton by @JacobHayes in https://github.com/artigraph/artigraph/pull/1 CI Tweaks by @JacobHayes in https://github.com/artigraph/artigraph/pull/6 Apply isort/black formatting in pre-commit (pytest only checks) by @JacobHayes in https://github.com/artigraph/artigraph/pull/13 Add dev tool to automate worktree creation by @JacobHayes in https://github.com/artigraph/artigraph/pull/14 Add start to a set of the core Artigraph interfaces! by @JacobHayes in https://github.com/artigraph/artigraph/pull/5 Add internal typing stubs by @JacobHayes in https://github.com/artigraph/artigraph/pull/26 Add base Fingerprint class by @JacobHayes in https://github.com/artigraph/artigraph/pull/35 Add base Version classes by @JacobHayes in https://github.com/artigraph/artigraph/pull/36 Migrate ancillary classes to Pydantic Models by @JacobHayes in https://github.com/artigraph/artigraph/pull/60 Initial example of Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/58 Introduce python view, pickle format, local storage by @mikss in https://github.com/artigraph/artigraph/pull/62 move read/write outside of View by @mikss in https://github.com/artigraph/artigraph/pull/65 Int as a view, not Python by @mikss in https://github.com/artigraph/artigraph/pull/66 Support registration priority and use for Views by @JacobHayes in https://github.com/artigraph/artigraph/pull/69 Rename non-standard dunder attributes to sunder by @JacobHayes in https://github.com/artigraph/artigraph/pull/70 Prep for pydantic Artifacts/Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/71 Use pydantic for Artifacts by @JacobHayes in https://github.com/artigraph/artigraph/pull/72 Expand Python TypeSystem by @mikss in https://github.com/artigraph/artigraph/pull/74 Use pydantic for Producers by @JacobHayes in https://github.com/artigraph/artigraph/pull/73 Add ObjectBox and frozen Type.metadata by @JacobHayes in https://github.com/artigraph/artigraph/pull/81 Add Type.nullable and Enum(Type) by @JacobHayes in https://github.com/artigraph/artigraph/pull/82 Add pydantic and initial sgqlc typesystems by @joycex99 in https://github.com/artigraph/artigraph/pull/75 Replace Type.metadata with TypeSystem hints by @JacobHayes in https://github.com/artigraph/artigraph/pull/90 Support setting sgqlc schema by @JacobHayes in https://github.com/artigraph/artigraph/pull/98 Refactor PartitionKeys and add StoragePartition by @JacobHayes in https://github.com/artigraph/artigraph/pull/99 CI Multiple Python Versions and py 3.10 support by @JacobHayes in https://github.com/artigraph/artigraph/pull/116 Add Graph.build and lots of other changes by @JacobHayes in https://github.com/artigraph/artigraph/pull/133 Fix typo in Collection.__init__ by @bnaul in https://github.com/artigraph/artigraph/pull/143 Open Source by @JacobHayes in https://github.com/artigraph/artigraph/pull/148 Bump pydantic to 1.9+ by @JacobHayes in https://github.com/artigraph/artigraph/pull/153","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"","title":"Changed"},{"location":"CHANGELOG/#removed_1","text":"","title":"Removed"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at artigraph-security@lists.lfaidata.foundation. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at artigraph-security@lists.lfaidata.foundation. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing to Artigraph Thank you for your interest in contributing to Artigraph! This document explains our contribution process and procedures. If you just need help or have a question, refer to our support page . How to Contribute a Bug Fix or Enhancement Contributions can be submitted via Pull Requests to the golden branch and must: be submitted under the Apache 2.0 license. include a Developer Certificate of Origin signoff ( git commit -s ) include tests and documentation match the Coding Style Project committers will review the contribution in a timely manner and advise of any changes needed to merge the request. Coding Style Code is formatted with black and isort . Docstring style is not yet standardized, but they should generally follow PEP257 . Development Workflow The default branch is golden (poking fun at \"golden data\"). The project is managed using poetry . We use pre-commit to automate rapid feedback via git hooks. Environment Setup If you work on macOS, the .envrc script (used by direnv ) in the repo root can automate project and environment setup for both Intel and M1 computers. Run bash .envrc to: - install brew (if necessary) - install useful system packages ( direnv , git , and pyenv ) via the Brewfile - install the correct python version with pyenv - create a virtual environment and install dependencies - install and configure pre-commit After that completes, configure direnv for your shell and run exec $SHELL . With direnv configured, the project's virtual environment will automatically be activated (and python and package versions synced!) upon cd into the repo. If you use another platform or would rather install manually, use poetry directly to manage your virtual environment(s). Contributions supporting direnv for other platforms would be appreciated!","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-artigraph","text":"Thank you for your interest in contributing to Artigraph! This document explains our contribution process and procedures. If you just need help or have a question, refer to our support page .","title":"Contributing to Artigraph"},{"location":"CONTRIBUTING/#how-to-contribute-a-bug-fix-or-enhancement","text":"Contributions can be submitted via Pull Requests to the golden branch and must: be submitted under the Apache 2.0 license. include a Developer Certificate of Origin signoff ( git commit -s ) include tests and documentation match the Coding Style Project committers will review the contribution in a timely manner and advise of any changes needed to merge the request.","title":"How to Contribute a Bug Fix or Enhancement"},{"location":"CONTRIBUTING/#coding-style","text":"Code is formatted with black and isort . Docstring style is not yet standardized, but they should generally follow PEP257 .","title":"Coding Style"},{"location":"CONTRIBUTING/#development-workflow","text":"The default branch is golden (poking fun at \"golden data\"). The project is managed using poetry . We use pre-commit to automate rapid feedback via git hooks.","title":"Development Workflow"},{"location":"CONTRIBUTING/#environment-setup","text":"If you work on macOS, the .envrc script (used by direnv ) in the repo root can automate project and environment setup for both Intel and M1 computers. Run bash .envrc to: - install brew (if necessary) - install useful system packages ( direnv , git , and pyenv ) via the Brewfile - install the correct python version with pyenv - create a virtual environment and install dependencies - install and configure pre-commit After that completes, configure direnv for your shell and run exec $SHELL . With direnv configured, the project's virtual environment will automatically be activated (and python and package versions synced!) upon cd into the repo. If you use another platform or would rather install manually, use poetry directly to manage your virtual environment(s). Contributions supporting direnv for other platforms would be appreciated!","title":"Environment Setup"},{"location":"GOVERNANCE/","text":"Overview This project aims to be governed in a transparent, accessible way for the benefit of the community. All participation in this project is open and not bound to corporate affiliation. Participants are bound to the project's Code of Conduct . Project Roles Contributor The contributor role is the starting role for anyone participating in the project and wishing to contribute code. Process for Becoming a Contributor Review the Contribution Guidelines to ensure your contribution is inline with the project's coding and styling guidelines. Submit your code as a PR with the appropriate DCO signoff Have your submission approved by the committer(s) and merged into the codebase. Committer The committer role enables the contributor to commit code directly to the repository, but also comes with the responsibility of being a responsible leader in the community. The existing committers are: Name GitHub ID Jacob Hayes @JacobHayes Process for Becoming a Committer Show your experience with the codebase through contributions and engagement on the community channels. Request to become a committer. To do this, create a new pull request that adds your name and details to the table above and request existing committers to approve. After the majority of committers approve you, merge in the PR. Be sure to tag whomever is managing the GitHub permissions to update the committers team in GitHub. Committer Responsibilities Triage GitHub issues and perform pull request reviews for other committers and the community. Make sure that ongoing PRs are moving forward at the right pace or closing them. In general continue to be willing to spend at least 25% of ones time working on the project (~1.25 business days per week). When Does a Committer Lose Committer Status If a committer is no longer interested or cannot perform the committer duties listed above, they should volunteer to be moved to emeritus status. In extreme cases this can also occur by a vote of the committers per the voting process below. Lead The project committers will elect a lead (and optionally a co-lead) which will be the primary point of contact for the project and representative to the TAC upon becoming an Active stage project. The lead(s) will be responsible for the overall project health and direction, coordination of activities, and working with other projects and committees as needed for the continuted growth of the project. The current project lead is: @JacobHayes. Release Process Project releases will occur on a scheduled basis as agreed to by the committers. Conflict Resolution and Voting In general, we prefer that technical issues and committer membership are amicably worked out between the persons involved. If a dispute cannot be decided independently, the committers can be called in to decide an issue. If the committers themselves cannot decide an issue, the issue will be resolved by voting. The voting process is a simple majority in which each committer receives one vote. Communication This project, just like all of open source, is a global community. In addition to the Code of Conduct , this project will: Keep all communication on open channels (mailing list, forums, chat). Be respectful of time and language differences between community members (such as scheduling meetings, email/issue responsiveness, etc). Ensure tools are able to be used by community members regardless of their region. If you have concerns about communication challenges for this project, please contact the committers.","title":"Governance"},{"location":"GOVERNANCE/#overview","text":"This project aims to be governed in a transparent, accessible way for the benefit of the community. All participation in this project is open and not bound to corporate affiliation. Participants are bound to the project's Code of Conduct .","title":"Overview"},{"location":"GOVERNANCE/#project-roles","text":"","title":"Project Roles"},{"location":"GOVERNANCE/#contributor","text":"The contributor role is the starting role for anyone participating in the project and wishing to contribute code.","title":"Contributor"},{"location":"GOVERNANCE/#process-for-becoming-a-contributor","text":"Review the Contribution Guidelines to ensure your contribution is inline with the project's coding and styling guidelines. Submit your code as a PR with the appropriate DCO signoff Have your submission approved by the committer(s) and merged into the codebase.","title":"Process for Becoming a Contributor"},{"location":"GOVERNANCE/#committer","text":"The committer role enables the contributor to commit code directly to the repository, but also comes with the responsibility of being a responsible leader in the community. The existing committers are: Name GitHub ID Jacob Hayes @JacobHayes","title":"Committer"},{"location":"GOVERNANCE/#process-for-becoming-a-committer","text":"Show your experience with the codebase through contributions and engagement on the community channels. Request to become a committer. To do this, create a new pull request that adds your name and details to the table above and request existing committers to approve. After the majority of committers approve you, merge in the PR. Be sure to tag whomever is managing the GitHub permissions to update the committers team in GitHub.","title":"Process for Becoming a Committer"},{"location":"GOVERNANCE/#committer-responsibilities","text":"Triage GitHub issues and perform pull request reviews for other committers and the community. Make sure that ongoing PRs are moving forward at the right pace or closing them. In general continue to be willing to spend at least 25% of ones time working on the project (~1.25 business days per week).","title":"Committer Responsibilities"},{"location":"GOVERNANCE/#when-does-a-committer-lose-committer-status","text":"If a committer is no longer interested or cannot perform the committer duties listed above, they should volunteer to be moved to emeritus status. In extreme cases this can also occur by a vote of the committers per the voting process below.","title":"When Does a Committer Lose Committer Status"},{"location":"GOVERNANCE/#lead","text":"The project committers will elect a lead (and optionally a co-lead) which will be the primary point of contact for the project and representative to the TAC upon becoming an Active stage project. The lead(s) will be responsible for the overall project health and direction, coordination of activities, and working with other projects and committees as needed for the continuted growth of the project. The current project lead is: @JacobHayes.","title":"Lead"},{"location":"GOVERNANCE/#release-process","text":"Project releases will occur on a scheduled basis as agreed to by the committers.","title":"Release Process"},{"location":"GOVERNANCE/#conflict-resolution-and-voting","text":"In general, we prefer that technical issues and committer membership are amicably worked out between the persons involved. If a dispute cannot be decided independently, the committers can be called in to decide an issue. If the committers themselves cannot decide an issue, the issue will be resolved by voting. The voting process is a simple majority in which each committer receives one vote.","title":"Conflict Resolution and Voting"},{"location":"GOVERNANCE/#communication","text":"This project, just like all of open source, is a global community. In addition to the Code of Conduct , this project will: Keep all communication on open channels (mailing list, forums, chat). Be respectful of time and language differences between community members (such as scheduling meetings, email/issue responsiveness, etc). Ensure tools are able to be used by community members regardless of their region. If you have concerns about communication challenges for this project, please contact the committers.","title":"Communication"},{"location":"SECURITY/","text":"Security Policy Reporting a Vulnerability Send vulnerability reports to artigraph-security@lists.lfaidata.foundation and a committer will respond soon.","title":"Security"},{"location":"SECURITY/#security-policy","text":"","title":"Security Policy"},{"location":"SECURITY/#reporting-a-vulnerability","text":"Send vulnerability reports to artigraph-security@lists.lfaidata.foundation and a committer will respond soon.","title":"Reporting a Vulnerability"},{"location":"SUPPORT/","text":"Artigraph Support How to Ask for Help If you have trouble installing, building, or using Artigraph, but there's not yet reason to suspect you've encountered a genuine bug, start a Discussions . This is a great place for questions such has \"How do I...\". How to Report a Bug or Request an Enhancement Artigraph manages bug and enhancements via Issues . The issue template will guide you on making an effective report. How to Report a Security Vulnerability If you think you've found a potential vulnerability in Artigraph, follow the steps in the security policy to responsibly disclose it. How to Contribute We'd love to have your contribution - please refer to our contributing page for direction!","title":"Support"},{"location":"SUPPORT/#artigraph-support","text":"","title":"Artigraph Support"},{"location":"SUPPORT/#how-to-ask-for-help","text":"If you have trouble installing, building, or using Artigraph, but there's not yet reason to suspect you've encountered a genuine bug, start a Discussions . This is a great place for questions such has \"How do I...\".","title":"How to Ask for Help"},{"location":"SUPPORT/#how-to-report-a-bug-or-request-an-enhancement","text":"Artigraph manages bug and enhancements via Issues . The issue template will guide you on making an effective report.","title":"How to Report a Bug or Request an Enhancement"},{"location":"SUPPORT/#how-to-report-a-security-vulnerability","text":"If you think you've found a potential vulnerability in Artigraph, follow the steps in the security policy to responsibly disclose it.","title":"How to Report a Security Vulnerability"},{"location":"SUPPORT/#how-to-contribute","text":"We'd love to have your contribution - please refer to our contributing page for direction!","title":"How to Contribute"},{"location":"reference/arti/","text":"Module arti None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import threading from typing import Optional from arti.annotations import Annotation from arti.artifacts import Artifact from arti.backends import Backend from arti.executors import Executor from arti.fingerprints import Fingerprint from arti.formats import Format from arti.graphs import Graph from arti.io import read , register_reader , register_writer , write from arti.partitions import CompositeKey , CompositeKeyTypes , PartitionKey from arti.producers import PartitionDependencies , Producer , producer from arti.statistics import Statistic from arti.storage import InputFingerprints , Storage , StoragePartition , StoragePartitions from arti.thresholds import Threshold from arti.types import Type , TypeAdapter , TypeSystem from arti.versions import Version from arti.views import View # Export all interfaces. __all__ = [ \"Annotation\" , \"Artifact\" , \"Backend\" , \"CompositeKey\" , \"CompositeKeyTypes\" , \"Executor\" , \"Fingerprint\" , \"Format\" , \"Graph\" , \"InputFingerprints\" , \"PartitionDependencies\" , \"PartitionKey\" , \"Producer\" , \"Statistic\" , \"Storage\" , \"StoragePartition\" , \"StoragePartitions\" , \"Threshold\" , \"Type\" , \"TypeAdapter\" , \"TypeSystem\" , \"Version\" , \"View\" , \"producer\" , \"read\" , \"register_reader\" , \"register_writer\" , \"write\" , ] class _Context ( threading . local ): def __init__ ( self ) -> None : super () . __init__ () self . graph : Optional [ Graph ] = None context = _Context () Sub-modules arti.annotations arti.artifacts arti.backends arti.executors arti.fingerprints arti.formats arti.graphs arti.internal arti.io arti.partitions arti.producers arti.statistics arti.storage arti.thresholds arti.types arti.versions arti.views Variables CompositeKey CompositeKeyTypes InputFingerprints PartitionDependencies Functions producer def producer ( * , annotations : Optional [ tuple [ arti . annotations . Annotation , ... ]] = None , map : Optional [ collections . abc . Callable [ ... , arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]]]]] = None , name : Optional [ str ] = None , validate_outputs : Optional [ collections . abc . Callable [ ... , tuple [ bool , str ]]] = None , version : Optional [ arti . versions . Version ] = None ) -> collections . abc . Callable [[ collections . abc . Callable [ ... , typing . Any ]], type [ arti . producers . Producer ]] View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : __annotations__ [ param.name ] = Producer . _get_artifact_from_annotation ( param . annotation ) # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate read def read ( type_ : arti . types . Type , format : arti . formats . Format , storage_partitions : collections . abc . Sequence [ arti . storage . StoragePartition ], view : arti . views . View ) -> Any View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not ( isinstance ( type_ , Collection ) and type_ . is_partitioned ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) _discover () # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view ) register_reader def register_reader ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore register_writer def register_writer ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore write def write ( data : Any , type_ : arti . types . Type , format : arti . formats . Format , storage_partition : ~ _StoragePartition , view : arti . views . View ) -> ~ _StoragePartition View Source def write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> _StoragePartition : _discover () if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition Classes Annotation class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Artifact class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( BaseArtifact ) : \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" _abstract_ = True # The Artifact . _by_type registry is used to track Artifact classes generated from literal python # values . This is populated by Artifact . from_type and used in Producer class validation # to find a default Artifact type for un - Annotated hints ( eg : ` def build ( i : int ) ` ). _by_type : \"ClassVar[dict[Type, type[Artifact]]]\" = {} annotations : tuple [ Annotation, ... ] = () statistics : tuple [ Statistic, ... ] = () @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any, ... ] , field : ModelField ) -> tuple [ Any, ... ] : return tuple ( chain ( cls . __fields__ [ field.name ] . default , value )) @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti . producers import Producer if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma : no cover # TODO : \"side effect\" Producers : https : // github . com / artigraph / artigraph / issues / 11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral from arti . types . python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {} )) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str, Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy ` Producer . out ` use . } cls . _by_type [ type_ ] = type ( f \"{type_.friendly_key}Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults } , ** defaults , } , ) return cls . _by_type [ type_ ] Ancestors (in MRO) arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config is_partitioned partition_key_types Static methods cast def cast ( value : Any ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an Artifact.box (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised View Source @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti.producers import Producer if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values for_literal def for_literal ( value : Any ) -> 'Artifact' View Source @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral from arti.types.python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) from_orm def from_orm ( obj : Any ) -> 'Model' from_type def from_type ( type_ : arti . types . Type ) -> 'type[Artifact]' View Source @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str , Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy `Producer.out` use. } cls . _by_type [ type_ ] = type ( f \" { type_ . friendly_key } Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults }, ** defaults , }, ) return cls . _by_type [ type_ ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format validate_storage def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_storage_partitions def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Backend class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model ) : \" \"\" Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\" \" @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError () # Artifact partitions - independent of a specific Graph (snapshot) @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Artifact partitions for a specific Graph (snapshot) @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) # Graph Snapshot Tagging @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \" \"\" Fetch the Snapshot ID for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \" \"\" Tag a Graph Snapshot ID with an arbitrary name. \"\" \" raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.backends.memory.MemoryBackend Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods connect def connect ( self : ~ _Backend ) -> collections . abc . Iterator [ ~ _Backend ] View Source @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError () copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . read_artifact_partitions def read_artifact_partitions ( self , artifact : arti . artifacts . Artifact , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a Graph snapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError () read_graph_partitions def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact ) -> tuple [ arti . storage . StoragePartition , ... ] Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () read_graph_tag def read_graph_tag ( self , graph_name : str , tag : str ) -> arti . fingerprints . Fingerprint Fetch the Snapshot ID for the named tag. View Source @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \"\"\"Fetch the Snapshot ID for the named tag.\"\"\" raise NotImplementedError () write_artifact_and_graph_partitions def write_artifact_and_graph_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ], graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str ) -> None View Source def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) write_artifact_partitions def write_artifact_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () write_graph_partitions def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Link the Partitions to the named Artifact in a specific Graph snapshot. View Source @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () write_graph_tag def write_graph_tag ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , tag : str , overwrite : bool = False ) -> None Tag a Graph Snapshot ID with an arbitrary name. View Source @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Tag a Graph Snapshot ID with an arbitrary name.\"\"\" raise NotImplementedError () Executor class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.executors.local.LocalExecutor Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods build def build ( self , graph : 'Graph' ) -> 'None' View Source @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError () copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Fingerprint class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values empty def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) from_int def from_int ( x : int , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) from_int64 def from_int64 ( x : arti . internal . utils . int64 , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) from_orm def from_orm ( obj : Any ) -> 'Model' from_string def from_string ( x : str , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) from_uint64 def from_uint64 ( x : arti . internal . utils . uint64 , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) identity def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint is_empty is_identity Methods combine def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Format class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ) : \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" type : Optional [ Type ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_ Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.formats.json.JSON arti.formats.pickle.Pickle Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Graph class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" _fingerprint_excludes_ = frozenset ( [ \"backend\" ] ) name : str backend : Backend = Field ( default_factory = MemoryBackend ) path_tags : frozendict [ str, str ] = frozendict () snapshot_id : Optional [ Fingerprint ] = None # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifacts : ArtifactBox = PrivateAttr ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) def __enter__ ( self ) -> \"Graph\" : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : self . _status = status self . _artifacts = ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] ) self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifacts ( self ) -> ArtifactBox : return self . _artifacts @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn 't happen unless the logic above is faulty. raise ValueError(\"Fingerprint is empty!\") snapshot = self.copy(update={\"snapshot_id\": snapshot_id}) assert snapshot.snapshot_id is not None # mypy # Write the discovered partitions (if not already known) and link to this new snapshot. for key, partitions in known_artifact_partitions.items(): snapshot.backend.write_artifact_and_graph_partitions( snapshot.artifacts[key], partitions, self.name, snapshot.snapshot_id, key ) return snapshot def get_snapshot_id(self) -> Fingerprint: return cast(Fingerprint, self.snapshot().snapshot_id) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def dependencies(self) -> NodeDependencies: artifact_deps = { artifact: ( frozenset({artifact.producer_output.producer}) if artifact.producer_output is not None else frozenset() ) for _, artifact in self.artifacts.walk() } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output.producer: frozenset(producer_output.producer.inputs.values()) for artifact in artifact_deps if (producer_output := artifact.producer_output) is not None } return NodeDependencies(artifact_deps | producer_deps) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producers(self) -> frozenset[Producer]: return frozenset(self.producer_outputs) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producer_outputs(self) -> frozendict[Producer, tuple[Artifact, ...]]: d = defaultdict[Producer, dict[int, Artifact]](dict) for _, artifact in self.artifacts.walk(): if artifact.producer_output is None: continue output = artifact.producer_output d[output.producer][output.position] = artifact return frozendict( (producer, tuple(artifacts_by_position[i] for i in sorted(artifacts_by_position))) for producer, artifacts_by_position in d.items() ) @requires_sealed def tag(self, tag: str, overwrite: bool = False) -> \"Graph\": snapshot = self.snapshot() assert snapshot.snapshot_id is not None snapshot.backend.write_graph_tag(snapshot.name, snapshot.snapshot_id, tag, overwrite) return snapshot @requires_sealed def from_tag(self, tag: str) -> \"Graph\": return self.copy(update={\"snapshot_id\": self.backend.read_graph_tag(self.name, tag)}) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere # else), eg: type ~= view. Doing validation on the data, etc. Should some of this # live on the View? @requires_sealed def read( self, artifact: Artifact, *, annotation: Optional[Any] = None, storage_partitions: Optional[Sequence[StoragePartition]] = None, view: Optional[View] = None, ) -> Any: key = self.artifact_to_key[artifact] if annotation is None and view is None: raise ValueError(\"Either `annotation` or `view` must be passed\") elif annotation is not None and view is not None: raise ValueError(\"Only one of `annotation` or `view` may be passed\") elif annotation is not None: view = View.get_class_for(annotation, validation_type=artifact.type)() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None: with self.backend.connect() as backend: storage_partitions = backend.read_graph_partitions( self.name, self.get_snapshot_id(), key, artifact ) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if self.snapshot_id is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None: view = View.get_class_for(type(data), validation_type=artifact.type)() storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to # transparently batch requests, but that' s not so friendly with the transient # \".connect\" . with self . backend . connect () as backend : backend . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if the id would change : # - If snapshot_id is already set , we 'd link to the wrong snapshot (we guard against # this above) # - If unset, we' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables artifact_to_key artifacts fingerprint Methods build def build ( self , executor : 'Optional[Executor]' = None ) -> 'Graph' View Source @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dependencies def dependencies ( ... ) dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. from_tag def from_tag ( self , tag : str ) -> 'Graph' View Source @requires_sealed def from_tag ( self , tag : str ) -> \"Graph\" : return self . copy ( update = { \"snapshot_id\" : self . backend . read_graph_tag ( self . name , tag ) } ) get_snapshot_id def get_snapshot_id ( self ) -> arti . fingerprints . Fingerprint View Source def get_snapshot_id ( self ) -> Fingerprint : return cast ( Fingerprint , self . snapshot (). snapshot_id ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . producer_outputs def producer_outputs ( ... ) producers def producers ( ... ) read def read ( self , artifact : arti . artifacts . Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ collections . abc . Sequence [ arti . storage . StoragePartition ]] = None , view : Optional [ arti . views . View ] = None ) -> Any View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) elif annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) elif annotation is not None : view = View . get_class_for ( annotation , validation_type = artifact . type )() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : with self . backend . connect () as backend : storage_partitions = backend . read_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) snapshot def snapshot ( self ) -> 'Graph' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = self . copy ( update = { \"snapshot_id\" : snapshot_id } ) assert snapshot . snapshot_id is not None # mypy # Write the discovered partitions ( if not already known ) and link to this new snapshot . for key , partitions in known_artifact_partitions . items () : snapshot . backend . write_artifact_and_graph_partitions ( snapshot . artifacts [ key ] , partitions , self . name , snapshot . snapshot_id , key ) return snapshot tag def tag ( self , tag : str , overwrite : bool = False ) -> 'Graph' View Source @requires_sealed def tag ( self , tag : str , overwrite : bool = False ) -> \"Graph\" : snapshot = self . snapshot () assert snapshot . snapshot_id is not None snapshot . backend . write_graph_tag ( snapshot . name , snapshot . snapshot_id , tag , overwrite ) return snapshot write def write ( self , data : Any , * , artifact : arti . artifacts . Artifact , input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), view : Optional [ arti . views . View ] = None ) -> arti . storage . StoragePartition View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if self . snapshot_id is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None : view = View . get_class_for ( type ( data ), validation_type = artifact . type )() storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to # transparently batch requests , but that 's not so friendly with the transient # \".connect\". with self.backend.connect() as backend: backend.write_artifact_partitions(artifact, (storage_partition,)) # Skip linking this partition to the snapshot if the id would change: # - If snapshot_id is already set, we' d link to the wrong snapshot ( we guard against # this above ) # - If unset , we ' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition ) PartitionKey class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : \"ClassVar[dict[type[Type], type[PartitionKey]]]\" = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = ( set ( cls . default_key_components ) - cls . key_components ) : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty @classmethod def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey Class variables Config key_components Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Producer class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_types_: ClassVar[frozendict[str, type[Artifact]]] _build_sig_: ClassVar[Signature] _build_input_views_: ClassVar[BuildInputViews] _output_metadata_: ClassVar[OutputMetadata] _map_sig_: ClassVar[Signature] _map_input_metadata_: ClassVar[MapInputMetadata] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_types_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_input_views_, cls._output_metadata_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_input_metadata_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _get_artifact_from_annotation(cls, annotation: Any) -> type[Artifact]: # Avoid importing non-interface modules at root from arti.types.python import python_type_system origin, args = get_origin(annotation), get_args(annotation) if origin is not Annotated: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) annotation, *hints = args artifacts = [hint for hint in hints if lenient_issubclass(hint, Artifact)] if len(artifacts) == 0: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) if len(artifacts) > 1: raise ValueError(\"multiple Artifacts set\") return cast(type[Artifact], artifacts[0]) @classmethod def _get_view_from_annotation(cls, annotation: Any, artifact: type[Artifact]) -> type[View]: wrap_msg = f\"{artifact.__name__}\" if artifact.is_partitioned: wrap_msg = f\"partitions of {artifact.__name__}\" with wrap_exc(ValueError, prefix=f\" ({wrap_msg})\"): return View.get_class_for(annotation, validation_type=artifact._type) @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter, type[Artifact]], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_types_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") artifact = cls.__fields__[param.name].outer_type_ yield validator(name, param, artifact) @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> ArtifactViewPair: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): artifact = cls._get_artifact_from_annotation(annotation) return artifact, cls._get_view_from_annotation(annotation, artifact) @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputViews, OutputMetadata]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_input_metadata = BuildInputViews( cls._validate_parameters( build_sig, validator=( lambda name, param, artifact: ( name, cls._get_view_from_annotation(param.annotation, artifact), ) ), ) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") output_metadata = OutputMetadata( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all output Artifacts have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the # future we might be able to extend the dependency metadata to support # heterogeneous names if necessary. artifacts_by_composite_key = defaultdict[CompositeKeyTypes, list[type[Artifact]]](list) for (artifact, _) in output_metadata: artifacts_by_composite_key[artifact.partition_key_types].append(artifact) if len(artifacts_by_composite_key) != 1: raise ValueError(\"all output Artifacts must have the same partitioning scheme\") # TODO: Save off output composite_key_types return build_sig, build_input_metadata, output_metadata @classmethod def _validate_validate_output_sig(cls) -> None: build_output_types = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_types}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_type ) for output_type in build_output_types ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_types ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_types [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputMetadata ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : partitioned_outputs = [ artifact for (artifact, view) in cls._output_metadata_ if artifact.is_partitioned ] # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if partitioned_outputs : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) else : def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : { name : partitions for name , partitions in kwargs . items () }} ) # Narrow the map signature , which is validated below and used at graph build # time ( via cls . _map_input_metadata_ ) to determine what arguments to pass to # map . map . __signature__ = Signature ( # type : ignore [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_types_.items() if name in cls._build_input_views_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) def validate_map_param ( name : str , param : Parameter , artifact : type [ Artifact ] ) -> tuple [ str, type[Artifact ] ]: # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name , artifact map_input_metadata = MapInputMetadata ( cls . _validate_parameters ( map_sig , validator = validate_map_param ) ) return map_sig , map_input_metadata # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_types_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_types_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_outputs def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\" Instance variables fingerprint inputs Methods compute_input_fingerprint def compute_input_fingerprint ( self , dependency_partitions : arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]] ) -> arti . fingerprints . Fingerprint View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . out def out ( self , * outputs : arti . artifacts . Artifact ) -> Union [ arti . artifacts . Artifact , tuple [ arti . artifacts . Artifact , ... ]] Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs Statistic class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( BaseArtifact ): \"\"\"A Statistic is a piece of data derived from an Artifact that can be tracked over time.\"\"\" # TODO: Set format/storage to some \"system default\" that can be used across backends? _abstract_ = True Ancestors (in MRO) arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config is_partitioned partition_key_types Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format validate_storage def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_storage_partitions def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Storage class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( _StorageMixin , Model , Generic [ _StoragePartition ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep storage_partition_type : ClassVar [ type[_StoragePartition ] ] # type : ignore type : Optional [ Type ] = Field ( None , repr = False ) format : Optional [ Format ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> _StoragePartition : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) Ancestors (in MRO) arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Descendants arti.storage.local.LocalFile arti.storage.literal.StringLiteral Class variables Config key_value_sep partition_name_component_sep segment_sep Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ ~ _StoragePartition , ... ] View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError () generate_partition def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve_templates def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) StoragePartition class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( _StorageMixin , Model ) : type : Type = Field ( repr = False ) format : Format = Field ( repr = False ) keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) Ancestors (in MRO) arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_keys def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys Instance variables fingerprint key_types Methods compute_content_fingerprint def compute_content_fingerprint ( self ) -> arti . fingerprints . Fingerprint View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) StoragePartitions class StoragePartitions ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.tuple Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. Threshold class Threshold ( / , * args , ** kwargs ) View Source class Threshold : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError () Methods check def check ( self , value : Any ) -> bool View Source def check ( self , value : Any ) -> bool : raise NotImplementedError () Type class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . TypeAdapter class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError () Descendants arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.pydantic.BaseModelAdapter Class variables key priority Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError () to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError () TypeSystem class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str _adapter_by_key : dict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = dict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> Iterator [ type[TypeAdapter ] ]: return reversed ( sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ))) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . register_adapter def register_adapter ( self , adapter : type [ arti . types . TypeAdapter ] ) -> type [ arti . types . TypeAdapter ] View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) to_artigraph def to_artigraph ( self , type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ]) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" ) to_system def to_system ( self , type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source def to_system ( self , type_ : Type , * , hints : dict [ str , Any ]) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" ) Version class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . View class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : \"ClassVar[dict[type, type[View]]]\" = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ type ] type_system : ClassVar [ TypeSystem ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.views.python.PythonBuiltin Class variables Config priority Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/#module-arti","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import threading from typing import Optional from arti.annotations import Annotation from arti.artifacts import Artifact from arti.backends import Backend from arti.executors import Executor from arti.fingerprints import Fingerprint from arti.formats import Format from arti.graphs import Graph from arti.io import read , register_reader , register_writer , write from arti.partitions import CompositeKey , CompositeKeyTypes , PartitionKey from arti.producers import PartitionDependencies , Producer , producer from arti.statistics import Statistic from arti.storage import InputFingerprints , Storage , StoragePartition , StoragePartitions from arti.thresholds import Threshold from arti.types import Type , TypeAdapter , TypeSystem from arti.versions import Version from arti.views import View # Export all interfaces. __all__ = [ \"Annotation\" , \"Artifact\" , \"Backend\" , \"CompositeKey\" , \"CompositeKeyTypes\" , \"Executor\" , \"Fingerprint\" , \"Format\" , \"Graph\" , \"InputFingerprints\" , \"PartitionDependencies\" , \"PartitionKey\" , \"Producer\" , \"Statistic\" , \"Storage\" , \"StoragePartition\" , \"StoragePartitions\" , \"Threshold\" , \"Type\" , \"TypeAdapter\" , \"TypeSystem\" , \"Version\" , \"View\" , \"producer\" , \"read\" , \"register_reader\" , \"register_writer\" , \"write\" , ] class _Context ( threading . local ): def __init__ ( self ) -> None : super () . __init__ () self . graph : Optional [ Graph ] = None context = _Context ()","title":"Module arti"},{"location":"reference/arti/#sub-modules","text":"arti.annotations arti.artifacts arti.backends arti.executors arti.fingerprints arti.formats arti.graphs arti.internal arti.io arti.partitions arti.producers arti.statistics arti.storage arti.thresholds arti.types arti.versions arti.views","title":"Sub-modules"},{"location":"reference/arti/#variables","text":"CompositeKey CompositeKeyTypes InputFingerprints PartitionDependencies","title":"Variables"},{"location":"reference/arti/#functions","text":"","title":"Functions"},{"location":"reference/arti/#producer","text":"def producer ( * , annotations : Optional [ tuple [ arti . annotations . Annotation , ... ]] = None , map : Optional [ collections . abc . Callable [ ... , arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]]]]] = None , name : Optional [ str ] = None , validate_outputs : Optional [ collections . abc . Callable [ ... , tuple [ bool , str ]]] = None , version : Optional [ arti . versions . Version ] = None ) -> collections . abc . Callable [[ collections . abc . Callable [ ... , typing . Any ]], type [ arti . producers . Producer ]] View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : __annotations__ [ param.name ] = Producer . _get_artifact_from_annotation ( param . annotation ) # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate","title":"producer"},{"location":"reference/arti/#read","text":"def read ( type_ : arti . types . Type , format : arti . formats . Format , storage_partitions : collections . abc . Sequence [ arti . storage . StoragePartition ], view : arti . views . View ) -> Any View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not ( isinstance ( type_ , Collection ) and type_ . is_partitioned ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) _discover () # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view )","title":"read"},{"location":"reference/arti/#register_reader","text":"def register_reader ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore","title":"register_reader"},{"location":"reference/arti/#register_writer","text":"def register_writer ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore","title":"register_writer"},{"location":"reference/arti/#write","text":"def write ( data : Any , type_ : arti . types . Type , format : arti . formats . Format , storage_partition : ~ _StoragePartition , view : arti . views . View ) -> ~ _StoragePartition View Source def write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> _StoragePartition : _discover () if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition","title":"write"},{"location":"reference/arti/#classes","text":"","title":"Classes"},{"location":"reference/arti/#annotation","text":"class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True","title":"Annotation"},{"location":"reference/arti/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods","text":"","title":"Methods"},{"location":"reference/arti/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#artifact","text":"class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( BaseArtifact ) : \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" _abstract_ = True # The Artifact . _by_type registry is used to track Artifact classes generated from literal python # values . This is populated by Artifact . from_type and used in Producer class validation # to find a default Artifact type for un - Annotated hints ( eg : ` def build ( i : int ) ` ). _by_type : \"ClassVar[dict[Type, type[Artifact]]]\" = {} annotations : tuple [ Annotation, ... ] = () statistics : tuple [ Statistic, ... ] = () @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any, ... ] , field : ModelField ) -> tuple [ Any, ... ] : return tuple ( chain ( cls . __fields__ [ field.name ] . default , value )) @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti . producers import Producer if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma : no cover # TODO : \"side effect\" Producers : https : // github . com / artigraph / artigraph / issues / 11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral from arti . types . python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {} )) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str, Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy ` Producer . out ` use . } cls . _by_type [ type_ ] = type ( f \"{type_.friendly_key}Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults } , ** defaults , } , ) return cls . _by_type [ type_ ]","title":"Artifact"},{"location":"reference/arti/#ancestors-in-mro_1","text":"arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_1","text":"Config is_partitioned partition_key_types","title":"Class variables"},{"location":"reference/arti/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/#cast","text":"def cast ( value : Any ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an Artifact.box (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised View Source @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti.producers import Producer if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value )","title":"cast"},{"location":"reference/arti/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#for_literal","text":"def for_literal ( value : Any ) -> 'Artifact' View Source @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral from arti.types.python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"for_literal"},{"location":"reference/arti/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#from_type","text":"def from_type ( type_ : arti . types . Type ) -> 'type[Artifact]' View Source @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str , Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy `Producer.out` use. } cls . _by_type [ type_ ] = type ( f \" { type_ . friendly_key } Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults }, ** defaults , }, ) return cls . _by_type [ type_ ]","title":"from_type"},{"location":"reference/arti/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_format","text":"def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format","title":"validate_format"},{"location":"reference/arti/#validate_storage","text":"def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates ()","title":"validate_storage"},{"location":"reference/arti/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_","title":"validate_type"},{"location":"reference/arti/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#discover_storage_partitions","text":"def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints )","title":"discover_storage_partitions"},{"location":"reference/arti/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#backend","text":"class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model ) : \" \"\" Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\" \" @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError () # Artifact partitions - independent of a specific Graph (snapshot) @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Artifact partitions for a specific Graph (snapshot) @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) # Graph Snapshot Tagging @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \" \"\" Fetch the Snapshot ID for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \" \"\" Tag a Graph Snapshot ID with an arbitrary name. \"\" \" raise NotImplementedError ()","title":"Backend"},{"location":"reference/arti/#ancestors-in-mro_2","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants","text":"arti.backends.memory.MemoryBackend","title":"Descendants"},{"location":"reference/arti/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/#connect","text":"def connect ( self : ~ _Backend ) -> collections . abc . Iterator [ ~ _Backend ] View Source @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError ()","title":"connect"},{"location":"reference/arti/#copy_2","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#read_artifact_partitions","text":"def read_artifact_partitions ( self , artifact : arti . artifacts . Artifact , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a Graph snapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError ()","title":"read_artifact_partitions"},{"location":"reference/arti/#read_graph_partitions","text":"def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact ) -> tuple [ arti . storage . StoragePartition , ... ] Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError ()","title":"read_graph_partitions"},{"location":"reference/arti/#read_graph_tag","text":"def read_graph_tag ( self , graph_name : str , tag : str ) -> arti . fingerprints . Fingerprint Fetch the Snapshot ID for the named tag. View Source @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \"\"\"Fetch the Snapshot ID for the named tag.\"\"\" raise NotImplementedError ()","title":"read_graph_tag"},{"location":"reference/arti/#write_artifact_and_graph_partitions","text":"def write_artifact_and_graph_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ], graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str ) -> None View Source def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions )","title":"write_artifact_and_graph_partitions"},{"location":"reference/arti/#write_artifact_partitions","text":"def write_artifact_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError ()","title":"write_artifact_partitions"},{"location":"reference/arti/#write_graph_partitions","text":"def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Link the Partitions to the named Artifact in a specific Graph snapshot. View Source @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError ()","title":"write_graph_partitions"},{"location":"reference/arti/#write_graph_tag","text":"def write_graph_tag ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , tag : str , overwrite : bool = False ) -> None Tag a Graph Snapshot ID with an arbitrary name. View Source @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Tag a Graph Snapshot ID with an arbitrary name.\"\"\" raise NotImplementedError ()","title":"write_graph_tag"},{"location":"reference/arti/#executor","text":"class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError ()","title":"Executor"},{"location":"reference/arti/#ancestors-in-mro_3","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_1","text":"arti.executors.local.LocalExecutor","title":"Descendants"},{"location":"reference/arti/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_3","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/#build","text":"def build ( self , graph : 'Graph' ) -> 'None' View Source @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError ()","title":"build"},{"location":"reference/arti/#copy_3","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#fingerprint","text":"class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented","title":"Fingerprint"},{"location":"reference/arti/#ancestors-in-mro_4","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_4","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#empty","text":"def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None )","title":"empty"},{"location":"reference/arti/#from_int","text":"def from_int ( x : int , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x ))","title":"from_int"},{"location":"reference/arti/#from_int64","text":"def from_int64 ( x : arti . internal . utils . int64 , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x )","title":"from_int64"},{"location":"reference/arti/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#from_string","text":"def from_string ( x : str , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x )))","title":"from_string"},{"location":"reference/arti/#from_uint64","text":"def from_uint64 ( x : arti . internal . utils . uint64 , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x ))","title":"from_uint64"},{"location":"reference/arti/#identity","text":"def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 ))","title":"identity"},{"location":"reference/arti/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_4","text":"fingerprint is_empty is_identity","title":"Instance variables"},{"location":"reference/arti/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/#combine","text":"def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self )","title":"combine"},{"location":"reference/arti/#copy_4","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#format","text":"class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ) : \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" type : Optional [ Type ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_","title":"Format"},{"location":"reference/arti/#ancestors-in-mro_5","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_2","text":"arti.formats.json.JSON arti.formats.pickle.Pickle","title":"Descendants"},{"location":"reference/arti/#class-variables_5","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_type_1","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_","title":"validate_type"},{"location":"reference/arti/#instance-variables_5","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/#copy_5","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#graph","text":"class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" _fingerprint_excludes_ = frozenset ( [ \"backend\" ] ) name : str backend : Backend = Field ( default_factory = MemoryBackend ) path_tags : frozendict [ str, str ] = frozendict () snapshot_id : Optional [ Fingerprint ] = None # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifacts : ArtifactBox = PrivateAttr ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) def __enter__ ( self ) -> \"Graph\" : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : self . _status = status self . _artifacts = ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] ) self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifacts ( self ) -> ArtifactBox : return self . _artifacts @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn 't happen unless the logic above is faulty. raise ValueError(\"Fingerprint is empty!\") snapshot = self.copy(update={\"snapshot_id\": snapshot_id}) assert snapshot.snapshot_id is not None # mypy # Write the discovered partitions (if not already known) and link to this new snapshot. for key, partitions in known_artifact_partitions.items(): snapshot.backend.write_artifact_and_graph_partitions( snapshot.artifacts[key], partitions, self.name, snapshot.snapshot_id, key ) return snapshot def get_snapshot_id(self) -> Fingerprint: return cast(Fingerprint, self.snapshot().snapshot_id) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def dependencies(self) -> NodeDependencies: artifact_deps = { artifact: ( frozenset({artifact.producer_output.producer}) if artifact.producer_output is not None else frozenset() ) for _, artifact in self.artifacts.walk() } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output.producer: frozenset(producer_output.producer.inputs.values()) for artifact in artifact_deps if (producer_output := artifact.producer_output) is not None } return NodeDependencies(artifact_deps | producer_deps) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producers(self) -> frozenset[Producer]: return frozenset(self.producer_outputs) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producer_outputs(self) -> frozendict[Producer, tuple[Artifact, ...]]: d = defaultdict[Producer, dict[int, Artifact]](dict) for _, artifact in self.artifacts.walk(): if artifact.producer_output is None: continue output = artifact.producer_output d[output.producer][output.position] = artifact return frozendict( (producer, tuple(artifacts_by_position[i] for i in sorted(artifacts_by_position))) for producer, artifacts_by_position in d.items() ) @requires_sealed def tag(self, tag: str, overwrite: bool = False) -> \"Graph\": snapshot = self.snapshot() assert snapshot.snapshot_id is not None snapshot.backend.write_graph_tag(snapshot.name, snapshot.snapshot_id, tag, overwrite) return snapshot @requires_sealed def from_tag(self, tag: str) -> \"Graph\": return self.copy(update={\"snapshot_id\": self.backend.read_graph_tag(self.name, tag)}) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere # else), eg: type ~= view. Doing validation on the data, etc. Should some of this # live on the View? @requires_sealed def read( self, artifact: Artifact, *, annotation: Optional[Any] = None, storage_partitions: Optional[Sequence[StoragePartition]] = None, view: Optional[View] = None, ) -> Any: key = self.artifact_to_key[artifact] if annotation is None and view is None: raise ValueError(\"Either `annotation` or `view` must be passed\") elif annotation is not None and view is not None: raise ValueError(\"Only one of `annotation` or `view` may be passed\") elif annotation is not None: view = View.get_class_for(annotation, validation_type=artifact.type)() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None: with self.backend.connect() as backend: storage_partitions = backend.read_graph_partitions( self.name, self.get_snapshot_id(), key, artifact ) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if self.snapshot_id is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None: view = View.get_class_for(type(data), validation_type=artifact.type)() storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to # transparently batch requests, but that' s not so friendly with the transient # \".connect\" . with self . backend . connect () as backend : backend . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if the id would change : # - If snapshot_id is already set , we 'd link to the wrong snapshot (we guard against # this above) # - If unset, we' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition )","title":"Graph"},{"location":"reference/arti/#ancestors-in-mro_6","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_6","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_6","text":"artifact_to_key artifacts fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/#build_1","text":"def build ( self , executor : 'Optional[Executor]' = None ) -> 'Graph' View Source @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot","title":"build"},{"location":"reference/arti/#copy_6","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dependencies","text":"def dependencies ( ... )","title":"dependencies"},{"location":"reference/arti/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#from_tag","text":"def from_tag ( self , tag : str ) -> 'Graph' View Source @requires_sealed def from_tag ( self , tag : str ) -> \"Graph\" : return self . copy ( update = { \"snapshot_id\" : self . backend . read_graph_tag ( self . name , tag ) } )","title":"from_tag"},{"location":"reference/arti/#get_snapshot_id","text":"def get_snapshot_id ( self ) -> arti . fingerprints . Fingerprint View Source def get_snapshot_id ( self ) -> Fingerprint : return cast ( Fingerprint , self . snapshot (). snapshot_id )","title":"get_snapshot_id"},{"location":"reference/arti/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#producer_outputs","text":"def producer_outputs ( ... )","title":"producer_outputs"},{"location":"reference/arti/#producers","text":"def producers ( ... )","title":"producers"},{"location":"reference/arti/#read_1","text":"def read ( self , artifact : arti . artifacts . Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ collections . abc . Sequence [ arti . storage . StoragePartition ]] = None , view : Optional [ arti . views . View ] = None ) -> Any View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) elif annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) elif annotation is not None : view = View . get_class_for ( annotation , validation_type = artifact . type )() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : with self . backend . connect () as backend : storage_partitions = backend . read_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , )","title":"read"},{"location":"reference/arti/#snapshot","text":"def snapshot ( self ) -> 'Graph' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = self . copy ( update = { \"snapshot_id\" : snapshot_id } ) assert snapshot . snapshot_id is not None # mypy # Write the discovered partitions ( if not already known ) and link to this new snapshot . for key , partitions in known_artifact_partitions . items () : snapshot . backend . write_artifact_and_graph_partitions ( snapshot . artifacts [ key ] , partitions , self . name , snapshot . snapshot_id , key ) return snapshot","title":"snapshot"},{"location":"reference/arti/#tag","text":"def tag ( self , tag : str , overwrite : bool = False ) -> 'Graph' View Source @requires_sealed def tag ( self , tag : str , overwrite : bool = False ) -> \"Graph\" : snapshot = self . snapshot () assert snapshot . snapshot_id is not None snapshot . backend . write_graph_tag ( snapshot . name , snapshot . snapshot_id , tag , overwrite ) return snapshot","title":"tag"},{"location":"reference/arti/#write_1","text":"def write ( self , data : Any , * , artifact : arti . artifacts . Artifact , input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), view : Optional [ arti . views . View ] = None ) -> arti . storage . StoragePartition View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if self . snapshot_id is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None : view = View . get_class_for ( type ( data ), validation_type = artifact . type )() storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to # transparently batch requests , but that 's not so friendly with the transient # \".connect\". with self.backend.connect() as backend: backend.write_artifact_partitions(artifact, (storage_partition,)) # Skip linking this partition to the snapshot if the id would change: # - If snapshot_id is already set, we' d link to the wrong snapshot ( we guard against # this above ) # - If unset , we ' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition )","title":"write"},{"location":"reference/arti/#partitionkey","text":"class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : \"ClassVar[dict[type[Type], type[PartitionKey]]]\" = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = ( set ( cls . default_key_components ) - cls . key_components ) : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty @classmethod def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"PartitionKey"},{"location":"reference/arti/#ancestors-in-mro_7","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_3","text":"arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey","title":"Descendants"},{"location":"reference/arti/#class-variables_7","text":"Config key_components","title":"Class variables"},{"location":"reference/arti/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/#construct_7","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_key_components","text":"def from_key_components ( ** key_components : str ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" )","title":"from_key_components"},{"location":"reference/arti/#from_orm_7","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#get_class_for","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/#parse_file_7","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_7","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_7","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_7","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_7","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#types_from","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/#update_forward_refs_7","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_7","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_7","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/#copy_7","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_7","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_7","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#producer_1","text":"class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_types_: ClassVar[frozendict[str, type[Artifact]]] _build_sig_: ClassVar[Signature] _build_input_views_: ClassVar[BuildInputViews] _output_metadata_: ClassVar[OutputMetadata] _map_sig_: ClassVar[Signature] _map_input_metadata_: ClassVar[MapInputMetadata] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_types_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_input_views_, cls._output_metadata_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_input_metadata_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _get_artifact_from_annotation(cls, annotation: Any) -> type[Artifact]: # Avoid importing non-interface modules at root from arti.types.python import python_type_system origin, args = get_origin(annotation), get_args(annotation) if origin is not Annotated: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) annotation, *hints = args artifacts = [hint for hint in hints if lenient_issubclass(hint, Artifact)] if len(artifacts) == 0: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) if len(artifacts) > 1: raise ValueError(\"multiple Artifacts set\") return cast(type[Artifact], artifacts[0]) @classmethod def _get_view_from_annotation(cls, annotation: Any, artifact: type[Artifact]) -> type[View]: wrap_msg = f\"{artifact.__name__}\" if artifact.is_partitioned: wrap_msg = f\"partitions of {artifact.__name__}\" with wrap_exc(ValueError, prefix=f\" ({wrap_msg})\"): return View.get_class_for(annotation, validation_type=artifact._type) @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter, type[Artifact]], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_types_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") artifact = cls.__fields__[param.name].outer_type_ yield validator(name, param, artifact) @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> ArtifactViewPair: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): artifact = cls._get_artifact_from_annotation(annotation) return artifact, cls._get_view_from_annotation(annotation, artifact) @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputViews, OutputMetadata]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_input_metadata = BuildInputViews( cls._validate_parameters( build_sig, validator=( lambda name, param, artifact: ( name, cls._get_view_from_annotation(param.annotation, artifact), ) ), ) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") output_metadata = OutputMetadata( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all output Artifacts have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the # future we might be able to extend the dependency metadata to support # heterogeneous names if necessary. artifacts_by_composite_key = defaultdict[CompositeKeyTypes, list[type[Artifact]]](list) for (artifact, _) in output_metadata: artifacts_by_composite_key[artifact.partition_key_types].append(artifact) if len(artifacts_by_composite_key) != 1: raise ValueError(\"all output Artifacts must have the same partitioning scheme\") # TODO: Save off output composite_key_types return build_sig, build_input_metadata, output_metadata @classmethod def _validate_validate_output_sig(cls) -> None: build_output_types = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_types}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_type ) for output_type in build_output_types ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_types ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_types [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputMetadata ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : partitioned_outputs = [ artifact for (artifact, view) in cls._output_metadata_ if artifact.is_partitioned ] # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if partitioned_outputs : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) else : def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : { name : partitions for name , partitions in kwargs . items () }} ) # Narrow the map signature , which is validated below and used at graph build # time ( via cls . _map_input_metadata_ ) to determine what arguments to pass to # map . map . __signature__ = Signature ( # type : ignore [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_types_.items() if name in cls._build_input_views_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) def validate_map_param ( name : str , param : Parameter , artifact : type [ Artifact ] ) -> tuple [ str, type[Artifact ] ]: # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name , artifact map_input_metadata = MapInputMetadata ( cls . _validate_parameters ( map_sig , validator = validate_map_param ) ) return map_sig , map_input_metadata # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_types_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_types_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"Producer"},{"location":"reference/arti/#ancestors-in-mro_8","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_8","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/#construct_8","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_8","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_8","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_8","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_8","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_8","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_8","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_8","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_8","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_outputs","text":"def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\"","title":"validate_outputs"},{"location":"reference/arti/#instance-variables_8","text":"fingerprint inputs","title":"Instance variables"},{"location":"reference/arti/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/#compute_input_fingerprint","text":"def compute_input_fingerprint ( self , dependency_partitions : arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]] ) -> arti . fingerprints . Fingerprint View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), )","title":"compute_input_fingerprint"},{"location":"reference/arti/#copy_8","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_8","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_8","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#out","text":"def out ( self , * outputs : arti . artifacts . Artifact ) -> Union [ arti . artifacts . Artifact , tuple [ arti . artifacts . Artifact , ... ]] Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"out"},{"location":"reference/arti/#statistic","text":"class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( BaseArtifact ): \"\"\"A Statistic is a piece of data derived from an Artifact that can be tracked over time.\"\"\" # TODO: Set format/storage to some \"system default\" that can be used across backends? _abstract_ = True","title":"Statistic"},{"location":"reference/arti/#ancestors-in-mro_9","text":"arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_9","text":"Config is_partitioned partition_key_types","title":"Class variables"},{"location":"reference/arti/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/#construct_9","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_9","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_9","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_9","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_9","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_9","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_9","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_9","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_9","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_format_1","text":"def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format","title":"validate_format"},{"location":"reference/arti/#validate_storage_1","text":"def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates ()","title":"validate_storage"},{"location":"reference/arti/#validate_type_2","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_","title":"validate_type"},{"location":"reference/arti/#instance-variables_9","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_9","text":"","title":"Methods"},{"location":"reference/arti/#copy_9","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_9","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#discover_storage_partitions_1","text":"def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints )","title":"discover_storage_partitions"},{"location":"reference/arti/#json_9","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#storage","text":"class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( _StorageMixin , Model , Generic [ _StoragePartition ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep storage_partition_type : ClassVar [ type[_StoragePartition ] ] # type : ignore type : Optional [ Type ] = Field ( None , repr = False ) format : Optional [ Format ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> _StoragePartition : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"Storage"},{"location":"reference/arti/#ancestors-in-mro_10","text":"arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_4","text":"arti.storage.local.LocalFile arti.storage.literal.StringLiteral","title":"Descendants"},{"location":"reference/arti/#class-variables_10","text":"Config key_value_sep partition_name_component_sep segment_sep","title":"Class variables"},{"location":"reference/arti/#static-methods_10","text":"","title":"Static methods"},{"location":"reference/arti/#construct_10","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_10","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_10","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_10","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_10","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_10","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_10","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_10","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_10","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_format_2","text":"def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format","title":"validate_format"},{"location":"reference/arti/#validate_type_3","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_","title":"validate_type"},{"location":"reference/arti/#instance-variables_10","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/#methods_10","text":"","title":"Methods"},{"location":"reference/arti/#copy_10","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_10","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ ~ _StoragePartition , ... ] View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError ()","title":"discover_partitions"},{"location":"reference/arti/#generate_partition","text":"def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/#json_10","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#resolve_templates","text":"def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"resolve_templates"},{"location":"reference/arti/#storagepartition","text":"class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( _StorageMixin , Model ) : type : Type = Field ( repr = False ) format : Format = Field ( repr = False ) keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"StoragePartition"},{"location":"reference/arti/#ancestors-in-mro_11","text":"arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_5","text":"arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition","title":"Descendants"},{"location":"reference/arti/#class-variables_11","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_11","text":"","title":"Static methods"},{"location":"reference/arti/#construct_11","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_11","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_11","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_11","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_11","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_11","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_11","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_11","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_11","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#validate_keys","text":"def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys","title":"validate_keys"},{"location":"reference/arti/#instance-variables_11","text":"fingerprint key_types","title":"Instance variables"},{"location":"reference/arti/#methods_11","text":"","title":"Methods"},{"location":"reference/arti/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self ) -> arti . fingerprints . Fingerprint View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"compute_content_fingerprint"},{"location":"reference/arti/#copy_11","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_11","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_11","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#with_content_fingerprint","text":"def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/#storagepartitions","text":"class StoragePartitions ( / , * args , ** kwargs )","title":"StoragePartitions"},{"location":"reference/arti/#ancestors-in-mro_12","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/arti/#methods_12","text":"","title":"Methods"},{"location":"reference/arti/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/arti/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/arti/#threshold","text":"class Threshold ( / , * args , ** kwargs ) View Source class Threshold : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Threshold"},{"location":"reference/arti/#methods_13","text":"","title":"Methods"},{"location":"reference/arti/#check","text":"def check ( self , value : Any ) -> bool View Source def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"check"},{"location":"reference/arti/#type","text":"class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_","title":"Type"},{"location":"reference/arti/#ancestors-in-mro_13","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_6","text":"arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp","title":"Descendants"},{"location":"reference/arti/#class-variables_12","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_12","text":"","title":"Static methods"},{"location":"reference/arti/#construct_12","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_12","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_12","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_12","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_12","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_12","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_12","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_12","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_12","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_12","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/#methods_14","text":"","title":"Methods"},{"location":"reference/arti/#copy_12","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_12","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_12","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#typeadapter","text":"class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError ()","title":"TypeAdapter"},{"location":"reference/arti/#descendants_7","text":"arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.pydantic.BaseModelAdapter","title":"Descendants"},{"location":"reference/arti/#class-variables_13","text":"key priority","title":"Class variables"},{"location":"reference/arti/#static-methods_13","text":"","title":"Static methods"},{"location":"reference/arti/#matches_artigraph","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/#matches_system","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError ()","title":"matches_system"},{"location":"reference/arti/#to_artigraph","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError ()","title":"to_artigraph"},{"location":"reference/arti/#to_system","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError ()","title":"to_system"},{"location":"reference/arti/#typesystem","text":"class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str _adapter_by_key : dict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = dict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> Iterator [ type[TypeAdapter ] ]: return reversed ( sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ))) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" )","title":"TypeSystem"},{"location":"reference/arti/#ancestors-in-mro_14","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#class-variables_14","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_14","text":"","title":"Static methods"},{"location":"reference/arti/#construct_13","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_13","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_13","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_13","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_13","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_13","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_13","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_13","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_13","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_13","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_15","text":"","title":"Methods"},{"location":"reference/arti/#copy_13","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_13","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_13","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#register_adapter","text":"def register_adapter ( self , adapter : type [ arti . types . TypeAdapter ] ) -> type [ arti . types . TypeAdapter ] View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter )","title":"register_adapter"},{"location":"reference/arti/#to_artigraph_1","text":"def to_artigraph ( self , type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ]) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" )","title":"to_artigraph"},{"location":"reference/arti/#to_system_1","text":"def to_system ( self , type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source def to_system ( self , type_ : Type , * , hints : dict [ str , Any ]) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" )","title":"to_system"},{"location":"reference/arti/#version","text":"class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True","title":"Version"},{"location":"reference/arti/#ancestors-in-mro_15","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_8","text":"arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp","title":"Descendants"},{"location":"reference/arti/#class-variables_15","text":"Config","title":"Class variables"},{"location":"reference/arti/#static-methods_15","text":"","title":"Static methods"},{"location":"reference/arti/#construct_14","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_14","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#parse_file_14","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_14","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_14","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_14","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_14","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_14","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_14","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_14","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_16","text":"","title":"Methods"},{"location":"reference/arti/#copy_14","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_14","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_14","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/#view","text":"class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : \"ClassVar[dict[type, type[View]]]\" = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ type ] type_system : ClassVar [ TypeSystem ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"View"},{"location":"reference/arti/#ancestors-in-mro_16","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/#descendants_9","text":"arti.views.python.PythonBuiltin","title":"Descendants"},{"location":"reference/arti/#class-variables_16","text":"Config priority","title":"Class variables"},{"location":"reference/arti/#static-methods_16","text":"","title":"Static methods"},{"location":"reference/arti/#check_type_similarity","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/#construct_15","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/#from_orm_15","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/#get_class_for_1","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/#parse_file_15","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/#parse_obj_15","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/#parse_raw_15","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/#schema_15","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/#schema_json_15","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/#update_forward_refs_15","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/#validate_15","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/#instance-variables_15","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/#methods_17","text":"","title":"Methods"},{"location":"reference/arti/#copy_15","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/#dict_15","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/#json_15","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/annotations/","text":"Module arti.annotations None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.internal.models import Model class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True Classes Annotation class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Annotations"},{"location":"reference/arti/annotations/#module-artiannotations","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.internal.models import Model class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True","title":"Module arti.annotations"},{"location":"reference/arti/annotations/#classes","text":"","title":"Classes"},{"location":"reference/arti/annotations/#annotation","text":"class Annotation ( __pydantic_self__ , ** data : Any ) View Source class Annotation ( Model ): \"\"\"An Annotation is a piece of human knowledge associated with an Artifact.\"\"\" _abstract_ = True","title":"Annotation"},{"location":"reference/arti/annotations/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/annotations/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/annotations/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/annotations/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/annotations/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/annotations/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/annotations/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/annotations/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/annotations/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/annotations/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/annotations/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/annotations/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/annotations/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/annotations/#methods","text":"","title":"Methods"},{"location":"reference/arti/annotations/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/annotations/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/annotations/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/artifacts/","text":"Module arti.artifacts None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import json from itertools import chain from typing import Any , ClassVar , Optional from pydantic import Field , validator from pydantic.fields import ModelField from arti.annotations import Annotation from arti.formats import Format from arti.internal.models import Model from arti.internal.type_hints import get_annotation_from_value from arti.internal.utils import classproperty from arti.partitions import CompositeKeyTypes , PartitionKey from arti.storage import InputFingerprints , Storage , StoragePartition from arti.types import Type class BaseArtifact ( Model ): \"\"\"A BaseArtifact is the most basic data structure describing data in the Artigraph ecosystem. A BaseArtifact is comprised of three key elements: - type: spec of the data's structure, such as data types, nullable, etc. - format: the data's serialized format, such as CSV, Parquet, database native, etc. - storage: the data's persistent storage system, such as blob storage, database native, etc. \"\"\" # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ([ \"type\" , \"format\" , \"storage\" ]) # Type *must* be set on the class and be rather static - small additions may be necessary at # Graph level (eg: dynamic column additions), but these should be minor. We might allow Struct # Types to be \"open\" (partial type) or \"closed\". # # Format and storage *should* be set with defaults on Artifact subclasses to ease most Graph # definitions, but will often need to be overridden at the Graph level. # # In order to override on the instance, avoid ClassVars lest mypy complains when/if we override. type : Type format : Format storage : Storage [ Any ] # Hide in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : \"Optional[ProducerOutput]\" = Field ( None , repr = False ) # Class level alias for `type`, which must be set on (non-abstract) subclasses. # # Pydantic removes class defaults and stashes them in cls.__fields__. To ease access, we # automatically populate this from `type` in `__init_subclass__`. _type : ClassVar [ Type ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ and cls . __fields__ [ \"type\" ] . default is None : raise ValueError ( f \" { cls . __name__ } must set `type`\" ) cls . _type = cls . __fields__ [ \"type\" ] . default @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str , Any ]) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ]}) return format @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ], values : dict [ str , Any ]) -> Storage [ Any ]: return storage . copy ( update = { name : values [ name ] for name in [ \"type\" , \"format\" ] if name in values } ) . resolve_templates () @classproperty @classmethod def partition_key_types ( cls ) -> CompositeKeyTypes : return PartitionKey . types_from ( cls . _type ) @classproperty @classmethod def is_partitioned ( cls ) -> bool : return bool ( cls . partition_key_types ) def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ... ]: # TODO: Should we support calculating the input fingerprints if not passed? return self . storage . discover_partitions ( input_fingerprints ) class Statistic ( BaseArtifact ): \"\"\"A Statistic is a piece of data derived from an Artifact that can be tracked over time.\"\"\" # TODO: Set format/storage to some \"system default\" that can be used across backends? _abstract_ = True class Artifact ( BaseArtifact ): \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" _abstract_ = True # The Artifact._by_type registry is used to track Artifact classes generated from literal python # values. This is populated by Artifact.from_type and used in Producer class validation # to find a default Artifact type for un-Annotated hints (eg: `def build(i: int)`). _by_type : \"ClassVar[dict[Type, type[Artifact]]]\" = {} annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ], field : ModelField ) -> tuple [ Any , ... ]: return tuple ( chain ( cls . __fields__ [ field . name ] . default , value )) @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti.producers import Producer if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral from arti.types.python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str , Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy `Producer.out` use. } cls . _by_type [ type_ ] = type ( f \" { type_ . friendly_key } Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults }, ** defaults , }, ) return cls . _by_type [ type_ ] from arti.producers import ProducerOutput # noqa: E402 Classes Artifact class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( BaseArtifact ) : \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" _abstract_ = True # The Artifact . _by_type registry is used to track Artifact classes generated from literal python # values . This is populated by Artifact . from_type and used in Producer class validation # to find a default Artifact type for un - Annotated hints ( eg : ` def build ( i : int ) ` ). _by_type : \"ClassVar[dict[Type, type[Artifact]]]\" = {} annotations : tuple [ Annotation, ... ] = () statistics : tuple [ Statistic, ... ] = () @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any, ... ] , field : ModelField ) -> tuple [ Any, ... ] : return tuple ( chain ( cls . __fields__ [ field.name ] . default , value )) @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti . producers import Producer if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma : no cover # TODO : \"side effect\" Producers : https : // github . com / artigraph / artigraph / issues / 11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral from arti . types . python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {} )) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str, Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy ` Producer . out ` use . } cls . _by_type [ type_ ] = type ( f \"{type_.friendly_key}Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults } , ** defaults , } , ) return cls . _by_type [ type_ ] Ancestors (in MRO) arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config is_partitioned partition_key_types Static methods cast def cast ( value : Any ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an Artifact.box (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised View Source @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti.producers import Producer if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values for_literal def for_literal ( value : Any ) -> 'Artifact' View Source @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral from arti.types.python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) from_orm def from_orm ( obj : Any ) -> 'Model' from_type def from_type ( type_ : arti . types . Type ) -> 'type[Artifact]' View Source @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str , Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy `Producer.out` use. } cls . _by_type [ type_ ] = type ( f \" { type_ . friendly_key } Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults }, ** defaults , }, ) return cls . _by_type [ type_ ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format validate_storage def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_storage_partitions def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . BaseArtifact class BaseArtifact ( __pydantic_self__ , ** data : Any ) View Source class BaseArtifact ( Model ) : \"\"\"A BaseArtifact is the most basic data structure describing data in the Artigraph ecosystem. A BaseArtifact is comprised of three key elements: - type: spec of the data's structure, such as data types, nullable, etc. - format: the data's serialized format, such as CSV, Parquet, database native, etc. - storage: the data's persistent storage system, such as blob storage, database native, etc. \"\"\" # NOTE : Narrow the fields that affect the fingerprint to minimize changes ( which trigger # recompute ). Importantly , avoid fingerprinting the ` . producer_output ` ( ie : the * upstream * # producer ) to prevent cascading fingerprint changes ( Producer . fingerprint accesses the * input * # Artifact . fingerprints ). Even so , this may still be quite sensitive . _fingerprint_includes_ = frozenset ( [ \"type\", \"format\", \"storage\" ] ) # Type * must * be set on the class and be rather static - small additions may be necessary at # Graph level ( eg : dynamic column additions ), but these should be minor . We might allow Struct # Types to be \"open\" ( partial type ) or \"closed\" . # # Format and storage * should * be set with defaults on Artifact subclasses to ease most Graph # definitions , but will often need to be overridden at the Graph level . # # In order to override on the instance , avoid ClassVars lest mypy complains when / if we override . type : Type format : Format storage : Storage [ Any ] # Hide in repr to prevent showing the entire upstream graph . # # ProducerOutput is a ForwardRef / cyclic import . Quote the entire hint to force full resolution # during ` . update_forward_refs ` , rather than ` Optional [ ForwardRef(\"ProducerOutput\") ] ` . producer_output : \"Optional[ProducerOutput]\" = Field ( None , repr = False ) # Class level alias for ` type ` , which must be set on ( non - abstract ) subclasses . # # Pydantic removes class defaults and stashes them in cls . __fields__ . To ease access , we # automatically populate this from ` type ` in ` __init_subclass__ ` . _type : ClassVar [ Type ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ and cls . __fields__ [ \"type\" ] . default is None : raise ValueError ( f \"{cls.__name__} must set `type`\" ) cls . _type = cls . __fields__ [ \"type\" ] . default @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE : We do a lot of class level validation ( particularly in Producer ) that relies on # the * class * type , such as partition key validation . It ' s possible we could loosen this # a bit by allowing * new * Struct fields , but still requiring an exact match for other # Types ( including existing Struct fields ). raise ValueError ( \"overriding `type` is not supported\" ) return type_ @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () @classproperty @classmethod def partition_key_types ( cls ) -> CompositeKeyTypes : return PartitionKey . types_from ( cls . _type ) @classproperty @classmethod def is_partitioned ( cls ) -> bool : return bool ( cls . partition_key_types ) def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition, ... ] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.artifacts.Statistic arti.artifacts.Artifact Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format validate_storage def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_storage_partitions def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints ) is_partitioned def is_partitioned ( ... ) Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc). json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . partition_key_types def partition_key_types ( ... ) Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc). Statistic class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( BaseArtifact ): \"\"\"A Statistic is a piece of data derived from an Artifact that can be tracked over time.\"\"\" # TODO: Set format/storage to some \"system default\" that can be used across backends? _abstract_ = True Ancestors (in MRO) arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config is_partitioned partition_key_types Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format validate_storage def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_storage_partitions def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Artifacts"},{"location":"reference/arti/artifacts/#module-artiartifacts","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import json from itertools import chain from typing import Any , ClassVar , Optional from pydantic import Field , validator from pydantic.fields import ModelField from arti.annotations import Annotation from arti.formats import Format from arti.internal.models import Model from arti.internal.type_hints import get_annotation_from_value from arti.internal.utils import classproperty from arti.partitions import CompositeKeyTypes , PartitionKey from arti.storage import InputFingerprints , Storage , StoragePartition from arti.types import Type class BaseArtifact ( Model ): \"\"\"A BaseArtifact is the most basic data structure describing data in the Artigraph ecosystem. A BaseArtifact is comprised of three key elements: - type: spec of the data's structure, such as data types, nullable, etc. - format: the data's serialized format, such as CSV, Parquet, database native, etc. - storage: the data's persistent storage system, such as blob storage, database native, etc. \"\"\" # NOTE: Narrow the fields that affect the fingerprint to minimize changes (which trigger # recompute). Importantly, avoid fingerprinting the `.producer_output` (ie: the *upstream* # producer) to prevent cascading fingerprint changes (Producer.fingerprint accesses the *input* # Artifact.fingerprints). Even so, this may still be quite sensitive. _fingerprint_includes_ = frozenset ([ \"type\" , \"format\" , \"storage\" ]) # Type *must* be set on the class and be rather static - small additions may be necessary at # Graph level (eg: dynamic column additions), but these should be minor. We might allow Struct # Types to be \"open\" (partial type) or \"closed\". # # Format and storage *should* be set with defaults on Artifact subclasses to ease most Graph # definitions, but will often need to be overridden at the Graph level. # # In order to override on the instance, avoid ClassVars lest mypy complains when/if we override. type : Type format : Format storage : Storage [ Any ] # Hide in repr to prevent showing the entire upstream graph. # # ProducerOutput is a ForwardRef/cyclic import. Quote the entire hint to force full resolution # during `.update_forward_refs`, rather than `Optional[ForwardRef(\"ProducerOutput\")]`. producer_output : \"Optional[ProducerOutput]\" = Field ( None , repr = False ) # Class level alias for `type`, which must be set on (non-abstract) subclasses. # # Pydantic removes class defaults and stashes them in cls.__fields__. To ease access, we # automatically populate this from `type` in `__init_subclass__`. _type : ClassVar [ Type ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ and cls . __fields__ [ \"type\" ] . default is None : raise ValueError ( f \" { cls . __name__ } must set `type`\" ) cls . _type = cls . __fields__ [ \"type\" ] . default @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_ @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str , Any ]) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ]}) return format @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ], values : dict [ str , Any ]) -> Storage [ Any ]: return storage . copy ( update = { name : values [ name ] for name in [ \"type\" , \"format\" ] if name in values } ) . resolve_templates () @classproperty @classmethod def partition_key_types ( cls ) -> CompositeKeyTypes : return PartitionKey . types_from ( cls . _type ) @classproperty @classmethod def is_partitioned ( cls ) -> bool : return bool ( cls . partition_key_types ) def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ... ]: # TODO: Should we support calculating the input fingerprints if not passed? return self . storage . discover_partitions ( input_fingerprints ) class Statistic ( BaseArtifact ): \"\"\"A Statistic is a piece of data derived from an Artifact that can be tracked over time.\"\"\" # TODO: Set format/storage to some \"system default\" that can be used across backends? _abstract_ = True class Artifact ( BaseArtifact ): \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" _abstract_ = True # The Artifact._by_type registry is used to track Artifact classes generated from literal python # values. This is populated by Artifact.from_type and used in Producer class validation # to find a default Artifact type for un-Annotated hints (eg: `def build(i: int)`). _by_type : \"ClassVar[dict[Type, type[Artifact]]]\" = {} annotations : tuple [ Annotation , ... ] = () statistics : tuple [ Statistic , ... ] = () @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any , ... ], field : ModelField ) -> tuple [ Any , ... ]: return tuple ( chain ( cls . __fields__ [ field . name ] . default , value )) @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti.producers import Producer if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral from arti.types.python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str , Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy `Producer.out` use. } cls . _by_type [ type_ ] = type ( f \" { type_ . friendly_key } Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults }, ** defaults , }, ) return cls . _by_type [ type_ ] from arti.producers import ProducerOutput # noqa: E402","title":"Module arti.artifacts"},{"location":"reference/arti/artifacts/#classes","text":"","title":"Classes"},{"location":"reference/arti/artifacts/#artifact","text":"class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( BaseArtifact ) : \"\"\"An Artifact is the base structure describing an existing or generated dataset. An Artifact is comprised of three key elements: - `type`: spec of the data's structure, such as data types, nullable, etc. - `format`: the data's serialized format, such as CSV, Parquet, database native, etc. - `storage`: the data's persistent storage system, such as blob storage, database native, etc. In addition to the core elements, an Artifact can be tagged with additional `annotations` (to associate it with human knowledge) and `statistics` (to track derived characteristics over time). \"\"\" _abstract_ = True # The Artifact . _by_type registry is used to track Artifact classes generated from literal python # values . This is populated by Artifact . from_type and used in Producer class validation # to find a default Artifact type for un - Annotated hints ( eg : ` def build ( i : int ) ` ). _by_type : \"ClassVar[dict[Type, type[Artifact]]]\" = {} annotations : tuple [ Annotation, ... ] = () statistics : tuple [ Statistic, ... ] = () @validator ( \"annotations\" , \"statistics\" , always = True , pre = True ) @classmethod def _merge_class_defaults ( cls , value : tuple [ Any, ... ] , field : ModelField ) -> tuple [ Any, ... ] : return tuple ( chain ( cls . __fields__ [ field.name ] . default , value )) @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti . producers import Producer if isinstance ( value , Artifact ) : return value if isinstance ( value , Producer ) : output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ) : return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma : no cover # TODO : \"side effect\" Producers : https : // github . com / artigraph / artigraph / issues / 11 raise ValueError ( f \"{type(value).__name__} doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \"{type(value).__name__} produces {len(output_artifacts)} Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value ) @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral from arti . types . python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {} )) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), ) @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti . formats . json import JSON from arti . storage . literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str, Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy ` Producer . out ` use . } cls . _by_type [ type_ ] = type ( f \"{type_.friendly_key}Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults } , ** defaults , } , ) return cls . _by_type [ type_ ]","title":"Artifact"},{"location":"reference/arti/artifacts/#ancestors-in-mro","text":"arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/artifacts/#class-variables","text":"Config is_partitioned partition_key_types","title":"Class variables"},{"location":"reference/arti/artifacts/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/artifacts/#cast","text":"def cast ( value : Any ) -> 'Artifact' Attempt to convert an arbitrary value to an appropriate Artifact instance. Artifact.cast is used to convert values assigned to an Artifact.box (such as Graph.artifacts ) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised View Source @classmethod def cast ( cls , value : Any ) -> \"Artifact\" : \"\"\"Attempt to convert an arbitrary value to an appropriate Artifact instance. `Artifact.cast` is used to convert values assigned to an `Artifact.box` (such as `Graph.artifacts`) into an Artifact. When called with: - an Artifact instance, it is returned - a Producer instance with a single output Artifact, the output Artifact is returned - a Producer instance with a multiple output Artifacts, an error is raised - other types, an error is raised \"\"\" from arti.producers import Producer if isinstance ( value , Artifact ): return value if isinstance ( value , Producer ): output_artifacts = value . out () if isinstance ( output_artifacts , Artifact ): return output_artifacts n_outputs = len ( output_artifacts ) if n_outputs == 0 : # pragma: no cover # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( f \" { type ( value ) . __name__ } doesn't produce any Artifacts!\" ) assert n_outputs > 1 raise ValueError ( f \" { type ( value ) . __name__ } produces { len ( output_artifacts ) } Artifacts. Try assigning each to a new name in the Graph!\" ) return Artifact . for_literal ( value )","title":"cast"},{"location":"reference/arti/artifacts/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/artifacts/#for_literal","text":"def for_literal ( value : Any ) -> 'Artifact' View Source @classmethod def for_literal ( cls , value : Any ) -> \"Artifact\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral from arti.types.python import python_type_system annotation = get_annotation_from_value ( value ) klass = cls . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) return klass ( format = JSON (), storage = StringLiteral ( value = json . dumps ( value )), )","title":"for_literal"},{"location":"reference/arti/artifacts/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/artifacts/#from_type","text":"def from_type ( type_ : arti . types . Type ) -> 'type[Artifact]' View Source @classmethod def from_type ( cls , type_ : Type ) -> \"type[Artifact]\" : from arti.formats.json import JSON from arti.storage.literal import StringLiteral if type_ not in cls . _by_type : defaults : dict [ str , Any ] = { \"type\" : type_ , \"format\" : JSON (), \"storage\" : StringLiteral (), # Set a default Storage instance to support easy `Producer.out` use. } cls . _by_type [ type_ ] = type ( f \" { type_ . friendly_key } Artifact\" , ( cls ,), { \"__annotations__\" : { # Preserve the looser default type hints field : cls . __fields__ [ field ] . outer_type_ for field in defaults }, ** defaults , }, ) return cls . _by_type [ type_ ]","title":"from_type"},{"location":"reference/arti/artifacts/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/artifacts/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/artifacts/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/artifacts/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/artifacts/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/artifacts/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/artifacts/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/artifacts/#validate_format","text":"def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format","title":"validate_format"},{"location":"reference/arti/artifacts/#validate_storage","text":"def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates ()","title":"validate_storage"},{"location":"reference/arti/artifacts/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_","title":"validate_type"},{"location":"reference/arti/artifacts/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/artifacts/#methods","text":"","title":"Methods"},{"location":"reference/arti/artifacts/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/artifacts/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/artifacts/#discover_storage_partitions","text":"def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints )","title":"discover_storage_partitions"},{"location":"reference/arti/artifacts/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/artifacts/#baseartifact","text":"class BaseArtifact ( __pydantic_self__ , ** data : Any ) View Source class BaseArtifact ( Model ) : \"\"\"A BaseArtifact is the most basic data structure describing data in the Artigraph ecosystem. A BaseArtifact is comprised of three key elements: - type: spec of the data's structure, such as data types, nullable, etc. - format: the data's serialized format, such as CSV, Parquet, database native, etc. - storage: the data's persistent storage system, such as blob storage, database native, etc. \"\"\" # NOTE : Narrow the fields that affect the fingerprint to minimize changes ( which trigger # recompute ). Importantly , avoid fingerprinting the ` . producer_output ` ( ie : the * upstream * # producer ) to prevent cascading fingerprint changes ( Producer . fingerprint accesses the * input * # Artifact . fingerprints ). Even so , this may still be quite sensitive . _fingerprint_includes_ = frozenset ( [ \"type\", \"format\", \"storage\" ] ) # Type * must * be set on the class and be rather static - small additions may be necessary at # Graph level ( eg : dynamic column additions ), but these should be minor . We might allow Struct # Types to be \"open\" ( partial type ) or \"closed\" . # # Format and storage * should * be set with defaults on Artifact subclasses to ease most Graph # definitions , but will often need to be overridden at the Graph level . # # In order to override on the instance , avoid ClassVars lest mypy complains when / if we override . type : Type format : Format storage : Storage [ Any ] # Hide in repr to prevent showing the entire upstream graph . # # ProducerOutput is a ForwardRef / cyclic import . Quote the entire hint to force full resolution # during ` . update_forward_refs ` , rather than ` Optional [ ForwardRef(\"ProducerOutput\") ] ` . producer_output : \"Optional[ProducerOutput]\" = Field ( None , repr = False ) # Class level alias for ` type ` , which must be set on ( non - abstract ) subclasses . # # Pydantic removes class defaults and stashes them in cls . __fields__ . To ease access , we # automatically populate this from ` type ` in ` __init_subclass__ ` . _type : ClassVar [ Type ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ and cls . __fields__ [ \"type\" ] . default is None : raise ValueError ( f \"{cls.__name__} must set `type`\" ) cls . _type = cls . __fields__ [ \"type\" ] . default @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE : We do a lot of class level validation ( particularly in Producer ) that relies on # the * class * type , such as partition key validation . It ' s possible we could loosen this # a bit by allowing * new * Struct fields , but still requiring an exact match for other # Types ( including existing Struct fields ). raise ValueError ( \"overriding `type` is not supported\" ) return type_ @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates () @classproperty @classmethod def partition_key_types ( cls ) -> CompositeKeyTypes : return PartitionKey . types_from ( cls . _type ) @classproperty @classmethod def is_partitioned ( cls ) -> bool : return bool ( cls . partition_key_types ) def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition, ... ] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints )","title":"BaseArtifact"},{"location":"reference/arti/artifacts/#ancestors-in-mro_1","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/artifacts/#descendants","text":"arti.artifacts.Statistic arti.artifacts.Artifact","title":"Descendants"},{"location":"reference/arti/artifacts/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/artifacts/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/artifacts/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/artifacts/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/artifacts/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/artifacts/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/artifacts/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/artifacts/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/artifacts/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/artifacts/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/artifacts/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/artifacts/#validate_format_1","text":"def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format","title":"validate_format"},{"location":"reference/arti/artifacts/#validate_storage_1","text":"def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates ()","title":"validate_storage"},{"location":"reference/arti/artifacts/#validate_type_1","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_","title":"validate_type"},{"location":"reference/arti/artifacts/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/artifacts/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/artifacts/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/artifacts/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/artifacts/#discover_storage_partitions_1","text":"def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints )","title":"discover_storage_partitions"},{"location":"reference/arti/artifacts/#is_partitioned","text":"def is_partitioned ( ... ) Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc).","title":"is_partitioned"},{"location":"reference/arti/artifacts/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/artifacts/#partition_key_types","text":"def partition_key_types ( ... ) Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc).","title":"partition_key_types"},{"location":"reference/arti/artifacts/#statistic","text":"class Statistic ( __pydantic_self__ , ** data : Any ) View Source class Statistic ( BaseArtifact ): \"\"\"A Statistic is a piece of data derived from an Artifact that can be tracked over time.\"\"\" # TODO: Set format/storage to some \"system default\" that can be used across backends? _abstract_ = True","title":"Statistic"},{"location":"reference/arti/artifacts/#ancestors-in-mro_2","text":"arti.artifacts.BaseArtifact arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/artifacts/#class-variables_2","text":"Config is_partitioned partition_key_types","title":"Class variables"},{"location":"reference/arti/artifacts/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/artifacts/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/artifacts/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/artifacts/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/artifacts/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/artifacts/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/artifacts/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/artifacts/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/artifacts/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/artifacts/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/artifacts/#validate_format_2","text":"def validate_format ( format : arti . formats . Format , values : dict [ str , typing . Any ] ) -> arti . formats . Format View Source @validator ( \"format\" , always = True ) @classmethod def validate_format ( cls , format : Format , values : dict [ str, Any ] ) -> Format : if \"type\" in values : return format . copy ( update = { \"type\" : values [ \"type\" ] } ) return format","title":"validate_format"},{"location":"reference/arti/artifacts/#validate_storage_2","text":"def validate_storage ( storage : arti . storage . Storage [ typing . Any ], values : dict [ str , typing . Any ] ) -> arti . storage . Storage [ typing . Any ] View Source @validator ( \"storage\" , always = True ) @classmethod def validate_storage ( cls , storage : Storage [ Any ] , values : dict [ str, Any ] ) -> Storage [ Any ] : return storage . copy ( update = { name : values [ name ] for name in [ \"type\", \"format\" ] if name in values } ). resolve_templates ()","title":"validate_storage"},{"location":"reference/arti/artifacts/#validate_type_2","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" , always = True ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : if type_ != cls . _type : # NOTE: We do a lot of class level validation (particularly in Producer) that relies on # the *class* type, such as partition key validation. It's possible we could loosen this # a bit by allowing *new* Struct fields, but still requiring an exact match for other # Types (including existing Struct fields). raise ValueError ( \"overriding `type` is not supported\" ) return type_","title":"validate_type"},{"location":"reference/arti/artifacts/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/artifacts/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/artifacts/#copy_2","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/artifacts/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/artifacts/#discover_storage_partitions_2","text":"def discover_storage_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] View Source def discover_storage_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StoragePartition , ...] : # TODO : Should we support calculating the input fingerprints if not passed ? return self . storage . discover_partitions ( input_fingerprints )","title":"discover_storage_partitions"},{"location":"reference/arti/artifacts/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/fingerprints/","text":"Module arti.fingerprints None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import operator from collections.abc import Callable from functools import reduce from typing import Optional , Union import farmhash from arti.internal.models import Model from arti.internal.utils import int64 , uint64 def _gen_fingerprint_binop ( op : Callable [[ int , int ], int ] ) -> \"Callable[[Fingerprint, Union[int, Fingerprint]], Fingerprint]\" : def _fingerprint_binop ( self : \"Fingerprint\" , other : \"Union[int, Fingerprint]\" ) -> \"Fingerprint\" : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): if self . key is None or other . key is None : return Fingerprint . empty () return Fingerprint ( key = op ( self . key , other . key )) return NotImplemented return _fingerprint_binop class Fingerprint ( Model ): \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \"special\" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): return self . key == other . key return NotImplemented Classes Fingerprint class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values empty def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) from_int def from_int ( x : int , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) from_int64 def from_int64 ( x : arti . internal . utils . int64 , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) from_orm def from_orm ( obj : Any ) -> 'Model' from_string def from_string ( x : str , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) from_uint64 def from_uint64 ( x : arti . internal . utils . uint64 , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) identity def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint is_empty is_identity Methods combine def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Fingerprints"},{"location":"reference/arti/fingerprints/#module-artifingerprints","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import operator from collections.abc import Callable from functools import reduce from typing import Optional , Union import farmhash from arti.internal.models import Model from arti.internal.utils import int64 , uint64 def _gen_fingerprint_binop ( op : Callable [[ int , int ], int ] ) -> \"Callable[[Fingerprint, Union[int, Fingerprint]], Fingerprint]\" : def _fingerprint_binop ( self : \"Fingerprint\" , other : \"Union[int, Fingerprint]\" ) -> \"Fingerprint\" : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): if self . key is None or other . key is None : return Fingerprint . empty () return Fingerprint ( key = op ( self . key , other . key )) return NotImplemented return _fingerprint_binop class Fingerprint ( Model ): \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \"special\" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ): other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ): return self . key == other . key return NotImplemented","title":"Module arti.fingerprints"},{"location":"reference/arti/fingerprints/#classes","text":"","title":"Classes"},{"location":"reference/arti/fingerprints/#fingerprint","text":"class Fingerprint ( __pydantic_self__ , ** data : Any ) View Source class Fingerprint ( Model ) : \"\"\"Fingerprint represents a unique identity as an int64 value. Using an int(64) has a number of convenient properties: - can be combined independent of order with XOR - can be stored relatively cheaply - empty 0 values drop out when combined (5 ^ 0 = 5) - is relatively cross-platform (across databases, languages, etc) There are two \" special \" Fingerprints w/ factory functions that, when combined with other Fingerprints: - `empty()`: returns `empty()` - `identity()`: return the other Fingerprint \"\"\" key : Optional [ int64 ] def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self ) @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None ) @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x ) @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x ))) @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x )) @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 )) @property def is_empty ( self ) -> bool : return self . key is None @property def is_identity ( self ) -> bool : return self . key == 0 __and__ = _gen_fingerprint_binop ( operator . __and__ ) __lshift__ = _gen_fingerprint_binop ( operator . __lshift__ ) __or__ = _gen_fingerprint_binop ( operator . __or__ ) __rshift__ = _gen_fingerprint_binop ( operator . __rshift__ ) __xor__ = _gen_fingerprint_binop ( operator . __xor__ ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , int ) : other = Fingerprint . from_int ( other ) if isinstance ( other , Fingerprint ) : return self . key == other . key return NotImplemented","title":"Fingerprint"},{"location":"reference/arti/fingerprints/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/fingerprints/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/fingerprints/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/fingerprints/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/fingerprints/#empty","text":"def empty ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return Fingerprint.empty() View Source @classmethod def empty ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return Fingerprint.empty()\"\"\" return cls ( key = None )","title":"empty"},{"location":"reference/arti/fingerprints/#from_int","text":"def from_int ( x : int , / ) -> 'Fingerprint' View Source @classmethod def from_int ( cls , x : int , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x ))","title":"from_int"},{"location":"reference/arti/fingerprints/#from_int64","text":"def from_int64 ( x : arti . internal . utils . int64 , / ) -> 'Fingerprint' View Source @classmethod def from_int64 ( cls , x : int64 , / ) -> \"Fingerprint\" : return cls ( key = x )","title":"from_int64"},{"location":"reference/arti/fingerprints/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/fingerprints/#from_string","text":"def from_string ( x : str , / ) -> 'Fingerprint' Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. View Source @classmethod def from_string ( cls , x : str , / ) -> \"Fingerprint\" : \"\"\"Fingerprint an arbitrary string. Fingerprints using Farmhash Fingerprint64, converted to int64 via two's complement. \"\"\" return cls . from_uint64 ( uint64 ( farmhash . fingerprint64 ( x )))","title":"from_string"},{"location":"reference/arti/fingerprints/#from_uint64","text":"def from_uint64 ( x : arti . internal . utils . uint64 , / ) -> 'Fingerprint' View Source @classmethod def from_uint64 ( cls , x : uint64 , / ) -> \"Fingerprint\" : return cls . from_int64 ( int64 ( x ))","title":"from_uint64"},{"location":"reference/arti/fingerprints/#identity","text":"def identity ( ) -> 'Fingerprint' Return a Fingerprint that, when combined, will return the other Fingerprint. View Source @classmethod def identity ( cls ) -> \"Fingerprint\" : \"\"\"Return a Fingerprint that, when combined, will return the other Fingerprint.\"\"\" return cls ( key = int64 ( 0 ))","title":"identity"},{"location":"reference/arti/fingerprints/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/fingerprints/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/fingerprints/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/fingerprints/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/fingerprints/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/fingerprints/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/fingerprints/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/fingerprints/#instance-variables","text":"fingerprint is_empty is_identity","title":"Instance variables"},{"location":"reference/arti/fingerprints/#methods","text":"","title":"Methods"},{"location":"reference/arti/fingerprints/#combine","text":"def combine ( self , * others : 'Fingerprint' ) -> 'Fingerprint' View Source def combine ( self , * others : \"Fingerprint\" ) -> \"Fingerprint\" : return reduce ( operator . xor , others , self )","title":"combine"},{"location":"reference/arti/fingerprints/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/fingerprints/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/fingerprints/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/graphs/","text":"Module arti.graphs None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections import defaultdict from collections.abc import Callable , Sequence from functools import cached_property , wraps from graphlib import TopologicalSorter from types import TracebackType from typing import TYPE_CHECKING , Any , Literal , Optional , TypeVar , Union , cast from pydantic import Field , PrivateAttr import arti from arti import io from arti.artifacts import Artifact from arti.backends import Backend from arti.backends.memory import MemoryBackend from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.utils import TypedBox , frozendict from arti.partitions import CompositeKey from arti.producers import Producer from arti.storage import StoragePartition , StoragePartitions from arti.views import View if TYPE_CHECKING : from arti.executors import Executor else : from arti.internal.patches import patch_TopologicalSorter_class_getitem patch_TopologicalSorter_class_getitem () # TODO: Add GraphMetadata model SEALED : Literal [ True ] = True OPEN : Literal [ False ] = False BOX_KWARGS = { status : { \"box_dots\" : True , \"default_box\" : status is OPEN , \"frozen_box\" : status is SEALED , } for status in ( OPEN , SEALED ) } _Return = TypeVar ( \"_Return\" ) def requires_sealed ( fn : Callable [ ... , _Return ]) -> Callable [ ... , _Return ]: @wraps ( fn ) def check_if_sealed ( self : \"Graph\" , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \" { fn . __name__ } cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed class ArtifactBox ( TypedBox [ str , Artifact ]): def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : object . __setattr__ ( self , \"_path\" , ()) super () . __init__ ( * args , ** kwargs ) def _TypedBox__cast_value ( self , item : str , artifact : Any ) -> Artifact : artifact = super () . _TypedBox__cast_value ( item , artifact ) # type: ignore storage_template_values = dict [ str , Any ]() if ( graph := arti . context . graph ) is not None : storage_template_values . update ( { \"graph_name\" : graph . name , \"names\" : ( * self . _path , item ), \"path_tags\" : graph . path_tags , } ) # Require an {input_fingerprint} template in the Storage if this Artifact is being generated # by a Producer. Otherwise, strip the {input_fingerprint} template (if set) for \"raw\" # Artifacts. # # We can't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won't know the # \"final\" instance until assignment here to the Graph. if artifact . producer_output is None : storage_template_values [ \"input_fingerprint\" ] = Fingerprint . empty () elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a ' {input_fingerprint} ' template in their Storage\" ) return cast ( Artifact , artifact . copy ( update = { \"storage\" : artifact . storage . resolve_templates ( ** storage_template_values )} ), ) def _Box__convert_and_store ( self , item : str , value : Artifact ) -> None : if isinstance ( value , dict ): super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member # TODO: Test if this works with `Box({\"some\": {\"nested\": \"thing\"}})`. # Guessing not, may need to put in an empty dict/box first, set the path, # and then update it. object . __setattr__ ( self [ item ], \"_path\" , self . _path + ( item ,)) else : super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member def _Box__get_default ( self , item : str , attr : bool = False ) -> Any : value = super () . _Box__get_default ( item , attr = attr ) # type: ignore object . __setattr__ ( value , \"_path\" , self . _path + ( item ,)) return value Node = Union [ Artifact , Producer ] NodeDependencies = frozendict [ Node , frozenset [ Node ]] class Graph ( Model ): \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" _fingerprint_excludes_ = frozenset ([ \"backend\" ]) name : str backend : Backend = Field ( default_factory = MemoryBackend ) path_tags : frozendict [ str , str ] = frozendict () snapshot_id : Optional [ Fingerprint ] = None # Graph starts off sealed, but is opened within a `with Graph(...)` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifacts : ArtifactBox = PrivateAttr ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ])) _artifact_to_key : frozendict [ Artifact , str ] = PrivateAttr ( frozendict ()) def __enter__ ( self ) -> \"Graph\" : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: { arti . context . graph } \" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type [ BaseException ]], exc_value : Optional [ BaseException ], exc_traceback : Optional [ TracebackType ], ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ) . prepare () def _toggle ( self , status : bool ) -> None : self . _status = status self . _artifacts = ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ]) self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk ()} ) @property def artifacts ( self ) -> ArtifactBox : return self . _artifacts @property def artifact_to_key ( self ) -> frozendict [ Artifact , str ]: return self . _artifact_to_key @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO: Resolve and statically set all available fingerprints. Specifically, we # should pin the Producer.fingerprint, which may by dynamic (eg: version is a # Timestamp). Unbuilt Artifact (partitions) won't be fully resolved yet. if self . snapshot_id : return self snapshot_id , known_artifact_partitions = self . fingerprint , dict [ str , StoragePartitions ]() for node , _ in self . dependencies . items (): snapshot_id = snapshot_id . combine ( node . fingerprint ) if isinstance ( node , Artifact ): key = self . artifact_to_key [ node ] snapshot_id = snapshot_id . combine ( Fingerprint . from_string ( key )) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we'll have to handle things a bit # differently depending on if the external Artifacts are Produced (in an upstream # Graph) or not. if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ]: content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No { content_str } found for ` { key } `: { node } \" ) snapshot_id = snapshot_id . combine ( * [ partition . fingerprint for partition in known_artifact_partitions [ key ]] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma: no cover # NOTE: This shouldn't happen unless the logic above is faulty. raise ValueError ( \"Fingerprint is empty!\" ) snapshot = self . copy ( update = { \"snapshot_id\" : snapshot_id }) assert snapshot . snapshot_id is not None # mypy # Write the discovered partitions (if not already known) and link to this new snapshot. for key , partitions in known_artifact_partitions . items (): snapshot . backend . write_artifact_and_graph_partitions ( snapshot . artifacts [ key ], partitions , self . name , snapshot . snapshot_id , key ) return snapshot def get_snapshot_id ( self ) -> Fingerprint : return cast ( Fingerprint , self . snapshot () . snapshot_id ) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ({ artifact . producer_output . producer }) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output := artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producers ( self ) -> frozenset [ Producer ]: return frozenset ( self . producer_outputs ) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer , tuple [ Artifact , ... ]]: d = defaultdict [ Producer , dict [ int , Artifact ]]( dict ) for _ , artifact in self . artifacts . walk (): if artifact . producer_output is None : continue output = artifact . producer_output d [ output . producer ][ output . position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) @requires_sealed def tag ( self , tag : str , overwrite : bool = False ) -> \"Graph\" : snapshot = self . snapshot () assert snapshot . snapshot_id is not None snapshot . backend . write_graph_tag ( snapshot . name , snapshot . snapshot_id , tag , overwrite ) return snapshot @requires_sealed def from_tag ( self , tag : str ) -> \"Graph\" : return self . copy ( update = { \"snapshot_id\" : self . backend . read_graph_tag ( self . name , tag )}) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere # else), eg: type ~= view. Doing validation on the data, etc. Should some of this # live on the View? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence [ StoragePartition ]] = None , view : Optional [ View ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) elif annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) elif annotation is not None : view = View . get_class_for ( annotation , validation_type = artifact . type )() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : with self . backend . connect () as backend : storage_partitions = backend . read_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if self . snapshot_id is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (` { key } `) would cause a `snapshot_id` change.\" ) if view is None : view = View . get_class_for ( type ( data ), validation_type = artifact . type )() storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ) . with_content_fingerprint () # TODO: Should we only do this in bulk? We might want the backends to # transparently batch requests, but that's not so friendly with the transient # \".connect\". with self . backend . connect () as backend : backend . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if the id would change: # - If snapshot_id is already set, we'd link to the wrong snapshot (we guard against # this above) # - If unset, we'd calculate the new id, but future `.snapshot` calls would handle too # - Additionally, snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition ) Variables BOX_KWARGS Node NodeDependencies OPEN SEALED TYPE_CHECKING Functions requires_sealed def requires_sealed ( fn : collections . abc . Callable [ ... , ~ _Return ] ) -> collections . abc . Callable [ ... , ~ _Return ] View Source def requires_sealed ( fn : Callable [ ..., _Return ] ) -> Callable [ ..., _Return ] : @wraps ( fn ) def check_if_sealed ( self : \"Graph\" , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \"{fn.__name__} cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed Classes ArtifactBox class ArtifactBox ( * args : Any , ** kwargs : Any ) View Source class ArtifactBox ( TypedBox [ str , Artifact ] ) : def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : object . __setattr__ ( self , \"_path\" , ()) super (). __init__ ( * args , ** kwargs ) def _TypedBox__cast_value ( self , item : str , artifact : Any ) -> Artifact : artifact = super (). _TypedBox__cast_value ( item , artifact ) # type: ignore storage_template_values = dict [ str , Any ] () if ( graph := arti . context . graph ) is not None : storage_template_values . update ( { \"graph_name\" : graph . name , \"names\" : ( * self . _path , item ), \"path_tags\" : graph . path_tags , } ) # Require an {input_fingerprint} template in the Storage if this Artifact is being generated # by a Producer. Otherwise, strip the {input_fingerprint} template (if set) for \"raw\" # Artifacts. # # We can't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won't know the # \"final\" instance until assignment here to the Graph. if artifact . producer_output is None : storage_template_values [ \"input_fingerprint\" ] = Fingerprint . empty () elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a '{input_fingerprint}' template in their Storage\" ) return cast ( Artifact , artifact . copy ( update = { \"storage\" : artifact . storage . resolve_templates ( ** storage_template_values ) } ), ) def _Box__convert_and_store ( self , item : str , value : Artifact ) -> None : if isinstance ( value , dict ) : super (). _Box__convert_and_store ( item , value ) # pylint: disable=no-member # TODO: Test if this works with `Box({\"some\": {\"nested\": \"thing\"}})`. # Guessing not, may need to put in an empty dict/box first, set the path, # and then update it. object . __setattr__ ( self [ item ] , \"_path\" , self . _path + ( item ,)) else : super (). _Box__convert_and_store ( item , value ) # pylint: disable=no-member def _Box__get_default ( self , item : str , attr : bool = False ) -> Any : value = super (). _Box__get_default ( item , attr = attr ) # type: ignore object . __setattr__ ( value , \"_path\" , self . _path + ( item ,)) return value Ancestors (in MRO) arti.internal.utils.TypedBox arti.internal.utils.TypedBox box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container Static methods from_json def from_json ( json_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) from_msgpack def from_msgpack ( msgpack_bytes : bytes = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : bytes = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' ) from_toml def from_toml ( toml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args ) from_yaml def from_yaml ( yaml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) Methods clear def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) super () . clear () self . _box_config [ \" __safe_keys \" ]. clear () copy def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : return Box ( super (). copy (), ** self . __box_config ()) fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default =< object object at 0x7f664d253ed0 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ] items def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ] keys def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x )) merge_update def merge_update ( self , _Box__m = None , ** kwargs ) View Source def merge_update ( self , __m = None , ** kwargs ) : def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ()) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ()) merge_type = kwargs . get ( \"box_merge_lists\" ) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if __m : if hasattr ( __m , \"keys\" ) : for key in __m : convert_and_set ( key , __m [ key ] ) else : for key , value in __m : convert_and_set ( key , value ) for key in kwargs : convert_and_set ( key , kwargs [ key ] ) pop def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item popitem def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \" Empty box \" ) from None return key , self . pop ( key ) setdefault def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ()) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ()) self [ item ] = default return self [ item ] to_dict def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict to_json def to_json ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** json_kwargs ) : \"\"\" Transform the Box object into a JSON string . : param filename : If provided will save to file : param encoding : File encoding : param errors : How to handle encoding errors : param json_kwargs : additional arguments to pass to json . dump ( s ) : return : string of JSON ( if no filename provided ) \"\"\" return _to_json ( self . to_dict () , filename = filename , encoding = encoding , errors = errors , ** json_kwargs ) to_msgpack def to_msgpack ( self , filename : Union [ str , os . PathLike ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Union[str, PathLike] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package') to_toml def to_toml ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" ) : \"\"\" Transform the Box object into a toml string . : param filename : File to write toml object too : param encoding : File encoding : param errors : How to handle encoding errors : return : string of TOML ( if no filename provided ) \"\"\" return _to_toml ( self . to_dict () , filename = filename , encoding = encoding , errors = errors ) to_yaml def to_yaml ( self , filename : Union [ str , os . PathLike ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Union [ str , PathLike ] = None , default_flow_style : bool = False , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** yaml_kwargs , ) : \"\"\" Transform the Box object into a YAML string . : param filename : If provided will save to file : param default_flow_style : False will recursively dump dicts : param encoding : File encoding : param errors : How to handle encoding errors : param yaml_kwargs : additional arguments to pass to yaml . dump : return : string of YAML ( if no filename provided ) \"\"\" return _to_yaml ( self . to_dict () , filename = filename , default_flow_style = default_flow_style , encoding = encoding , errors = errors , ** yaml_kwargs , ) update def update ( self , _Box__m = None , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , __m = None , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if __m : if hasattr ( __m , \"keys\" ) : for k in __m : self . __convert_and_store ( k , __m [ k ] ) else : for k , v in __m : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] ) values def values ( ... ) D.values() -> an object providing a view on D's values walk def walk ( self , root : tuple [ ~ _K_str , ... ] = () ) -> collections . abc . Iterator [ tuple [ ~ _K_str , ~ _V ]] View Source def walk ( self , root : tuple [ _K_str , ...] = ()) -> Iterator [ tuple [ _K_str , _V ]] : for k , v in self . items () : subroot = root + ( k ,) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore Graph class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" _fingerprint_excludes_ = frozenset ( [ \"backend\" ] ) name : str backend : Backend = Field ( default_factory = MemoryBackend ) path_tags : frozendict [ str, str ] = frozendict () snapshot_id : Optional [ Fingerprint ] = None # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifacts : ArtifactBox = PrivateAttr ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) def __enter__ ( self ) -> \"Graph\" : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : self . _status = status self . _artifacts = ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] ) self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifacts ( self ) -> ArtifactBox : return self . _artifacts @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn 't happen unless the logic above is faulty. raise ValueError(\"Fingerprint is empty!\") snapshot = self.copy(update={\"snapshot_id\": snapshot_id}) assert snapshot.snapshot_id is not None # mypy # Write the discovered partitions (if not already known) and link to this new snapshot. for key, partitions in known_artifact_partitions.items(): snapshot.backend.write_artifact_and_graph_partitions( snapshot.artifacts[key], partitions, self.name, snapshot.snapshot_id, key ) return snapshot def get_snapshot_id(self) -> Fingerprint: return cast(Fingerprint, self.snapshot().snapshot_id) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def dependencies(self) -> NodeDependencies: artifact_deps = { artifact: ( frozenset({artifact.producer_output.producer}) if artifact.producer_output is not None else frozenset() ) for _, artifact in self.artifacts.walk() } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output.producer: frozenset(producer_output.producer.inputs.values()) for artifact in artifact_deps if (producer_output := artifact.producer_output) is not None } return NodeDependencies(artifact_deps | producer_deps) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producers(self) -> frozenset[Producer]: return frozenset(self.producer_outputs) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producer_outputs(self) -> frozendict[Producer, tuple[Artifact, ...]]: d = defaultdict[Producer, dict[int, Artifact]](dict) for _, artifact in self.artifacts.walk(): if artifact.producer_output is None: continue output = artifact.producer_output d[output.producer][output.position] = artifact return frozendict( (producer, tuple(artifacts_by_position[i] for i in sorted(artifacts_by_position))) for producer, artifacts_by_position in d.items() ) @requires_sealed def tag(self, tag: str, overwrite: bool = False) -> \"Graph\": snapshot = self.snapshot() assert snapshot.snapshot_id is not None snapshot.backend.write_graph_tag(snapshot.name, snapshot.snapshot_id, tag, overwrite) return snapshot @requires_sealed def from_tag(self, tag: str) -> \"Graph\": return self.copy(update={\"snapshot_id\": self.backend.read_graph_tag(self.name, tag)}) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere # else), eg: type ~= view. Doing validation on the data, etc. Should some of this # live on the View? @requires_sealed def read( self, artifact: Artifact, *, annotation: Optional[Any] = None, storage_partitions: Optional[Sequence[StoragePartition]] = None, view: Optional[View] = None, ) -> Any: key = self.artifact_to_key[artifact] if annotation is None and view is None: raise ValueError(\"Either `annotation` or `view` must be passed\") elif annotation is not None and view is not None: raise ValueError(\"Only one of `annotation` or `view` may be passed\") elif annotation is not None: view = View.get_class_for(annotation, validation_type=artifact.type)() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None: with self.backend.connect() as backend: storage_partitions = backend.read_graph_partitions( self.name, self.get_snapshot_id(), key, artifact ) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if self.snapshot_id is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None: view = View.get_class_for(type(data), validation_type=artifact.type)() storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to # transparently batch requests, but that' s not so friendly with the transient # \".connect\" . with self . backend . connect () as backend : backend . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if the id would change : # - If snapshot_id is already set , we 'd link to the wrong snapshot (we guard against # this above) # - If unset, we' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables artifact_to_key artifacts fingerprint Methods build def build ( self , executor : 'Optional[Executor]' = None ) -> 'Graph' View Source @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dependencies def dependencies ( ... ) dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. from_tag def from_tag ( self , tag : str ) -> 'Graph' View Source @requires_sealed def from_tag ( self , tag : str ) -> \"Graph\" : return self . copy ( update = { \"snapshot_id\" : self . backend . read_graph_tag ( self . name , tag ) } ) get_snapshot_id def get_snapshot_id ( self ) -> arti . fingerprints . Fingerprint View Source def get_snapshot_id ( self ) -> Fingerprint : return cast ( Fingerprint , self . snapshot (). snapshot_id ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . producer_outputs def producer_outputs ( ... ) producers def producers ( ... ) read def read ( self , artifact : arti . artifacts . Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ collections . abc . Sequence [ arti . storage . StoragePartition ]] = None , view : Optional [ arti . views . View ] = None ) -> Any View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) elif annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) elif annotation is not None : view = View . get_class_for ( annotation , validation_type = artifact . type )() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : with self . backend . connect () as backend : storage_partitions = backend . read_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) snapshot def snapshot ( self ) -> 'Graph' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = self . copy ( update = { \"snapshot_id\" : snapshot_id } ) assert snapshot . snapshot_id is not None # mypy # Write the discovered partitions ( if not already known ) and link to this new snapshot . for key , partitions in known_artifact_partitions . items () : snapshot . backend . write_artifact_and_graph_partitions ( snapshot . artifacts [ key ] , partitions , self . name , snapshot . snapshot_id , key ) return snapshot tag def tag ( self , tag : str , overwrite : bool = False ) -> 'Graph' View Source @requires_sealed def tag ( self , tag : str , overwrite : bool = False ) -> \"Graph\" : snapshot = self . snapshot () assert snapshot . snapshot_id is not None snapshot . backend . write_graph_tag ( snapshot . name , snapshot . snapshot_id , tag , overwrite ) return snapshot write def write ( self , data : Any , * , artifact : arti . artifacts . Artifact , input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), view : Optional [ arti . views . View ] = None ) -> arti . storage . StoragePartition View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if self . snapshot_id is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None : view = View . get_class_for ( type ( data ), validation_type = artifact . type )() storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to # transparently batch requests , but that 's not so friendly with the transient # \".connect\". with self.backend.connect() as backend: backend.write_artifact_partitions(artifact, (storage_partition,)) # Skip linking this partition to the snapshot if the id would change: # - If snapshot_id is already set, we' d link to the wrong snapshot ( we guard against # this above ) # - If unset , we ' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition )","title":"Graphs"},{"location":"reference/arti/graphs/#module-artigraphs","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections import defaultdict from collections.abc import Callable , Sequence from functools import cached_property , wraps from graphlib import TopologicalSorter from types import TracebackType from typing import TYPE_CHECKING , Any , Literal , Optional , TypeVar , Union , cast from pydantic import Field , PrivateAttr import arti from arti import io from arti.artifacts import Artifact from arti.backends import Backend from arti.backends.memory import MemoryBackend from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.internal.utils import TypedBox , frozendict from arti.partitions import CompositeKey from arti.producers import Producer from arti.storage import StoragePartition , StoragePartitions from arti.views import View if TYPE_CHECKING : from arti.executors import Executor else : from arti.internal.patches import patch_TopologicalSorter_class_getitem patch_TopologicalSorter_class_getitem () # TODO: Add GraphMetadata model SEALED : Literal [ True ] = True OPEN : Literal [ False ] = False BOX_KWARGS = { status : { \"box_dots\" : True , \"default_box\" : status is OPEN , \"frozen_box\" : status is SEALED , } for status in ( OPEN , SEALED ) } _Return = TypeVar ( \"_Return\" ) def requires_sealed ( fn : Callable [ ... , _Return ]) -> Callable [ ... , _Return ]: @wraps ( fn ) def check_if_sealed ( self : \"Graph\" , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \" { fn . __name__ } cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed class ArtifactBox ( TypedBox [ str , Artifact ]): def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : object . __setattr__ ( self , \"_path\" , ()) super () . __init__ ( * args , ** kwargs ) def _TypedBox__cast_value ( self , item : str , artifact : Any ) -> Artifact : artifact = super () . _TypedBox__cast_value ( item , artifact ) # type: ignore storage_template_values = dict [ str , Any ]() if ( graph := arti . context . graph ) is not None : storage_template_values . update ( { \"graph_name\" : graph . name , \"names\" : ( * self . _path , item ), \"path_tags\" : graph . path_tags , } ) # Require an {input_fingerprint} template in the Storage if this Artifact is being generated # by a Producer. Otherwise, strip the {input_fingerprint} template (if set) for \"raw\" # Artifacts. # # We can't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won't know the # \"final\" instance until assignment here to the Graph. if artifact . producer_output is None : storage_template_values [ \"input_fingerprint\" ] = Fingerprint . empty () elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a ' {input_fingerprint} ' template in their Storage\" ) return cast ( Artifact , artifact . copy ( update = { \"storage\" : artifact . storage . resolve_templates ( ** storage_template_values )} ), ) def _Box__convert_and_store ( self , item : str , value : Artifact ) -> None : if isinstance ( value , dict ): super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member # TODO: Test if this works with `Box({\"some\": {\"nested\": \"thing\"}})`. # Guessing not, may need to put in an empty dict/box first, set the path, # and then update it. object . __setattr__ ( self [ item ], \"_path\" , self . _path + ( item ,)) else : super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member def _Box__get_default ( self , item : str , attr : bool = False ) -> Any : value = super () . _Box__get_default ( item , attr = attr ) # type: ignore object . __setattr__ ( value , \"_path\" , self . _path + ( item ,)) return value Node = Union [ Artifact , Producer ] NodeDependencies = frozendict [ Node , frozenset [ Node ]] class Graph ( Model ): \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" _fingerprint_excludes_ = frozenset ([ \"backend\" ]) name : str backend : Backend = Field ( default_factory = MemoryBackend ) path_tags : frozendict [ str , str ] = frozendict () snapshot_id : Optional [ Fingerprint ] = None # Graph starts off sealed, but is opened within a `with Graph(...)` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifacts : ArtifactBox = PrivateAttr ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ])) _artifact_to_key : frozendict [ Artifact , str ] = PrivateAttr ( frozendict ()) def __enter__ ( self ) -> \"Graph\" : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: { arti . context . graph } \" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type [ BaseException ]], exc_value : Optional [ BaseException ], exc_traceback : Optional [ TracebackType ], ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ) . prepare () def _toggle ( self , status : bool ) -> None : self . _status = status self . _artifacts = ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ]) self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk ()} ) @property def artifacts ( self ) -> ArtifactBox : return self . _artifacts @property def artifact_to_key ( self ) -> frozendict [ Artifact , str ]: return self . _artifact_to_key @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO: Resolve and statically set all available fingerprints. Specifically, we # should pin the Producer.fingerprint, which may by dynamic (eg: version is a # Timestamp). Unbuilt Artifact (partitions) won't be fully resolved yet. if self . snapshot_id : return self snapshot_id , known_artifact_partitions = self . fingerprint , dict [ str , StoragePartitions ]() for node , _ in self . dependencies . items (): snapshot_id = snapshot_id . combine ( node . fingerprint ) if isinstance ( node , Artifact ): key = self . artifact_to_key [ node ] snapshot_id = snapshot_id . combine ( Fingerprint . from_string ( key )) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we'll have to handle things a bit # differently depending on if the external Artifacts are Produced (in an upstream # Graph) or not. if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ]: content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No { content_str } found for ` { key } `: { node } \" ) snapshot_id = snapshot_id . combine ( * [ partition . fingerprint for partition in known_artifact_partitions [ key ]] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma: no cover # NOTE: This shouldn't happen unless the logic above is faulty. raise ValueError ( \"Fingerprint is empty!\" ) snapshot = self . copy ( update = { \"snapshot_id\" : snapshot_id }) assert snapshot . snapshot_id is not None # mypy # Write the discovered partitions (if not already known) and link to this new snapshot. for key , partitions in known_artifact_partitions . items (): snapshot . backend . write_artifact_and_graph_partitions ( snapshot . artifacts [ key ], partitions , self . name , snapshot . snapshot_id , key ) return snapshot def get_snapshot_id ( self ) -> Fingerprint : return cast ( Fingerprint , self . snapshot () . snapshot_id ) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def dependencies ( self ) -> NodeDependencies : artifact_deps = { artifact : ( frozenset ({ artifact . producer_output . producer }) if artifact . producer_output is not None else frozenset () ) for _ , artifact in self . artifacts . walk () } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output . producer : frozenset ( producer_output . producer . inputs . values ()) for artifact in artifact_deps if ( producer_output := artifact . producer_output ) is not None } return NodeDependencies ( artifact_deps | producer_deps ) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producers ( self ) -> frozenset [ Producer ]: return frozenset ( self . producer_outputs ) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producer_outputs ( self ) -> frozendict [ Producer , tuple [ Artifact , ... ]]: d = defaultdict [ Producer , dict [ int , Artifact ]]( dict ) for _ , artifact in self . artifacts . walk (): if artifact . producer_output is None : continue output = artifact . producer_output d [ output . producer ][ output . position ] = artifact return frozendict ( ( producer , tuple ( artifacts_by_position [ i ] for i in sorted ( artifacts_by_position ))) for producer , artifacts_by_position in d . items () ) @requires_sealed def tag ( self , tag : str , overwrite : bool = False ) -> \"Graph\" : snapshot = self . snapshot () assert snapshot . snapshot_id is not None snapshot . backend . write_graph_tag ( snapshot . name , snapshot . snapshot_id , tag , overwrite ) return snapshot @requires_sealed def from_tag ( self , tag : str ) -> \"Graph\" : return self . copy ( update = { \"snapshot_id\" : self . backend . read_graph_tag ( self . name , tag )}) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere # else), eg: type ~= view. Doing validation on the data, etc. Should some of this # live on the View? @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence [ StoragePartition ]] = None , view : Optional [ View ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) elif annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) elif annotation is not None : view = View . get_class_for ( annotation , validation_type = artifact . type )() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : with self . backend . connect () as backend : storage_partitions = backend . read_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , ) @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if self . snapshot_id is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (` { key } `) would cause a `snapshot_id` change.\" ) if view is None : view = View . get_class_for ( type ( data ), validation_type = artifact . type )() storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ) . with_content_fingerprint () # TODO: Should we only do this in bulk? We might want the backends to # transparently batch requests, but that's not so friendly with the transient # \".connect\". with self . backend . connect () as backend : backend . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if the id would change: # - If snapshot_id is already set, we'd link to the wrong snapshot (we guard against # this above) # - If unset, we'd calculate the new id, but future `.snapshot` calls would handle too # - Additionally, snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition )","title":"Module arti.graphs"},{"location":"reference/arti/graphs/#variables","text":"BOX_KWARGS Node NodeDependencies OPEN SEALED TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/graphs/#functions","text":"","title":"Functions"},{"location":"reference/arti/graphs/#requires_sealed","text":"def requires_sealed ( fn : collections . abc . Callable [ ... , ~ _Return ] ) -> collections . abc . Callable [ ... , ~ _Return ] View Source def requires_sealed ( fn : Callable [ ..., _Return ] ) -> Callable [ ..., _Return ] : @wraps ( fn ) def check_if_sealed ( self : \"Graph\" , * args : Any , ** kwargs : Any ) -> _Return : if self . _status is not SEALED : raise ValueError ( f \"{fn.__name__} cannot be used while the Graph is still being defined\" ) return fn ( self , * args , ** kwargs ) return check_if_sealed","title":"requires_sealed"},{"location":"reference/arti/graphs/#classes","text":"","title":"Classes"},{"location":"reference/arti/graphs/#artifactbox","text":"class ArtifactBox ( * args : Any , ** kwargs : Any ) View Source class ArtifactBox ( TypedBox [ str , Artifact ] ) : def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : object . __setattr__ ( self , \"_path\" , ()) super (). __init__ ( * args , ** kwargs ) def _TypedBox__cast_value ( self , item : str , artifact : Any ) -> Artifact : artifact = super (). _TypedBox__cast_value ( item , artifact ) # type: ignore storage_template_values = dict [ str , Any ] () if ( graph := arti . context . graph ) is not None : storage_template_values . update ( { \"graph_name\" : graph . name , \"names\" : ( * self . _path , item ), \"path_tags\" : graph . path_tags , } ) # Require an {input_fingerprint} template in the Storage if this Artifact is being generated # by a Producer. Otherwise, strip the {input_fingerprint} template (if set) for \"raw\" # Artifacts. # # We can't validate this at Artifact instantiation because the Producer is tracked later (by # copying the instance and setting the `producer_output` attribute). We won't know the # \"final\" instance until assignment here to the Graph. if artifact . producer_output is None : storage_template_values [ \"input_fingerprint\" ] = Fingerprint . empty () elif not artifact . storage . includes_input_fingerprint_template : raise ValueError ( \"Produced Artifacts must have a '{input_fingerprint}' template in their Storage\" ) return cast ( Artifact , artifact . copy ( update = { \"storage\" : artifact . storage . resolve_templates ( ** storage_template_values ) } ), ) def _Box__convert_and_store ( self , item : str , value : Artifact ) -> None : if isinstance ( value , dict ) : super (). _Box__convert_and_store ( item , value ) # pylint: disable=no-member # TODO: Test if this works with `Box({\"some\": {\"nested\": \"thing\"}})`. # Guessing not, may need to put in an empty dict/box first, set the path, # and then update it. object . __setattr__ ( self [ item ] , \"_path\" , self . _path + ( item ,)) else : super (). _Box__convert_and_store ( item , value ) # pylint: disable=no-member def _Box__get_default ( self , item : str , attr : bool = False ) -> Any : value = super (). _Box__get_default ( item , attr = attr ) # type: ignore object . __setattr__ ( value , \"_path\" , self . _path + ( item ,)) return value","title":"ArtifactBox"},{"location":"reference/arti/graphs/#ancestors-in-mro","text":"arti.internal.utils.TypedBox arti.internal.utils.TypedBox box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container","title":"Ancestors (in MRO)"},{"location":"reference/arti/graphs/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/graphs/#from_json","text":"def from_json ( json_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_json"},{"location":"reference/arti/graphs/#from_msgpack","text":"def from_msgpack ( msgpack_bytes : bytes = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : bytes = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' )","title":"from_msgpack"},{"location":"reference/arti/graphs/#from_toml","text":"def from_toml ( toml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args )","title":"from_toml"},{"location":"reference/arti/graphs/#from_yaml","text":"def from_yaml ( yaml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_yaml"},{"location":"reference/arti/graphs/#methods","text":"","title":"Methods"},{"location":"reference/arti/graphs/#clear","text":"def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) super () . clear () self . _box_config [ \" __safe_keys \" ]. clear ()","title":"clear"},{"location":"reference/arti/graphs/#copy","text":"def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : return Box ( super (). copy (), ** self . __box_config ())","title":"copy"},{"location":"reference/arti/graphs/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/arti/graphs/#get","text":"def get ( self , key , default =< object object at 0x7f664d253ed0 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ]","title":"get"},{"location":"reference/arti/graphs/#items","text":"def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ]","title":"items"},{"location":"reference/arti/graphs/#keys","text":"def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x ))","title":"keys"},{"location":"reference/arti/graphs/#merge_update","text":"def merge_update ( self , _Box__m = None , ** kwargs ) View Source def merge_update ( self , __m = None , ** kwargs ) : def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ()) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ()) merge_type = kwargs . get ( \"box_merge_lists\" ) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if __m : if hasattr ( __m , \"keys\" ) : for key in __m : convert_and_set ( key , __m [ key ] ) else : for key , value in __m : convert_and_set ( key , value ) for key in kwargs : convert_and_set ( key , kwargs [ key ] )","title":"merge_update"},{"location":"reference/arti/graphs/#pop","text":"def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item","title":"pop"},{"location":"reference/arti/graphs/#popitem","text":"def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \" Empty box \" ) from None return key , self . pop ( key )","title":"popitem"},{"location":"reference/arti/graphs/#setdefault","text":"def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ()) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ()) self [ item ] = default return self [ item ]","title":"setdefault"},{"location":"reference/arti/graphs/#to_dict","text":"def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict","title":"to_dict"},{"location":"reference/arti/graphs/#to_json","text":"def to_json ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** json_kwargs ) : \"\"\" Transform the Box object into a JSON string . : param filename : If provided will save to file : param encoding : File encoding : param errors : How to handle encoding errors : param json_kwargs : additional arguments to pass to json . dump ( s ) : return : string of JSON ( if no filename provided ) \"\"\" return _to_json ( self . to_dict () , filename = filename , encoding = encoding , errors = errors , ** json_kwargs )","title":"to_json"},{"location":"reference/arti/graphs/#to_msgpack","text":"def to_msgpack ( self , filename : Union [ str , os . PathLike ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Union[str, PathLike] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package')","title":"to_msgpack"},{"location":"reference/arti/graphs/#to_toml","text":"def to_toml ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" ) : \"\"\" Transform the Box object into a toml string . : param filename : File to write toml object too : param encoding : File encoding : param errors : How to handle encoding errors : return : string of TOML ( if no filename provided ) \"\"\" return _to_toml ( self . to_dict () , filename = filename , encoding = encoding , errors = errors )","title":"to_toml"},{"location":"reference/arti/graphs/#to_yaml","text":"def to_yaml ( self , filename : Union [ str , os . PathLike ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Union [ str , PathLike ] = None , default_flow_style : bool = False , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** yaml_kwargs , ) : \"\"\" Transform the Box object into a YAML string . : param filename : If provided will save to file : param default_flow_style : False will recursively dump dicts : param encoding : File encoding : param errors : How to handle encoding errors : param yaml_kwargs : additional arguments to pass to yaml . dump : return : string of YAML ( if no filename provided ) \"\"\" return _to_yaml ( self . to_dict () , filename = filename , default_flow_style = default_flow_style , encoding = encoding , errors = errors , ** yaml_kwargs , )","title":"to_yaml"},{"location":"reference/arti/graphs/#update","text":"def update ( self , _Box__m = None , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , __m = None , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if __m : if hasattr ( __m , \"keys\" ) : for k in __m : self . __convert_and_store ( k , __m [ k ] ) else : for k , v in __m : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] )","title":"update"},{"location":"reference/arti/graphs/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/graphs/#walk","text":"def walk ( self , root : tuple [ ~ _K_str , ... ] = () ) -> collections . abc . Iterator [ tuple [ ~ _K_str , ~ _V ]] View Source def walk ( self , root : tuple [ _K_str , ...] = ()) -> Iterator [ tuple [ _K_str , _V ]] : for k , v in self . items () : subroot = root + ( k ,) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore","title":"walk"},{"location":"reference/arti/graphs/#graph","text":"class Graph ( __pydantic_self__ , ** data : Any ) View Source class Graph ( Model ) : \"\"\"Graph stores a web of Artifacts connected by Producers.\"\"\" _fingerprint_excludes_ = frozenset ( [ \"backend\" ] ) name : str backend : Backend = Field ( default_factory = MemoryBackend ) path_tags : frozendict [ str, str ] = frozendict () snapshot_id : Optional [ Fingerprint ] = None # Graph starts off sealed , but is opened within a ` with Graph (...) ` context _status : Optional [ bool ] = PrivateAttr ( None ) _artifacts : ArtifactBox = PrivateAttr ( default_factory = lambda : ArtifactBox ( ** BOX_KWARGS [ SEALED ] )) _artifact_to_key : frozendict [ Artifact, str ] = PrivateAttr ( frozendict ()) def __enter__ ( self ) -> \"Graph\" : if arti . context . graph is not None : raise ValueError ( f \"Another graph is being defined: {arti.context.graph}\" ) arti . context . graph = self self . _toggle ( OPEN ) return self def __exit__ ( self , exc_type : Optional [ type[BaseException ] ] , exc_value : Optional [ BaseException ] , exc_traceback : Optional [ TracebackType ] , ) -> None : arti . context . graph = None self . _toggle ( SEALED ) # Confirm the dependencies are acyclic TopologicalSorter ( self . dependencies ). prepare () def _toggle ( self , status : bool ) -> None : self . _status = status self . _artifacts = ArtifactBox ( self . artifacts , ** BOX_KWARGS [ status ] ) self . _artifact_to_key = frozendict ( { artifact : key for key , artifact in self . artifacts . walk () } ) @property def artifacts ( self ) -> ArtifactBox : return self . _artifacts @property def artifact_to_key ( self ) -> frozendict [ Artifact, str ] : return self . _artifact_to_key @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti . executors . local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn 't happen unless the logic above is faulty. raise ValueError(\"Fingerprint is empty!\") snapshot = self.copy(update={\"snapshot_id\": snapshot_id}) assert snapshot.snapshot_id is not None # mypy # Write the discovered partitions (if not already known) and link to this new snapshot. for key, partitions in known_artifact_partitions.items(): snapshot.backend.write_artifact_and_graph_partitions( snapshot.artifacts[key], partitions, self.name, snapshot.snapshot_id, key ) return snapshot def get_snapshot_id(self) -> Fingerprint: return cast(Fingerprint, self.snapshot().snapshot_id) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def dependencies(self) -> NodeDependencies: artifact_deps = { artifact: ( frozenset({artifact.producer_output.producer}) if artifact.producer_output is not None else frozenset() ) for _, artifact in self.artifacts.walk() } producer_deps = { # NOTE: multi-output Producers will appear multiple times (but be deduped) producer_output.producer: frozenset(producer_output.producer.inputs.values()) for artifact in artifact_deps if (producer_output := artifact.producer_output) is not None } return NodeDependencies(artifact_deps | producer_deps) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producers(self) -> frozenset[Producer]: return frozenset(self.producer_outputs) @cached_property # type: ignore # python/mypy#1362 @requires_sealed def producer_outputs(self) -> frozendict[Producer, tuple[Artifact, ...]]: d = defaultdict[Producer, dict[int, Artifact]](dict) for _, artifact in self.artifacts.walk(): if artifact.producer_output is None: continue output = artifact.producer_output d[output.producer][output.position] = artifact return frozendict( (producer, tuple(artifacts_by_position[i] for i in sorted(artifacts_by_position))) for producer, artifacts_by_position in d.items() ) @requires_sealed def tag(self, tag: str, overwrite: bool = False) -> \"Graph\": snapshot = self.snapshot() assert snapshot.snapshot_id is not None snapshot.backend.write_graph_tag(snapshot.name, snapshot.snapshot_id, tag, overwrite) return snapshot @requires_sealed def from_tag(self, tag: str) -> \"Graph\": return self.copy(update={\"snapshot_id\": self.backend.read_graph_tag(self.name, tag)}) # TODO: io.read/write probably need a bit of sanity checking (probably somewhere # else), eg: type ~= view. Doing validation on the data, etc. Should some of this # live on the View? @requires_sealed def read( self, artifact: Artifact, *, annotation: Optional[Any] = None, storage_partitions: Optional[Sequence[StoragePartition]] = None, view: Optional[View] = None, ) -> Any: key = self.artifact_to_key[artifact] if annotation is None and view is None: raise ValueError(\"Either `annotation` or `view` must be passed\") elif annotation is not None and view is not None: raise ValueError(\"Only one of `annotation` or `view` may be passed\") elif annotation is not None: view = View.get_class_for(annotation, validation_type=artifact.type)() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None: with self.backend.connect() as backend: storage_partitions = backend.read_graph_partitions( self.name, self.get_snapshot_id(), key, artifact ) return io.read( type_=artifact.type, format=artifact.format, storage_partitions=storage_partitions, view=view, ) @requires_sealed def write( self, data: Any, *, artifact: Artifact, input_fingerprint: Fingerprint = Fingerprint.empty(), keys: CompositeKey = CompositeKey(), view: Optional[View] = None, ) -> StoragePartition: key = self.artifact_to_key[artifact] if self.snapshot_id is not None and artifact.producer_output is None: raise ValueError( f\"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None: view = View.get_class_for(type(data), validation_type=artifact.type)() storage_partition = artifact.storage.generate_partition( input_fingerprint=input_fingerprint, keys=keys, with_content_fingerprint=False ) storage_partition = io.write( data, type_=artifact.type, format=artifact.format, storage_partition=storage_partition, view=view, ).with_content_fingerprint() # TODO: Should we only do this in bulk? We might want the backends to # transparently batch requests, but that' s not so friendly with the transient # \".connect\" . with self . backend . connect () as backend : backend . write_artifact_partitions ( artifact , ( storage_partition ,)) # Skip linking this partition to the snapshot if the id would change : # - If snapshot_id is already set , we 'd link to the wrong snapshot (we guard against # this above) # - If unset, we' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition )","title":"Graph"},{"location":"reference/arti/graphs/#ancestors-in-mro_1","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/graphs/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/graphs/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/graphs/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/graphs/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/graphs/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/graphs/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/graphs/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/graphs/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/graphs/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/graphs/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/graphs/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/graphs/#instance-variables","text":"artifact_to_key artifacts fingerprint","title":"Instance variables"},{"location":"reference/arti/graphs/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/graphs/#build","text":"def build ( self , executor : 'Optional[Executor]' = None ) -> 'Graph' View Source @requires_sealed def build ( self , executor : \"Optional[Executor]\" = None ) -> \"Graph\" : snapshot = self . snapshot () if executor is None : from arti.executors.local import LocalExecutor executor = LocalExecutor () executor . build ( snapshot ) return snapshot","title":"build"},{"location":"reference/arti/graphs/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/graphs/#dependencies","text":"def dependencies ( ... )","title":"dependencies"},{"location":"reference/arti/graphs/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/graphs/#from_tag","text":"def from_tag ( self , tag : str ) -> 'Graph' View Source @requires_sealed def from_tag ( self , tag : str ) -> \"Graph\" : return self . copy ( update = { \"snapshot_id\" : self . backend . read_graph_tag ( self . name , tag ) } )","title":"from_tag"},{"location":"reference/arti/graphs/#get_snapshot_id","text":"def get_snapshot_id ( self ) -> arti . fingerprints . Fingerprint View Source def get_snapshot_id ( self ) -> Fingerprint : return cast ( Fingerprint , self . snapshot (). snapshot_id )","title":"get_snapshot_id"},{"location":"reference/arti/graphs/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/graphs/#producer_outputs","text":"def producer_outputs ( ... )","title":"producer_outputs"},{"location":"reference/arti/graphs/#producers","text":"def producers ( ... )","title":"producers"},{"location":"reference/arti/graphs/#read","text":"def read ( self , artifact : arti . artifacts . Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ collections . abc . Sequence [ arti . storage . StoragePartition ]] = None , view : Optional [ arti . views . View ] = None ) -> Any View Source @requires_sealed def read ( self , artifact : Artifact , * , annotation : Optional [ Any ] = None , storage_partitions : Optional [ Sequence[StoragePartition ] ] = None , view : Optional [ View ] = None , ) -> Any : key = self . artifact_to_key [ artifact ] if annotation is None and view is None : raise ValueError ( \"Either `annotation` or `view` must be passed\" ) elif annotation is not None and view is not None : raise ValueError ( \"Only one of `annotation` or `view` may be passed\" ) elif annotation is not None : view = View . get_class_for ( annotation , validation_type = artifact . type )() assert view is not None # mypy gets mixed up with ^ if storage_partitions is None : with self . backend . connect () as backend : storage_partitions = backend . read_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact ) return io . read ( type_ = artifact . type , format = artifact . format , storage_partitions = storage_partitions , view = view , )","title":"read"},{"location":"reference/arti/graphs/#snapshot","text":"def snapshot ( self ) -> 'Graph' Identify a \"unique\" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a snapshot of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. View Source @requires_sealed def snapshot ( self ) -> \"Graph\" : \"\"\"Identify a \" unique \" ID for this Graph at this point in time. The ID aims to encode the structure of the Graph plus a _snapshot_ of the raw Artifact data (partition kinds and contents). Any change that would affect data should prompt an ID change, however changes to this ID don't directly cause data to be reproduced. NOTE: There is currently a gap (and thus race condition) between when the Graph ID is computed and when we read raw Artifacts data during Producer builds. \"\"\" # TODO : Resolve and statically set all available fingerprints . Specifically , we # should pin the Producer . fingerprint , which may by dynamic ( eg : version is a # Timestamp ). Unbuilt Artifact ( partitions ) won 't be fully resolved yet. if self.snapshot_id: return self snapshot_id, known_artifact_partitions = self.fingerprint, dict[str, StoragePartitions]() for node, _ in self.dependencies.items(): snapshot_id = snapshot_id.combine(node.fingerprint) if isinstance(node, Artifact): key = self.artifact_to_key[node] snapshot_id = snapshot_id.combine(Fingerprint.from_string(key)) # Include fingerprints (including content_fingerprint!) for all raw Artifact # partitions, triggering a graph ID change if these artifacts change out-of-band. # # TODO: Should we *also* inspect Producer.inputs for Artifacts _not_ inside this # Graph and inspect their contents too? I guess we' ll have to handle things a bit # differently depending on if the external Artifacts are Produced ( in an upstream # Graph ) or not . if node . producer_output is None : known_artifact_partitions [ key ] = StoragePartitions ( partition . with_content_fingerprint () for partition in node . discover_storage_partitions () ) if not known_artifact_partitions [ key ] : content_str = \"partitions\" if node . is_partitioned else \"data\" raise ValueError ( f \"No {content_str} found for `{key}`: {node}\" ) snapshot_id = snapshot_id . combine ( *[ partition.fingerprint for partition in known_artifact_partitions[key ] ] ) if snapshot_id . is_empty or snapshot_id . is_identity : # pragma : no cover # NOTE : This shouldn ' t happen unless the logic above is faulty . raise ValueError ( \"Fingerprint is empty!\" ) snapshot = self . copy ( update = { \"snapshot_id\" : snapshot_id } ) assert snapshot . snapshot_id is not None # mypy # Write the discovered partitions ( if not already known ) and link to this new snapshot . for key , partitions in known_artifact_partitions . items () : snapshot . backend . write_artifact_and_graph_partitions ( snapshot . artifacts [ key ] , partitions , self . name , snapshot . snapshot_id , key ) return snapshot","title":"snapshot"},{"location":"reference/arti/graphs/#tag","text":"def tag ( self , tag : str , overwrite : bool = False ) -> 'Graph' View Source @requires_sealed def tag ( self , tag : str , overwrite : bool = False ) -> \"Graph\" : snapshot = self . snapshot () assert snapshot . snapshot_id is not None snapshot . backend . write_graph_tag ( snapshot . name , snapshot . snapshot_id , tag , overwrite ) return snapshot","title":"tag"},{"location":"reference/arti/graphs/#write","text":"def write ( self , data : Any , * , artifact : arti . artifacts . Artifact , input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), view : Optional [ arti . views . View ] = None ) -> arti . storage . StoragePartition View Source @requires_sealed def write ( self , data : Any , * , artifact : Artifact , input_fingerprint : Fingerprint = Fingerprint . empty (), keys : CompositeKey = CompositeKey (), view : Optional [ View ] = None , ) -> StoragePartition : key = self . artifact_to_key [ artifact ] if self . snapshot_id is not None and artifact . producer_output is None : raise ValueError ( f \"Writing to a raw Artifact (`{key}`) would cause a `snapshot_id` change.\" ) if view is None : view = View . get_class_for ( type ( data ), validation_type = artifact . type )() storage_partition = artifact . storage . generate_partition ( input_fingerprint = input_fingerprint , keys = keys , with_content_fingerprint = False ) storage_partition = io . write ( data , type_ = artifact . type , format = artifact . format , storage_partition = storage_partition , view = view , ). with_content_fingerprint () # TODO : Should we only do this in bulk ? We might want the backends to # transparently batch requests , but that 's not so friendly with the transient # \".connect\". with self.backend.connect() as backend: backend.write_artifact_partitions(artifact, (storage_partition,)) # Skip linking this partition to the snapshot if the id would change: # - If snapshot_id is already set, we' d link to the wrong snapshot ( we guard against # this above ) # - If unset , we ' d calculate the new id , but future ` . snapshot ` calls would handle too # - Additionally , snapshotting may fail if not all other inputs are available now if artifact . producer_output is not None : backend . write_graph_partitions ( self . name , self . get_snapshot_id (), key , artifact , ( storage_partition ,) ) return cast ( StoragePartition , storage_partition )","title":"write"},{"location":"reference/arti/partitions/","text":"Module arti.partitions None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc from datetime import date from inspect import getattr_static from typing import Any , ClassVar from arti.internal.models import Model from arti.internal.utils import classproperty , frozendict , register from arti.types import Collection , Date , Int8 , Int16 , Int32 , Int64 , Null , Type class key_component ( property ): pass class PartitionKey ( Model ): _abstract_ = True _by_type_ : \"ClassVar[dict[type[Type], type[PartitionKey]]]\" = {} default_key_components : ClassVar [ frozendict [ str , str ]] matching_type : ClassVar [ type [ Type ]] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ): if not hasattr ( cls , attr ): raise TypeError ( f \" { cls . __name__ } must set ` { attr } `\" ) if unknown := ( set ( cls . default_key_components ) - cls . key_components ): raise TypeError ( f \"Unknown key_components in { cls . __name__ } .default_key_components: { unknown } \" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty @classmethod def key_components ( cls ) -> frozenset [ str ]: return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse ' { cls . __name__ } ' from: { key_components } \" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ]: return cls . _by_type_ [ type ( type_ )] @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ): return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items ()} ) # CompositeKey is the set of named PartitionKeys that uniquely identify a single partition. CompositeKey = frozendict [ str , PartitionKey ] CompositeKeyTypes = frozendict [ str , type [ PartitionKey ]] NotPartitioned = CompositeKey () class DateKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ])) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ])) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( * [ int ( key_components [ k ]) for k in ( \"Y\" , \"m\" , \"d\" )])) return super () . from_key_components ( ** key_components ) class _IntKey ( PartitionKey ): _abstract_ = True default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) key : int @key_component def hex ( self ) -> str : return hex ( self . key ) @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ])) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ], base = 16 )) return super () . from_key_components ( ** key_components ) class Int8Key ( _IntKey ): matching_type = Int8 class Int16Key ( _IntKey ): matching_type = Int16 class Int32Key ( _IntKey ): matching_type = Int32 class Int64Key ( _IntKey ): matching_type = Int64 class NullKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"' { cls . __name__ } ' can only be used with 'None'!\" ) return cls () return super () . from_key_components ( ** key_components ) Variables CompositeKey CompositeKeyTypes NotPartitioned Classes DateKey class DateKey ( __pydantic_self__ , ** data : Any ) View Source class DateKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components ) Ancestors (in MRO) arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables Y d fingerprint iso m Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int16Key class Int16Key ( __pydantic_self__ , ** data : Any ) View Source class Int16Key ( _IntKey ): matching_type = Int16 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int32Key class Int32Key ( __pydantic_self__ , ** data : Any ) View Source class Int32Key ( _IntKey ): matching_type = Int32 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int64Key class Int64Key ( __pydantic_self__ , ** data : Any ) View Source class Int64Key ( _IntKey ): matching_type = Int64 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int8Key class Int8Key ( __pydantic_self__ , ** data : Any ) View Source class Int8Key ( _IntKey ): matching_type = Int8 Ancestors (in MRO) arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint hex Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . NullKey class NullKey ( __pydantic_self__ , ** data : Any ) View Source class NullKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components ) Ancestors (in MRO) arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config default_key_components key_components matching_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . PartitionKey class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : \"ClassVar[dict[type[Type], type[PartitionKey]]]\" = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = ( set ( cls . default_key_components ) - cls . key_components ) : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty @classmethod def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey Class variables Config key_components Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_key_components def from_key_components ( ** key_components : str ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' types_from def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } ) update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . key_component class key_component ( / , * args , ** kwargs ) View Source class key_component ( property ): pass Ancestors (in MRO) builtins.property Class variables fdel fget fset Methods deleter def deleter ( ... ) Descriptor to change the deleter on a property. getter def getter ( ... ) Descriptor to change the getter on a property. setter def setter ( ... ) Descriptor to change the setter on a property.","title":"Partitions"},{"location":"reference/arti/partitions/#module-artipartitions","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc from datetime import date from inspect import getattr_static from typing import Any , ClassVar from arti.internal.models import Model from arti.internal.utils import classproperty , frozendict , register from arti.types import Collection , Date , Int8 , Int16 , Int32 , Int64 , Null , Type class key_component ( property ): pass class PartitionKey ( Model ): _abstract_ = True _by_type_ : \"ClassVar[dict[type[Type], type[PartitionKey]]]\" = {} default_key_components : ClassVar [ frozendict [ str , str ]] matching_type : ClassVar [ type [ Type ]] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ): if not hasattr ( cls , attr ): raise TypeError ( f \" { cls . __name__ } must set ` { attr } `\" ) if unknown := ( set ( cls . default_key_components ) - cls . key_components ): raise TypeError ( f \"Unknown key_components in { cls . __name__ } .default_key_components: { unknown } \" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty @classmethod def key_components ( cls ) -> frozenset [ str ]: return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse ' { cls . __name__ } ' from: { key_components } \" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ]: return cls . _by_type_ [ type ( type_ )] @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ): return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items ()} ) # CompositeKey is the set of named PartitionKeys that uniquely identify a single partition. CompositeKey = frozendict [ str , PartitionKey ] CompositeKeyTypes = frozendict [ str , type [ PartitionKey ]] NotPartitioned = CompositeKey () class DateKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ])) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ])) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( * [ int ( key_components [ k ]) for k in ( \"Y\" , \"m\" , \"d\" )])) return super () . from_key_components ( ** key_components ) class _IntKey ( PartitionKey ): _abstract_ = True default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) key : int @key_component def hex ( self ) -> str : return hex ( self . key ) @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ])) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ], base = 16 )) return super () . from_key_components ( ** key_components ) class Int8Key ( _IntKey ): matching_type = Int8 class Int16Key ( _IntKey ): matching_type = Int16 class Int32Key ( _IntKey ): matching_type = Int32 class Int64Key ( _IntKey ): matching_type = Int64 class NullKey ( PartitionKey ): default_key_components : ClassVar [ frozendict [ str , str ]] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"' { cls . __name__ } ' can only be used with 'None'!\" ) return cls () return super () . from_key_components ( ** key_components )","title":"Module arti.partitions"},{"location":"reference/arti/partitions/#variables","text":"CompositeKey CompositeKeyTypes NotPartitioned","title":"Variables"},{"location":"reference/arti/partitions/#classes","text":"","title":"Classes"},{"location":"reference/arti/partitions/#datekey","text":"class DateKey ( __pydantic_self__ , ** data : Any ) View Source class DateKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( Y = \"Y\" , m = \"m:02\" , d = \"d:02\" ) matching_type = Date key : date @key_component def Y ( self ) -> int : return self . key . year @key_component def m ( self ) -> int : return self . key . month @key_component def d ( self ) -> int : return self . key . day @key_component def iso ( self ) -> str : return self . key . isoformat () @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components )","title":"DateKey"},{"location":"reference/arti/partitions/#ancestors-in-mro","text":"arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components","text":"def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = date . fromisoformat ( key_components [ \"key\" ] )) if names == { \"iso\" }: return cls ( key = date . fromisoformat ( key_components [ \"iso\" ] )) if names == { \"Y\" , \"m\" , \"d\" }: return cls ( key = date ( *[ int(key_components[k ] ) for k in ( \"Y\" , \"m\" , \"d\" ) ] )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables","text":"Y d fingerprint iso m","title":"Instance variables"},{"location":"reference/arti/partitions/#methods","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int16key","text":"class Int16Key ( __pydantic_self__ , ** data : Any ) View Source class Int16Key ( _IntKey ): matching_type = Int16","title":"Int16Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_1","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_1","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_1","text":"def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_1","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_1","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_1","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int32key","text":"class Int32Key ( __pydantic_self__ , ** data : Any ) View Source class Int32Key ( _IntKey ): matching_type = Int32","title":"Int32Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_2","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_2","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_2","text":"def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_2","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_2","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_2","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_2","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int64key","text":"class Int64Key ( __pydantic_self__ , ** data : Any ) View Source class Int64Key ( _IntKey ): matching_type = Int64","title":"Int64Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_3","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_3","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_3","text":"def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_3","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_3","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_3","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_3","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#int8key","text":"class Int8Key ( __pydantic_self__ , ** data : Any ) View Source class Int8Key ( _IntKey ): matching_type = Int8","title":"Int8Key"},{"location":"reference/arti/partitions/#ancestors-in-mro_4","text":"arti.partitions._IntKey arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_4","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_4","text":"def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : names = set ( key_components ) if names == { \"key\" }: return cls ( key = int ( key_components [ \"key\" ] )) if names == { \"hex\" }: return cls ( key = int ( key_components [ \"hex\" ] , base = 16 )) return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_4","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_4","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_4","text":"fingerprint hex","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_4","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#nullkey","text":"class NullKey ( __pydantic_self__ , ** data : Any ) View Source class NullKey ( PartitionKey ) : default_key_components : ClassVar [ frozendict[str, str ] ] = frozendict ( key = \"key\" ) matching_type = Null key : None = None @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components )","title":"NullKey"},{"location":"reference/arti/partitions/#ancestors-in-mro_5","text":"arti.partitions.PartitionKey arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_5","text":"Config default_key_components key_components matching_type","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_5","text":"def from_key_components ( ** key_components : str ) -> arti . partitions . PartitionKey View Source @classmethod def from_key_components ( cls , ** key_components : str ) -> PartitionKey : if set ( key_components ) == { \"key\" }: if key_components [ \"key\" ] != \"None\" : raise ValueError ( f \"'{cls.__name__}' can only be used with 'None'!\" ) return cls () return super (). from_key_components ( ** key_components )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_5","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_5","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_5","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_5","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#partitionkey","text":"class PartitionKey ( __pydantic_self__ , ** data : Any ) View Source class PartitionKey ( Model ) : _abstract_ = True _by_type_ : \"ClassVar[dict[type[Type], type[PartitionKey]]]\" = {} default_key_components : ClassVar [ frozendict[str, str ] ] matching_type : ClassVar [ type[Type ] ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return for attr in ( \"default_key_components\" , \"matching_type\" ) : if not hasattr ( cls , attr ) : raise TypeError ( f \"{cls.__name__} must set `{attr}`\" ) if unknown : = ( set ( cls . default_key_components ) - cls . key_components ) : raise TypeError ( f \"Unknown key_components in {cls.__name__}.default_key_components: {unknown}\" ) register ( cls . _by_type_ , cls . matching_type , cls ) @classproperty @classmethod def key_components ( cls ) -> frozenset [ str ] : return frozenset ( cls . __fields__ ) | frozenset ( name for name in dir ( cls ) if isinstance ( getattr_static ( cls , name ), key_component ) ) @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" ) @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ] @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"PartitionKey"},{"location":"reference/arti/partitions/#ancestors-in-mro_6","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#descendants","text":"arti.partitions.DateKey arti.partitions._IntKey arti.partitions.NullKey","title":"Descendants"},{"location":"reference/arti/partitions/#class-variables_6","text":"Config key_components","title":"Class variables"},{"location":"reference/arti/partitions/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/partitions/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/partitions/#from_key_components_6","text":"def from_key_components ( ** key_components : str ) -> 'PartitionKey' View Source @classmethod @abc . abstractmethod def from_key_components ( cls , ** key_components : str ) -> \"PartitionKey\" : raise NotImplementedError ( f \"Unable to parse '{cls.__name__}' from: {key_components}\" )","title":"from_key_components"},{"location":"reference/arti/partitions/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/partitions/#get_class_for_6","text":"def get_class_for ( type_ : arti . types . Type ) -> type [ 'PartitionKey' ] View Source @classmethod def get_class_for ( cls , type_ : Type ) -> type [ \"PartitionKey\" ] : return cls . _by_type_ [ type(type_) ]","title":"get_class_for"},{"location":"reference/arti/partitions/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/partitions/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/partitions/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/partitions/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/partitions/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/partitions/#types_from_6","text":"def types_from ( type_ : arti . types . Type ) -> 'CompositeKeyTypes' View Source @classmethod def types_from ( cls , type_ : Type ) -> \"CompositeKeyTypes\" : if not isinstance ( type_ , Collection ) : return frozendict () return frozendict ( { name : cls . get_class_for ( field ) for name , field in type_ . partition_fields . items () } )","title":"types_from"},{"location":"reference/arti/partitions/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/partitions/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/partitions/#instance-variables_6","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/partitions/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/partitions/#copy_6","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/partitions/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/partitions/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/partitions/#key_component","text":"class key_component ( / , * args , ** kwargs ) View Source class key_component ( property ): pass","title":"key_component"},{"location":"reference/arti/partitions/#ancestors-in-mro_7","text":"builtins.property","title":"Ancestors (in MRO)"},{"location":"reference/arti/partitions/#class-variables_7","text":"fdel fget fset","title":"Class variables"},{"location":"reference/arti/partitions/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/partitions/#deleter","text":"def deleter ( ... ) Descriptor to change the deleter on a property.","title":"deleter"},{"location":"reference/arti/partitions/#getter","text":"def getter ( ... ) Descriptor to change the getter on a property.","title":"getter"},{"location":"reference/arti/partitions/#setter","text":"def setter ( ... ) Descriptor to change the setter on a property.","title":"setter"},{"location":"reference/arti/producers/","text":"Module arti.producers None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections import defaultdict from collections.abc import Callable , Iterable , Iterator from inspect import Parameter , Signature , getattr_static from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Optional , TypeVar , Union , cast , get_args , get_origin , ) from arti.annotations import Annotation from arti.artifacts import Artifact , BaseArtifact , Statistic from arti.fingerprints import Fingerprint from arti.internal import wrap_exc from arti.internal.models import Model from arti.internal.type_hints import NoneType , lenient_issubclass , signature from arti.internal.utils import frozendict , ordinal from arti.partitions import CompositeKey , CompositeKeyTypes , NotPartitioned from arti.storage import StoragePartitions from arti.versions import SemVer , Version from arti.views import View def _commas ( vals : Iterable [ Any ]) -> str : return \", \" . join ([ str ( v ) for v in vals ]) # TODO: Add @validator over all (build) fields checking for an io.{read,write} handler. ArtifactViewPair = tuple [ type [ Artifact ], type [ View ]] BuildInputViews = frozendict [ str , type [ View ]] MapInputMetadata = frozendict [ str , type [ Artifact ]] OutputMetadata = tuple [ ArtifactViewPair , ... ] PartitionDependencies = frozendict [ CompositeKey , frozendict [ str , StoragePartitions ]] MapSig = Callable [ ... , PartitionDependencies ] BuildSig = Callable [ ... , Any ] ValidateSig = Callable [ ... , tuple [ bool , str ]] _T = TypeVar ( \"_T\" ) class Producer ( Model ): \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields/methods annotations : tuple [ Annotation , ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map/build/validate_outputs parameters are intended to be dynamic and set by subclasses, # however mypy doesn't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map : ClassVar [ MapSig ] build : ClassVar [ BuildSig ] if TYPE_CHECKING : validate_outputs : ClassVar [ ValidateSig ] else : @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True , \"No validation performed.\" # Internal fields/methods _abstract_ : ClassVar [ bool ] = True _fingerprint_excludes_ = frozenset ([ \"annotations\" ]) # NOTE: The following are set in __init_subclass__ _input_artifact_types_ : ClassVar [ frozendict [ str , type [ Artifact ]]] _build_sig_ : ClassVar [ Signature ] _build_input_views_ : ClassVar [ BuildInputViews ] _output_metadata_ : ClassVar [ OutputMetadata ] _map_sig_ : ClassVar [ Signature ] _map_input_metadata_ : ClassVar [ MapInputMetadata ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : with wrap_exc ( ValueError , prefix = cls . __name__ ): cls . _input_artifact_types_ = cls . _validate_fields () with wrap_exc ( ValueError , prefix = \".build\" ): ( cls . _build_sig_ , cls . _build_input_views_ , cls . _output_metadata_ , ) = cls . _validate_build_sig () with wrap_exc ( ValueError , prefix = \".validate_output\" ): cls . _validate_validate_output_sig () with wrap_exc ( ValueError , prefix = \".map\" ): cls . _map_sig_ , cls . _map_input_metadata_ = cls . _validate_map_sig () cls . _validate_no_unused_fields () @classmethod def _get_artifact_from_annotation ( cls , annotation : Any ) -> type [ Artifact ]: # Avoid importing non-interface modules at root from arti.types.python import python_type_system origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is not Annotated : return Artifact . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) annotation , * hints = args artifacts = [ hint for hint in hints if lenient_issubclass ( hint , Artifact )] if len ( artifacts ) == 0 : return Artifact . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) if len ( artifacts ) > 1 : raise ValueError ( \"multiple Artifacts set\" ) return cast ( type [ Artifact ], artifacts [ 0 ]) @classmethod def _get_view_from_annotation ( cls , annotation : Any , artifact : type [ Artifact ]) -> type [ View ]: wrap_msg = f \" { artifact . __name__ } \" if artifact . is_partitioned : wrap_msg = f \"partitions of { artifact . __name__ } \" with wrap_exc ( ValueError , prefix = f \" ( { wrap_msg } )\" ): return View . get_class_for ( annotation , validation_type = artifact . _type ) @classmethod def _validate_fields ( cls ) -> frozendict [ str , type [ Artifact ]]: # NOTE: Aside from the base producer fields, all others should be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won't interact with the \"framework\" and can't be parameters to build/map. artifact_fields = { k : v for k , v in cls . __fields__ . items () if k not in Producer . __fields__ } for name , field in artifact_fields . items (): with wrap_exc ( ValueError , prefix = f \". { name } \" ): if not ( field . default is None and field . default_factory is None and field . required ): raise ValueError ( \"field must not have a default nor be Optional.\" ) if not lenient_issubclass ( field . outer_type_ , Artifact ): raise ValueError ( f \"type hint must be an Artifact subclass, got: { field . outer_type_ } \" ) return frozendict ({ name : field . outer_type_ for name , field in artifact_fields . items ()}) @classmethod def _validate_parameters ( cls , sig : Signature , * , validator : Callable [[ str , Parameter , type [ Artifact ]], _T ] ) -> Iterator [ _T ]: if undefined_params := set ( sig . parameters ) - set ( cls . _input_artifact_types_ ): raise ValueError ( f \"the following parameter(s) must be defined as a field: { undefined_params } \" ) for name , param in sig . parameters . items (): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . annotation is param . empty : raise ValueError ( \"must have a type hint.\" ) if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_OR_KEYWORD , param . KEYWORD_ONLY ): raise ValueError ( \"must be usable as a keyword argument.\" ) artifact = cls . __fields__ [ param . name ] . outer_type_ yield validator ( name , param , artifact ) @classmethod def _validate_build_sig_return ( cls , annotation : Any , * , i : int ) -> ArtifactViewPair : with wrap_exc ( ValueError , prefix = f \" { ordinal ( i + 1 ) } return\" ): artifact = cls . _get_artifact_from_annotation ( annotation ) return artifact , cls . _get_view_from_annotation ( annotation , artifact ) @classmethod def _validate_build_sig ( cls ) -> tuple [ Signature , BuildInputViews , OutputMetadata ]: \"\"\"Validate the .build method\"\"\" if not hasattr ( cls , \"build\" ): raise ValueError ( \"must be implemented\" ) if not isinstance ( getattr_static ( cls , \"build\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) build_sig = signature ( cls . build , force_tuple_return = True , remove_owner = True ) # Validate the parameters build_input_metadata = BuildInputViews ( cls . _validate_parameters ( build_sig , validator = ( lambda name , param , artifact : ( name , cls . _get_view_from_annotation ( param . annotation , artifact ), ) ), ) ) # Validate the return definition return_annotation = build_sig . return_annotation if return_annotation is build_sig . empty : # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( \"a return value must be set with the output Artifact(s).\" ) if return_annotation == ( NoneType ,): raise ValueError ( \"missing return signature\" ) output_metadata = OutputMetadata ( cls . _validate_build_sig_return ( annotation , i = i ) for i , annotation in enumerate ( return_annotation ) ) # Validate all output Artifacts have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the # future we might be able to extend the dependency metadata to support # heterogeneous names if necessary. artifacts_by_composite_key = defaultdict [ CompositeKeyTypes , list [ type [ Artifact ]]]( list ) for ( artifact , _ ) in output_metadata : artifacts_by_composite_key [ artifact . partition_key_types ] . append ( artifact ) if len ( artifacts_by_composite_key ) != 1 : raise ValueError ( \"all output Artifacts must have the same partitioning scheme\" ) # TODO: Save off output composite_key_types return build_sig , build_input_metadata , output_metadata @classmethod def _validate_validate_output_sig ( cls ) -> None : build_output_types = [ get_args ( hint )[ 0 ] if get_origin ( hint ) is Annotated else hint for hint in cls . _build_sig_ . return_annotation ] match_build_str = f \"match the `.build` return (` { build_output_types } `)\" validate_parameters = signature ( cls . validate_outputs ) . parameters def param_matches ( param : Parameter , build_return : type ) -> bool : # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can't pass a \"Manager\" into a # function expecting an \"Employee\"), hence the lenient_issubclass has build_return as # the subtype and param.annotation as the supertype. return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow `*args: Any` or `*args: T` for `build(...) -> tuple[T, ...]` len ( validate_parameters ) == 1 and ( param := tuple ( validate_parameters . values ())[ 0 ]) . kind == param . VAR_POSITIONAL ): if not all ( param_matches ( param , output_type ) for output_type in build_output_types ): with wrap_exc ( ValueError , prefix = f \" { param . name } param\" ): raise ValueError ( f \"type hint must be `Any` or { match_build_str } \" ) else : # Otherwise, check pairwise if len ( validate_parameters ) != len ( build_output_types ): raise ValueError ( f \"must { match_build_str } \" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ): raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected := build_output_types [ i ])): raise ValueError ( f \"type hint must match the { ordinal ( i + 1 ) } `.build` return (` { expected } `)\" ) # TODO: Validate return signature? @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature , MapInputMetadata ]: \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ): partitioned_outputs = [ artifact for ( artifact , view ) in cls . _output_metadata_ if artifact . is_partitioned ] # TODO: Add runtime checking of `map` output (ie: output aligns w/ output # artifacts and such). if partitioned_outputs : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) else : def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : { name : partitions for name , partitions in kwargs . items ()}} ) # Narrow the map signature, which is validated below and used at graph build # time (via cls._map_input_metadata_) to determine what arguments to pass to # map. map . __signature__ = Signature ( # type: ignore [ Parameter ( name = name , annotation = StoragePartitions , kind = Parameter . KEYWORD_ONLY ) for name , artifact in cls . _input_artifact_types_ . items () if name in cls . _build_input_views_ ], return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) def validate_map_param ( name : str , param : Parameter , artifact : type [ Artifact ] ) -> tuple [ str , type [ Artifact ]]: # TODO: Should we add some ArtifactPartition[MyArtifact] type? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name , artifact map_input_metadata = MapInputMetadata ( cls . _validate_parameters ( map_sig , validator = validate_map_param ) ) return map_sig , map_input_metadata # TODO: Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields := set ( cls . _input_artifact_types_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ): raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: { unused_fields } \" ) # NOTE: pydantic defines .__iter__ to return `self.__dict__.items()` to support `dict(model)`, # but we want to override to support easy expansion/assignment to a Graph without `.out()` (eg: # `g.artifacts.a, g.artifacts.b = MyProducer(...)`). def __iter__ ( self ) -> Iterator [ Artifact ]: # type: ignore ret = self . out () if not isinstance ( ret , tuple ): ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected { expected_names } , got { input_names } \" ) # We only care if the *code* or *input partition contents* changed, not if the input file # paths changed (but have the same content as a prior run). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) @property def inputs ( self ) -> dict [ str , Artifact ]: return { k : getattr ( self , k ) for k in self . _input_artifact_types_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact , tuple [ Artifact , ... ]]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO: Raise a better error if the Artifacts don't have defaults set for # type/format/storage. outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \" { self . _class_key_ } .out() - expected { expected_n } arguments of ( { ret_str } ), but got: { outputs } \" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \" { self . _class_key_ } .out() { ordinal ( ord + 1 ) } argument\" ): if not isinstance ( artifact , expected_type ): raise ValueError ( f \"expected instance of { expected_type } , got { type ( artifact ) } \" ) # TODO: Validate type/format/storage/view compatibility? if artifact . producer_output is not None : raise ValueError ( f \" { artifact } is produced by { artifact . producer_output . producer } !\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord )} ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs def producer ( * , annotations : Optional [ tuple [ Annotation , ... ]] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [[ BuildSig ], type [ Producer ]]: def decorate ( build : BuildSig ) -> type [ Producer ]: nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str , Any ] = {} for param in signature ( build ) . parameters . values (): with wrap_exc ( ValueError , prefix = f \" { name } { param . name } param\" ): __annotations__ [ param . name ] = Producer . _get_artifact_from_annotation ( param . annotation ) # If overriding, set an explicit \"annotations\" hint until [1] is released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation , ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None }, ) return decorate class ProducerOutput ( Model ): producer : Producer position : int # TODO: Support named output (defaulting to artifact classname) BaseArtifact . update_forward_refs ( ProducerOutput = ProducerOutput ) Statistic . update_forward_refs ( ProducerOutput = ProducerOutput ) Artifact . update_forward_refs ( ProducerOutput = ProducerOutput ) Variables BuildInputViews MapInputMetadata PartitionDependencies TYPE_CHECKING Functions producer def producer ( * , annotations : Optional [ tuple [ arti . annotations . Annotation , ... ]] = None , map : Optional [ collections . abc . Callable [ ... , arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]]]]] = None , name : Optional [ str ] = None , validate_outputs : Optional [ collections . abc . Callable [ ... , tuple [ bool , str ]]] = None , version : Optional [ arti . versions . Version ] = None ) -> collections . abc . Callable [[ collections . abc . Callable [ ... , typing . Any ]], type [ arti . producers . Producer ]] View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : __annotations__ [ param.name ] = Producer . _get_artifact_from_annotation ( param . annotation ) # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate Classes ArtifactViewPair class ArtifactViewPair ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.tuple Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. BuildSig class BuildSig ( / , * args , ** kwargs ) Ancestors (in MRO) collections.abc.Callable MapSig class MapSig ( / , * args , ** kwargs ) Ancestors (in MRO) collections.abc.Callable OutputMetadata class OutputMetadata ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.tuple Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. Producer class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_types_: ClassVar[frozendict[str, type[Artifact]]] _build_sig_: ClassVar[Signature] _build_input_views_: ClassVar[BuildInputViews] _output_metadata_: ClassVar[OutputMetadata] _map_sig_: ClassVar[Signature] _map_input_metadata_: ClassVar[MapInputMetadata] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_types_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_input_views_, cls._output_metadata_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_input_metadata_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _get_artifact_from_annotation(cls, annotation: Any) -> type[Artifact]: # Avoid importing non-interface modules at root from arti.types.python import python_type_system origin, args = get_origin(annotation), get_args(annotation) if origin is not Annotated: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) annotation, *hints = args artifacts = [hint for hint in hints if lenient_issubclass(hint, Artifact)] if len(artifacts) == 0: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) if len(artifacts) > 1: raise ValueError(\"multiple Artifacts set\") return cast(type[Artifact], artifacts[0]) @classmethod def _get_view_from_annotation(cls, annotation: Any, artifact: type[Artifact]) -> type[View]: wrap_msg = f\"{artifact.__name__}\" if artifact.is_partitioned: wrap_msg = f\"partitions of {artifact.__name__}\" with wrap_exc(ValueError, prefix=f\" ({wrap_msg})\"): return View.get_class_for(annotation, validation_type=artifact._type) @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter, type[Artifact]], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_types_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") artifact = cls.__fields__[param.name].outer_type_ yield validator(name, param, artifact) @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> ArtifactViewPair: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): artifact = cls._get_artifact_from_annotation(annotation) return artifact, cls._get_view_from_annotation(annotation, artifact) @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputViews, OutputMetadata]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_input_metadata = BuildInputViews( cls._validate_parameters( build_sig, validator=( lambda name, param, artifact: ( name, cls._get_view_from_annotation(param.annotation, artifact), ) ), ) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") output_metadata = OutputMetadata( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all output Artifacts have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the # future we might be able to extend the dependency metadata to support # heterogeneous names if necessary. artifacts_by_composite_key = defaultdict[CompositeKeyTypes, list[type[Artifact]]](list) for (artifact, _) in output_metadata: artifacts_by_composite_key[artifact.partition_key_types].append(artifact) if len(artifacts_by_composite_key) != 1: raise ValueError(\"all output Artifacts must have the same partitioning scheme\") # TODO: Save off output composite_key_types return build_sig, build_input_metadata, output_metadata @classmethod def _validate_validate_output_sig(cls) -> None: build_output_types = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_types}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_type ) for output_type in build_output_types ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_types ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_types [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputMetadata ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : partitioned_outputs = [ artifact for (artifact, view) in cls._output_metadata_ if artifact.is_partitioned ] # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if partitioned_outputs : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) else : def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : { name : partitions for name , partitions in kwargs . items () }} ) # Narrow the map signature , which is validated below and used at graph build # time ( via cls . _map_input_metadata_ ) to determine what arguments to pass to # map . map . __signature__ = Signature ( # type : ignore [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_types_.items() if name in cls._build_input_views_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) def validate_map_param ( name : str , param : Parameter , artifact : type [ Artifact ] ) -> tuple [ str, type[Artifact ] ]: # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name , artifact map_input_metadata = MapInputMetadata ( cls . _validate_parameters ( map_sig , validator = validate_map_param ) ) return map_sig , map_input_metadata # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_types_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_types_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_outputs def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\" Instance variables fingerprint inputs Methods compute_input_fingerprint def compute_input_fingerprint ( self , dependency_partitions : arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]] ) -> arti . fingerprints . Fingerprint View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . out def out ( self , * outputs : arti . artifacts . Artifact ) -> Union [ arti . artifacts . Artifact , tuple [ arti . artifacts . Artifact , ... ]] Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs ProducerOutput class ProducerOutput ( __pydantic_self__ , ** data : Any ) View Source class ProducerOutput ( Model ): producer: Producer position: int # TODO: Support named output (defaulting to artifact classname) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . ValidateSig class ValidateSig ( / , * args , ** kwargs ) Ancestors (in MRO) collections.abc.Callable","title":"Producers"},{"location":"reference/arti/producers/#module-artiproducers","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections import defaultdict from collections.abc import Callable , Iterable , Iterator from inspect import Parameter , Signature , getattr_static from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Optional , TypeVar , Union , cast , get_args , get_origin , ) from arti.annotations import Annotation from arti.artifacts import Artifact , BaseArtifact , Statistic from arti.fingerprints import Fingerprint from arti.internal import wrap_exc from arti.internal.models import Model from arti.internal.type_hints import NoneType , lenient_issubclass , signature from arti.internal.utils import frozendict , ordinal from arti.partitions import CompositeKey , CompositeKeyTypes , NotPartitioned from arti.storage import StoragePartitions from arti.versions import SemVer , Version from arti.views import View def _commas ( vals : Iterable [ Any ]) -> str : return \", \" . join ([ str ( v ) for v in vals ]) # TODO: Add @validator over all (build) fields checking for an io.{read,write} handler. ArtifactViewPair = tuple [ type [ Artifact ], type [ View ]] BuildInputViews = frozendict [ str , type [ View ]] MapInputMetadata = frozendict [ str , type [ Artifact ]] OutputMetadata = tuple [ ArtifactViewPair , ... ] PartitionDependencies = frozendict [ CompositeKey , frozendict [ str , StoragePartitions ]] MapSig = Callable [ ... , PartitionDependencies ] BuildSig = Callable [ ... , Any ] ValidateSig = Callable [ ... , tuple [ bool , str ]] _T = TypeVar ( \"_T\" ) class Producer ( Model ): \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields/methods annotations : tuple [ Annotation , ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map/build/validate_outputs parameters are intended to be dynamic and set by subclasses, # however mypy doesn't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map : ClassVar [ MapSig ] build : ClassVar [ BuildSig ] if TYPE_CHECKING : validate_outputs : ClassVar [ ValidateSig ] else : @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True , \"No validation performed.\" # Internal fields/methods _abstract_ : ClassVar [ bool ] = True _fingerprint_excludes_ = frozenset ([ \"annotations\" ]) # NOTE: The following are set in __init_subclass__ _input_artifact_types_ : ClassVar [ frozendict [ str , type [ Artifact ]]] _build_sig_ : ClassVar [ Signature ] _build_input_views_ : ClassVar [ BuildInputViews ] _output_metadata_ : ClassVar [ OutputMetadata ] _map_sig_ : ClassVar [ Signature ] _map_input_metadata_ : ClassVar [ MapInputMetadata ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : with wrap_exc ( ValueError , prefix = cls . __name__ ): cls . _input_artifact_types_ = cls . _validate_fields () with wrap_exc ( ValueError , prefix = \".build\" ): ( cls . _build_sig_ , cls . _build_input_views_ , cls . _output_metadata_ , ) = cls . _validate_build_sig () with wrap_exc ( ValueError , prefix = \".validate_output\" ): cls . _validate_validate_output_sig () with wrap_exc ( ValueError , prefix = \".map\" ): cls . _map_sig_ , cls . _map_input_metadata_ = cls . _validate_map_sig () cls . _validate_no_unused_fields () @classmethod def _get_artifact_from_annotation ( cls , annotation : Any ) -> type [ Artifact ]: # Avoid importing non-interface modules at root from arti.types.python import python_type_system origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is not Annotated : return Artifact . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) annotation , * hints = args artifacts = [ hint for hint in hints if lenient_issubclass ( hint , Artifact )] if len ( artifacts ) == 0 : return Artifact . from_type ( python_type_system . to_artigraph ( annotation , hints = {})) if len ( artifacts ) > 1 : raise ValueError ( \"multiple Artifacts set\" ) return cast ( type [ Artifact ], artifacts [ 0 ]) @classmethod def _get_view_from_annotation ( cls , annotation : Any , artifact : type [ Artifact ]) -> type [ View ]: wrap_msg = f \" { artifact . __name__ } \" if artifact . is_partitioned : wrap_msg = f \"partitions of { artifact . __name__ } \" with wrap_exc ( ValueError , prefix = f \" ( { wrap_msg } )\" ): return View . get_class_for ( annotation , validation_type = artifact . _type ) @classmethod def _validate_fields ( cls ) -> frozendict [ str , type [ Artifact ]]: # NOTE: Aside from the base producer fields, all others should be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won't interact with the \"framework\" and can't be parameters to build/map. artifact_fields = { k : v for k , v in cls . __fields__ . items () if k not in Producer . __fields__ } for name , field in artifact_fields . items (): with wrap_exc ( ValueError , prefix = f \". { name } \" ): if not ( field . default is None and field . default_factory is None and field . required ): raise ValueError ( \"field must not have a default nor be Optional.\" ) if not lenient_issubclass ( field . outer_type_ , Artifact ): raise ValueError ( f \"type hint must be an Artifact subclass, got: { field . outer_type_ } \" ) return frozendict ({ name : field . outer_type_ for name , field in artifact_fields . items ()}) @classmethod def _validate_parameters ( cls , sig : Signature , * , validator : Callable [[ str , Parameter , type [ Artifact ]], _T ] ) -> Iterator [ _T ]: if undefined_params := set ( sig . parameters ) - set ( cls . _input_artifact_types_ ): raise ValueError ( f \"the following parameter(s) must be defined as a field: { undefined_params } \" ) for name , param in sig . parameters . items (): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . annotation is param . empty : raise ValueError ( \"must have a type hint.\" ) if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_OR_KEYWORD , param . KEYWORD_ONLY ): raise ValueError ( \"must be usable as a keyword argument.\" ) artifact = cls . __fields__ [ param . name ] . outer_type_ yield validator ( name , param , artifact ) @classmethod def _validate_build_sig_return ( cls , annotation : Any , * , i : int ) -> ArtifactViewPair : with wrap_exc ( ValueError , prefix = f \" { ordinal ( i + 1 ) } return\" ): artifact = cls . _get_artifact_from_annotation ( annotation ) return artifact , cls . _get_view_from_annotation ( annotation , artifact ) @classmethod def _validate_build_sig ( cls ) -> tuple [ Signature , BuildInputViews , OutputMetadata ]: \"\"\"Validate the .build method\"\"\" if not hasattr ( cls , \"build\" ): raise ValueError ( \"must be implemented\" ) if not isinstance ( getattr_static ( cls , \"build\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) build_sig = signature ( cls . build , force_tuple_return = True , remove_owner = True ) # Validate the parameters build_input_metadata = BuildInputViews ( cls . _validate_parameters ( build_sig , validator = ( lambda name , param , artifact : ( name , cls . _get_view_from_annotation ( param . annotation , artifact ), ) ), ) ) # Validate the return definition return_annotation = build_sig . return_annotation if return_annotation is build_sig . empty : # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError ( \"a return value must be set with the output Artifact(s).\" ) if return_annotation == ( NoneType ,): raise ValueError ( \"missing return signature\" ) output_metadata = OutputMetadata ( cls . _validate_build_sig_return ( annotation , i = i ) for i , annotation in enumerate ( return_annotation ) ) # Validate all output Artifacts have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the # future we might be able to extend the dependency metadata to support # heterogeneous names if necessary. artifacts_by_composite_key = defaultdict [ CompositeKeyTypes , list [ type [ Artifact ]]]( list ) for ( artifact , _ ) in output_metadata : artifacts_by_composite_key [ artifact . partition_key_types ] . append ( artifact ) if len ( artifacts_by_composite_key ) != 1 : raise ValueError ( \"all output Artifacts must have the same partitioning scheme\" ) # TODO: Save off output composite_key_types return build_sig , build_input_metadata , output_metadata @classmethod def _validate_validate_output_sig ( cls ) -> None : build_output_types = [ get_args ( hint )[ 0 ] if get_origin ( hint ) is Annotated else hint for hint in cls . _build_sig_ . return_annotation ] match_build_str = f \"match the `.build` return (` { build_output_types } `)\" validate_parameters = signature ( cls . validate_outputs ) . parameters def param_matches ( param : Parameter , build_return : type ) -> bool : # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can't pass a \"Manager\" into a # function expecting an \"Employee\"), hence the lenient_issubclass has build_return as # the subtype and param.annotation as the supertype. return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow `*args: Any` or `*args: T` for `build(...) -> tuple[T, ...]` len ( validate_parameters ) == 1 and ( param := tuple ( validate_parameters . values ())[ 0 ]) . kind == param . VAR_POSITIONAL ): if not all ( param_matches ( param , output_type ) for output_type in build_output_types ): with wrap_exc ( ValueError , prefix = f \" { param . name } param\" ): raise ValueError ( f \"type hint must be `Any` or { match_build_str } \" ) else : # Otherwise, check pairwise if len ( validate_parameters ) != len ( build_output_types ): raise ValueError ( f \"must { match_build_str } \" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()): with wrap_exc ( ValueError , prefix = f \" { name } param\" ): if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ): raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected := build_output_types [ i ])): raise ValueError ( f \"type hint must match the { ordinal ( i + 1 ) } `.build` return (` { expected } `)\" ) # TODO: Validate return signature? @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature , MapInputMetadata ]: \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ): partitioned_outputs = [ artifact for ( artifact , view ) in cls . _output_metadata_ if artifact . is_partitioned ] # TODO: Add runtime checking of `map` output (ie: output aligns w/ output # artifacts and such). if partitioned_outputs : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) else : def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : { name : partitions for name , partitions in kwargs . items ()}} ) # Narrow the map signature, which is validated below and used at graph build # time (via cls._map_input_metadata_) to determine what arguments to pass to # map. map . __signature__ = Signature ( # type: ignore [ Parameter ( name = name , annotation = StoragePartitions , kind = Parameter . KEYWORD_ONLY ) for name , artifact in cls . _input_artifact_types_ . items () if name in cls . _build_input_views_ ], return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )): raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) def validate_map_param ( name : str , param : Parameter , artifact : type [ Artifact ] ) -> tuple [ str , type [ Artifact ]]: # TODO: Should we add some ArtifactPartition[MyArtifact] type? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name , artifact map_input_metadata = MapInputMetadata ( cls . _validate_parameters ( map_sig , validator = validate_map_param ) ) return map_sig , map_input_metadata # TODO: Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields := set ( cls . _input_artifact_types_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ): raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: { unused_fields } \" ) # NOTE: pydantic defines .__iter__ to return `self.__dict__.items()` to support `dict(model)`, # but we want to override to support easy expansion/assignment to a Graph without `.out()` (eg: # `g.artifacts.a, g.artifacts.b = MyProducer(...)`). def __iter__ ( self ) -> Iterator [ Artifact ]: # type: ignore ret = self . out () if not isinstance ( ret , tuple ): ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected { expected_names } , got { input_names } \" ) # We only care if the *code* or *input partition contents* changed, not if the input file # paths changed (but have the same content as a prior run). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) @property def inputs ( self ) -> dict [ str , Artifact ]: return { k : getattr ( self , k ) for k in self . _input_artifact_types_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact , tuple [ Artifact , ... ]]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO: Raise a better error if the Artifacts don't have defaults set for # type/format/storage. outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \" { self . _class_key_ } .out() - expected { expected_n } arguments of ( { ret_str } ), but got: { outputs } \" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \" { self . _class_key_ } .out() { ordinal ( ord + 1 ) } argument\" ): if not isinstance ( artifact , expected_type ): raise ValueError ( f \"expected instance of { expected_type } , got { type ( artifact ) } \" ) # TODO: Validate type/format/storage/view compatibility? if artifact . producer_output is not None : raise ValueError ( f \" { artifact } is produced by { artifact . producer_output . producer } !\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord )} ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs def producer ( * , annotations : Optional [ tuple [ Annotation , ... ]] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [[ BuildSig ], type [ Producer ]]: def decorate ( build : BuildSig ) -> type [ Producer ]: nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str , Any ] = {} for param in signature ( build ) . parameters . values (): with wrap_exc ( ValueError , prefix = f \" { name } { param . name } param\" ): __annotations__ [ param . name ] = Producer . _get_artifact_from_annotation ( param . annotation ) # If overriding, set an explicit \"annotations\" hint until [1] is released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation , ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None }, ) return decorate class ProducerOutput ( Model ): producer : Producer position : int # TODO: Support named output (defaulting to artifact classname) BaseArtifact . update_forward_refs ( ProducerOutput = ProducerOutput ) Statistic . update_forward_refs ( ProducerOutput = ProducerOutput ) Artifact . update_forward_refs ( ProducerOutput = ProducerOutput )","title":"Module arti.producers"},{"location":"reference/arti/producers/#variables","text":"BuildInputViews MapInputMetadata PartitionDependencies TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/producers/#functions","text":"","title":"Functions"},{"location":"reference/arti/producers/#producer","text":"def producer ( * , annotations : Optional [ tuple [ arti . annotations . Annotation , ... ]] = None , map : Optional [ collections . abc . Callable [ ... , arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]]]]] = None , name : Optional [ str ] = None , validate_outputs : Optional [ collections . abc . Callable [ ... , tuple [ bool , str ]]] = None , version : Optional [ arti . versions . Version ] = None ) -> collections . abc . Callable [[ collections . abc . Callable [ ... , typing . Any ]], type [ arti . producers . Producer ]] View Source def producer ( * , annotations : Optional [ tuple[Annotation, ... ] ] = None , map : Optional [ MapSig ] = None , name : Optional [ str ] = None , validate_outputs : Optional [ ValidateSig ] = None , version : Optional [ Version ] = None , ) -> Callable [ [BuildSig ] , type [ Producer ] ]: def decorate ( build : BuildSig ) -> type [ Producer ] : nonlocal name name = build . __name__ if name is None else name __annotations__ : dict [ str, Any ] = {} for param in signature ( build ). parameters . values () : with wrap_exc ( ValueError , prefix = f \"{name} {param.name} param\" ) : __annotations__ [ param.name ] = Producer . _get_artifact_from_annotation ( param . annotation ) # If overriding , set an explicit \"annotations\" hint until [ 1 ] is released . # # 1 : https : // github . com / samuelcolvin / pydantic / pull / 3018 if annotations : __annotations__ [ \"annotations\" ] = tuple [ Annotation, ... ] if version : __annotations__ [ \"version\" ] = Version return type ( name , ( Producer ,), { k : v for k , v in { \"__annotations__\" : __annotations__ , \"annotations\" : annotations , \"build\" : staticmethod ( build ), \"map\" : None if map is None else staticmethod ( map ), \"validate_outputs\" : ( None if validate_outputs is None else staticmethod ( validate_outputs ) ), \"version\" : version , } . items () if v is not None } , ) return decorate","title":"producer"},{"location":"reference/arti/producers/#classes","text":"","title":"Classes"},{"location":"reference/arti/producers/#artifactviewpair","text":"class ArtifactViewPair ( / , * args , ** kwargs )","title":"ArtifactViewPair"},{"location":"reference/arti/producers/#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#methods","text":"","title":"Methods"},{"location":"reference/arti/producers/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/arti/producers/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/arti/producers/#buildsig","text":"class BuildSig ( / , * args , ** kwargs )","title":"BuildSig"},{"location":"reference/arti/producers/#ancestors-in-mro_1","text":"collections.abc.Callable","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#mapsig","text":"class MapSig ( / , * args , ** kwargs )","title":"MapSig"},{"location":"reference/arti/producers/#ancestors-in-mro_2","text":"collections.abc.Callable","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#outputmetadata","text":"class OutputMetadata ( / , * args , ** kwargs )","title":"OutputMetadata"},{"location":"reference/arti/producers/#ancestors-in-mro_3","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/producers/#count_1","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/arti/producers/#index_1","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/arti/producers/#producer_1","text":"class Producer ( __pydantic_self__ , ** data : Any ) View Source class Producer ( Model ) : \"\"\"A Producer is a task that builds one or more Artifacts.\"\"\" # User fields / methods annotations : tuple [ Annotation, ... ] = () version : Version = SemVer ( major = 0 , minor = 0 , patch = 1 ) # The map / build / validate_outputs parameters are intended to be dynamic and set by subclasses , # however mypy doesn 't like the \"incompatible\" signature on subclasses if actually defined here # (nor support ParamSpec yet). `map` is generated during subclassing if not set, `build` is # required, and `validate_outputs` defaults to no-op checks (hence is the only one with a # provided method). # # These must be @classmethods or @staticmethods. map: ClassVar[MapSig] build: ClassVar[BuildSig] if TYPE_CHECKING: validate_outputs: ClassVar[ValidateSig] else: @staticmethod def validate_outputs(*outputs: Any) -> Union[bool, tuple[bool, str]]: \"\"\"Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\"\" return True, \"No validation performed.\" # Internal fields/methods _abstract_: ClassVar[bool] = True _fingerprint_excludes_ = frozenset([\"annotations\"]) # NOTE: The following are set in __init_subclass__ _input_artifact_types_: ClassVar[frozendict[str, type[Artifact]]] _build_sig_: ClassVar[Signature] _build_input_views_: ClassVar[BuildInputViews] _output_metadata_: ClassVar[OutputMetadata] _map_sig_: ClassVar[Signature] _map_input_metadata_: ClassVar[MapInputMetadata] @classmethod def __init_subclass__(cls, **kwargs: Any) -> None: super().__init_subclass__(**kwargs) if not cls._abstract_: with wrap_exc(ValueError, prefix=cls.__name__): cls._input_artifact_types_ = cls._validate_fields() with wrap_exc(ValueError, prefix=\".build\"): ( cls._build_sig_, cls._build_input_views_, cls._output_metadata_, ) = cls._validate_build_sig() with wrap_exc(ValueError, prefix=\".validate_output\"): cls._validate_validate_output_sig() with wrap_exc(ValueError, prefix=\".map\"): cls._map_sig_, cls._map_input_metadata_ = cls._validate_map_sig() cls._validate_no_unused_fields() @classmethod def _get_artifact_from_annotation(cls, annotation: Any) -> type[Artifact]: # Avoid importing non-interface modules at root from arti.types.python import python_type_system origin, args = get_origin(annotation), get_args(annotation) if origin is not Annotated: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) annotation, *hints = args artifacts = [hint for hint in hints if lenient_issubclass(hint, Artifact)] if len(artifacts) == 0: return Artifact.from_type(python_type_system.to_artigraph(annotation, hints={})) if len(artifacts) > 1: raise ValueError(\"multiple Artifacts set\") return cast(type[Artifact], artifacts[0]) @classmethod def _get_view_from_annotation(cls, annotation: Any, artifact: type[Artifact]) -> type[View]: wrap_msg = f\"{artifact.__name__}\" if artifact.is_partitioned: wrap_msg = f\"partitions of {artifact.__name__}\" with wrap_exc(ValueError, prefix=f\" ({wrap_msg})\"): return View.get_class_for(annotation, validation_type=artifact._type) @classmethod def _validate_fields(cls) -> frozendict[str, type[Artifact]]: # NOTE: Aside from the base producer fields, all others should be Artifacts. # # Users can set additional class attributes, but they must be properly hinted as ClassVars. # These won' t interact with the \"framework\" and can 't be parameters to build/map. artifact_fields = {k: v for k, v in cls.__fields__.items() if k not in Producer.__fields__} for name, field in artifact_fields.items(): with wrap_exc(ValueError, prefix=f\".{name}\"): if not (field.default is None and field.default_factory is None and field.required): raise ValueError(\"field must not have a default nor be Optional.\") if not lenient_issubclass(field.outer_type_, Artifact): raise ValueError( f\"type hint must be an Artifact subclass, got: {field.outer_type_}\" ) return frozendict({name: field.outer_type_ for name, field in artifact_fields.items()}) @classmethod def _validate_parameters( cls, sig: Signature, *, validator: Callable[[str, Parameter, type[Artifact]], _T] ) -> Iterator[_T]: if undefined_params := set(sig.parameters) - set(cls._input_artifact_types_): raise ValueError( f\"the following parameter(s) must be defined as a field: {undefined_params}\" ) for name, param in sig.parameters.items(): with wrap_exc(ValueError, prefix=f\" {name} param\"): if param.annotation is param.empty: raise ValueError(\"must have a type hint.\") if param.default is not param.empty: raise ValueError(\"must not have a default.\") if param.kind not in (param.POSITIONAL_OR_KEYWORD, param.KEYWORD_ONLY): raise ValueError(\"must be usable as a keyword argument.\") artifact = cls.__fields__[param.name].outer_type_ yield validator(name, param, artifact) @classmethod def _validate_build_sig_return(cls, annotation: Any, *, i: int) -> ArtifactViewPair: with wrap_exc(ValueError, prefix=f\" {ordinal(i+1)} return\"): artifact = cls._get_artifact_from_annotation(annotation) return artifact, cls._get_view_from_annotation(annotation, artifact) @classmethod def _validate_build_sig(cls) -> tuple[Signature, BuildInputViews, OutputMetadata]: \"\"\"Validate the .build method\"\"\" if not hasattr(cls, \"build\"): raise ValueError(\"must be implemented\") if not isinstance(getattr_static(cls, \"build\"), (classmethod, staticmethod)): raise ValueError(\"must be a @classmethod or @staticmethod\") build_sig = signature(cls.build, force_tuple_return=True, remove_owner=True) # Validate the parameters build_input_metadata = BuildInputViews( cls._validate_parameters( build_sig, validator=( lambda name, param, artifact: ( name, cls._get_view_from_annotation(param.annotation, artifact), ) ), ) ) # Validate the return definition return_annotation = build_sig.return_annotation if return_annotation is build_sig.empty: # TODO: \"side effect\" Producers: https://github.com/artigraph/artigraph/issues/11 raise ValueError(\"a return value must be set with the output Artifact(s).\") if return_annotation == (NoneType,): raise ValueError(\"missing return signature\") output_metadata = OutputMetadata( cls._validate_build_sig_return(annotation, i=i) for i, annotation in enumerate(return_annotation) ) # Validate all output Artifacts have equivalent partitioning schemes. # # We currently require the partition key type *and* name to match, but in the # future we might be able to extend the dependency metadata to support # heterogeneous names if necessary. artifacts_by_composite_key = defaultdict[CompositeKeyTypes, list[type[Artifact]]](list) for (artifact, _) in output_metadata: artifacts_by_composite_key[artifact.partition_key_types].append(artifact) if len(artifacts_by_composite_key) != 1: raise ValueError(\"all output Artifacts must have the same partitioning scheme\") # TODO: Save off output composite_key_types return build_sig, build_input_metadata, output_metadata @classmethod def _validate_validate_output_sig(cls) -> None: build_output_types = [ get_args(hint)[0] if get_origin(hint) is Annotated else hint for hint in cls._build_sig_.return_annotation ] match_build_str = f\"match the `.build` return (`{build_output_types}`)\" validate_parameters = signature(cls.validate_outputs).parameters def param_matches(param: Parameter, build_return: type) -> bool: # Skip checking non-hinted parameters to allow lambdas. # # NOTE: Parameter type hints are *contravariant* (you can' t pass a \"Manager\" into a # function expecting an \"Employee\" ), hence the lenient_issubclass has build_return as # the subtype and param . annotation as the supertype . return param . annotation is param . empty or lenient_issubclass ( build_return , param . annotation ) if ( # Allow ` * args : Any ` or ` * args : T ` for ` build (...) -> tuple [ T, ... ] ` len ( validate_parameters ) == 1 and ( param : = tuple ( validate_parameters . values ()) [ 0 ] ). kind == param . VAR_POSITIONAL ) : if not all ( param_matches ( param , output_type ) for output_type in build_output_types ) : with wrap_exc ( ValueError , prefix = f \" {param.name} param\" ) : raise ValueError ( f \"type hint must be `Any` or {match_build_str}\" ) else : # Otherwise , check pairwise if len ( validate_parameters ) != len ( build_output_types ) : raise ValueError ( f \"must {match_build_str}\" ) for i , ( name , param ) in enumerate ( validate_parameters . items ()) : with wrap_exc ( ValueError , prefix = f \" {name} param\" ) : if param . default is not param . empty : raise ValueError ( \"must not have a default.\" ) if param . kind not in ( param . POSITIONAL_ONLY , param . POSITIONAL_OR_KEYWORD ) : raise ValueError ( \"must be usable as a positional argument.\" ) if not param_matches ( param , ( expected : = build_output_types [ i ] )) : raise ValueError ( f \"type hint must match the {ordinal(i+1)} `.build` return (`{expected}`)\" ) # TODO : Validate return signature ? @classmethod def _validate_map_sig ( cls ) -> tuple [ Signature, MapInputMetadata ] : \"\"\"Validate partitioned Artifacts and the .map method\"\"\" if not hasattr ( cls , \"map\" ) : partitioned_outputs = [ artifact for (artifact, view) in cls._output_metadata_ if artifact.is_partitioned ] # TODO : Add runtime checking of ` map ` output ( ie : output aligns w / output # artifacts and such ). if partitioned_outputs : raise ValueError ( \"must be implemented when the `build` outputs are partitioned\" ) else : def map ( ** kwargs : StoragePartitions ) -> PartitionDependencies : return PartitionDependencies ( { NotPartitioned : { name : partitions for name , partitions in kwargs . items () }} ) # Narrow the map signature , which is validated below and used at graph build # time ( via cls . _map_input_metadata_ ) to determine what arguments to pass to # map . map . __signature__ = Signature ( # type : ignore [ Parameter(name=name, annotation=StoragePartitions, kind=Parameter.KEYWORD_ONLY) for name, artifact in cls._input_artifact_types_.items() if name in cls._build_input_views_ ] , return_annotation = PartitionDependencies , ) cls . map = cast ( MapSig , staticmethod ( map )) if not isinstance ( getattr_static ( cls , \"map\" ), ( classmethod , staticmethod )) : raise ValueError ( \"must be a @classmethod or @staticmethod\" ) map_sig = signature ( cls . map ) def validate_map_param ( name : str , param : Parameter , artifact : type [ Artifact ] ) -> tuple [ str, type[Artifact ] ]: # TODO : Should we add some ArtifactPartition [ MyArtifact ] type ? if param . annotation != StoragePartitions : raise ValueError ( \"type hint must be `StoragePartitions`\" ) return name , artifact map_input_metadata = MapInputMetadata ( cls . _validate_parameters ( map_sig , validator = validate_map_param ) ) return map_sig , map_input_metadata # TODO : Verify map output hint matches TBD spec @classmethod def _validate_no_unused_fields ( cls ) -> None : if unused_fields : = set ( cls . _input_artifact_types_ ) - ( set ( cls . _build_sig_ . parameters ) | set ( cls . _map_sig_ . parameters ) ) : raise ValueError ( f \"the following fields aren't used in `.build` or `.map`: {unused_fields}\" ) # NOTE : pydantic defines . __iter__ to return ` self . __dict__ . items () ` to support ` dict ( model ) ` , # but we want to override to support easy expansion / assignment to a Graph without ` . out () ` ( eg : # ` g . artifacts . a , g . artifacts . b = MyProducer (...) ` ). def __iter__ ( self ) -> Iterator [ Artifact ] : # type : ignore ret = self . out () if not isinstance ( ret , tuple ) : ret = ( ret ,) return iter ( ret ) def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str, StoragePartitions ] ) -> Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ). combine ( self . version . fingerprint , * ( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), ) @property def inputs ( self ) -> dict [ str, Artifact ] : return { k : getattr ( self , k ) for k in self . _input_artifact_types_ } def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"Producer"},{"location":"reference/arti/producers/#ancestors-in-mro_4","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/producers/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/producers/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/producers/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/producers/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/producers/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/producers/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/producers/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/producers/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/producers/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/producers/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/producers/#validate_outputs","text":"def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] Validate the Producer.build outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of build will be passed in as it was returned, for example: def build(...): return 1, 2 will result in validate_outputs(1, 2) . NOTE: validate_outputs is a stopgap until Statistics and Thresholds are fully implemented. View Source @staticmethod def validate_outputs ( * outputs : Any ) -> Union [ bool , tuple [ bool , str ]] : \" \"\" Validate the `Producer.build` outputs, returning the status and a message. The validation status is applied to all outputs. If validation does not pass, the outputs will not be written to storage to prevent checkpointing the output. In the future, we may still write the data to ease debugging, but track validation status in the Backend (preventing downstream use). The arguments must not be mutated. The parameters must be usable with positional argument. The output of `build` will be passed in as it was returned, for example: `def build(...): return 1, 2` will result in `validate_outputs(1, 2)`. NOTE: `validate_outputs` is a stopgap until Statistics and Thresholds are fully implemented. \"\" \" return True , \"No validation performed.\"","title":"validate_outputs"},{"location":"reference/arti/producers/#instance-variables","text":"fingerprint inputs","title":"Instance variables"},{"location":"reference/arti/producers/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/producers/#compute_input_fingerprint","text":"def compute_input_fingerprint ( self , dependency_partitions : arti . internal . utils . frozendict [ str , tuple [ arti . storage . StoragePartition , ... ]] ) -> arti . fingerprints . Fingerprint View Source def compute_input_fingerprint ( self , dependency_partitions : frozendict [ str , StoragePartitions ] ) - > Fingerprint : input_names = set ( dependency_partitions ) expected_names = set ( self . _build_input_views_ ) if input_names != expected_names : raise ValueError ( f \"Mismatched dependency inputs; expected {expected_names}, got {input_names}\" ) # We only care if the * code * or * input partition contents * changed , not if the input file # paths changed ( but have the same content as a prior run ). return Fingerprint . from_string ( self . _class_key_ ) . combine ( self . version . fingerprint , *( partition . content_fingerprint for name , partitions in dependency_partitions . items () for partition in partitions ), )","title":"compute_input_fingerprint"},{"location":"reference/arti/producers/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/producers/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/producers/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/producers/#out","text":"def out ( self , * outputs : arti . artifacts . Artifact ) -> Union [ arti . artifacts . Artifact , tuple [ arti . artifacts . Artifact , ... ]] Configure the output Artifacts this Producer will build. The arguments are matched to the Producer.build return signature in order. View Source def out ( self , * outputs : Artifact ) -> Union [ Artifact, tuple[Artifact, ... ] ]: \"\"\"Configure the output Artifacts this Producer will build. The arguments are matched to the `Producer.build` return signature in order. \"\"\" if not outputs : # TODO : Raise a better error if the Artifacts don ' t have defaults set for # type / format / storage . outputs = tuple ( artifact () for ( artifact , _ ) in self . _output_metadata_ ) passed_n , expected_n = len ( outputs ), len ( self . _build_sig_ . return_annotation ) if passed_n != expected_n : ret_str = _commas ( self . _build_sig_ . return_annotation ) raise ValueError ( f \"{self._class_key_}.out() - expected {expected_n} arguments of ({ret_str}), but got: {outputs}\" ) def validate ( artifact : Artifact , * , ord : int ) -> Artifact : ( expected_type , _ ) = self . _output_metadata_ [ ord ] with wrap_exc ( ValueError , prefix = f \"{self._class_key_}.out() {ordinal(ord+1)} argument\" ) : if not isinstance ( artifact , expected_type ) : raise ValueError ( f \"expected instance of {expected_type}, got {type(artifact)}\" ) # TODO : Validate type / format / storage / view compatibility ? if artifact . producer_output is not None : raise ValueError ( f \"{artifact} is produced by {artifact.producer_output.producer}!\" ) return artifact . copy ( update = { \"producer_output\" : ProducerOutput ( producer = self , position = ord ) } ) outputs = tuple ( validate ( artifact , ord = i ) for i , artifact in enumerate ( outputs )) if len ( outputs ) == 1 : return outputs [ 0 ] return outputs","title":"out"},{"location":"reference/arti/producers/#produceroutput","text":"class ProducerOutput ( __pydantic_self__ , ** data : Any ) View Source class ProducerOutput ( Model ): producer: Producer position: int # TODO: Support named output (defaulting to artifact classname)","title":"ProducerOutput"},{"location":"reference/arti/producers/#ancestors-in-mro_5","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/producers/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/producers/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/producers/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/producers/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/producers/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/producers/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/producers/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/producers/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/producers/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/producers/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/producers/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/producers/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/producers/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/producers/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/producers/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/producers/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/producers/#validatesig","text":"class ValidateSig ( / , * args , ** kwargs )","title":"ValidateSig"},{"location":"reference/arti/producers/#ancestors-in-mro_6","text":"collections.abc.Callable","title":"Ancestors (in MRO)"},{"location":"reference/arti/statistics/","text":"Module arti.statistics None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.artifacts import Statistic as Statistic # noqa: F401 # class FieldStatistic(Statistic): # \"\"\" A FieldStatistic is a Statistic associated with a particular Artifact field. # \"\"\" # # # class Count(FieldStatistic): # type = Int64() # # # class CountDistinct(FieldStatistic): # type = Int64() # # # class MaxInt64(FieldStatistic): # type = Int64() # # # class MinInt64(FieldStatistic): # type = Int64() # # # class SumInt64(FieldStatistic): # type = Int64()","title":"Statistics"},{"location":"reference/arti/statistics/#module-artistatistics","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from arti.artifacts import Statistic as Statistic # noqa: F401 # class FieldStatistic(Statistic): # \"\"\" A FieldStatistic is a Statistic associated with a particular Artifact field. # \"\"\" # # # class Count(FieldStatistic): # type = Int64() # # # class CountDistinct(FieldStatistic): # type = Int64() # # # class MaxInt64(FieldStatistic): # type = Int64() # # # class MinInt64(FieldStatistic): # type = Int64() # # # class SumInt64(FieldStatistic): # type = Int64()","title":"Module arti.statistics"},{"location":"reference/arti/thresholds/","text":"Module arti.thresholds None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import Any , ClassVar from arti.types import Type class Threshold : type : ClassVar [ type [ Type ]] def check ( self , value : Any ) -> bool : raise NotImplementedError () Classes Threshold class Threshold ( / , * args , ** kwargs ) View Source class Threshold : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError () Methods check def check ( self , value : Any ) -> bool View Source def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Thresholds"},{"location":"reference/arti/thresholds/#module-artithresholds","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import Any , ClassVar from arti.types import Type class Threshold : type : ClassVar [ type [ Type ]] def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Module arti.thresholds"},{"location":"reference/arti/thresholds/#classes","text":"","title":"Classes"},{"location":"reference/arti/thresholds/#threshold","text":"class Threshold ( / , * args , ** kwargs ) View Source class Threshold : type : ClassVar [ type[Type ] ] def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"Threshold"},{"location":"reference/arti/thresholds/#methods","text":"","title":"Methods"},{"location":"reference/arti/thresholds/#check","text":"def check ( self , value : Any ) -> bool View Source def check ( self , value : Any ) -> bool : raise NotImplementedError ()","title":"check"},{"location":"reference/arti/versions/","text":"Module arti.versions None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import inspect import subprocess from collections.abc import Callable from datetime import datetime , timezone from typing import Any , cast from pydantic import Field , validator from arti.fingerprints import Fingerprint from arti.internal.models import Model class Version ( Model ): _abstract_ = True class GitCommit ( Version ): sha : str = Field ( default_factory = ( lambda : subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]) . decode () . strip () ) ) class SemVer ( Version ): \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \" { self . major } . { self . minor } . { self . patch } \" return Fingerprint . from_string ( s ) class String ( Version ): value : str class _SourceDescriptor : # Experimental :) # Using AST rather than literal source will likely be less \"noisy\": # https://github.com/artigraph/artigraph/pull/36#issuecomment-824131156 def __get__ ( self , obj : Any , type_ : type ) -> String : return String ( value = inspect . getsource ( type_ )) _Source = cast ( Callable [[], String ], _SourceDescriptor ) class Timestamp ( Version ): dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round (( self . dt or datetime . utcnow ()) . timestamp ())) # TODO: Consider a Timestamp like version with a \"frequency\" arg (day, hour, etc) that we floor/ceil # to trigger \"scheduled\" invalidation. This doesn't solve imperative work like \"process X every hour # on the hour and play catch up for missed runs\" (rather, we should aim to represent that with # upstream Artifact partitions), but rather solves work like \"ingest all data at most this # frequently\" (eg: full export from an API - no point in a backfill). Classes GitCommit class GitCommit ( __pydantic_self__ , ** data : Any ) View Source class GitCommit ( Version ): sha: str = Field ( default_factory =( lambda: subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]). decode (). strip () ) ) Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . SemVer class SemVer ( __pydantic_self__ , ** data : Any ) View Source class SemVer ( Version ) : \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \"{self.major}.{self.minor}.{self.patch}\" return Fingerprint . from_string ( s ) Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . String class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Version ): value: str Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Timestamp class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( Version ) : dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round (( self . dt or datetime . utcnow ()). timestamp ())) Ancestors (in MRO) arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Version class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Versions"},{"location":"reference/arti/versions/#module-artiversions","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import inspect import subprocess from collections.abc import Callable from datetime import datetime , timezone from typing import Any , cast from pydantic import Field , validator from arti.fingerprints import Fingerprint from arti.internal.models import Model class Version ( Model ): _abstract_ = True class GitCommit ( Version ): sha : str = Field ( default_factory = ( lambda : subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]) . decode () . strip () ) ) class SemVer ( Version ): \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \" { self . major } . { self . minor } . { self . patch } \" return Fingerprint . from_string ( s ) class String ( Version ): value : str class _SourceDescriptor : # Experimental :) # Using AST rather than literal source will likely be less \"noisy\": # https://github.com/artigraph/artigraph/pull/36#issuecomment-824131156 def __get__ ( self , obj : Any , type_ : type ) -> String : return String ( value = inspect . getsource ( type_ )) _Source = cast ( Callable [[], String ], _SourceDescriptor ) class Timestamp ( Version ): dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round (( self . dt or datetime . utcnow ()) . timestamp ())) # TODO: Consider a Timestamp like version with a \"frequency\" arg (day, hour, etc) that we floor/ceil # to trigger \"scheduled\" invalidation. This doesn't solve imperative work like \"process X every hour # on the hour and play catch up for missed runs\" (rather, we should aim to represent that with # upstream Artifact partitions), but rather solves work like \"ingest all data at most this # frequently\" (eg: full export from an API - no point in a backfill).","title":"Module arti.versions"},{"location":"reference/arti/versions/#classes","text":"","title":"Classes"},{"location":"reference/arti/versions/#gitcommit","text":"class GitCommit ( __pydantic_self__ , ** data : Any ) View Source class GitCommit ( Version ): sha: str = Field ( default_factory =( lambda: subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]). decode (). strip () ) )","title":"GitCommit"},{"location":"reference/arti/versions/#ancestors-in-mro","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#semver","text":"class SemVer ( __pydantic_self__ , ** data : Any ) View Source class SemVer ( Version ) : \"\"\"SemVer fingerprinting only considers the major component, unless it is less than 0. By only considering the major version, we can add incremental bumps to a Producer without triggering historical backfills. The major version MUST be incremented on schema or methodological changes. \"\"\" major : int minor : int patch : int @property def fingerprint ( self ) -> Fingerprint : s = str ( self . major ) if self . major == 0 : s = f \"{self.major}.{self.minor}.{self.patch}\" return Fingerprint . from_string ( s )","title":"SemVer"},{"location":"reference/arti/versions/#ancestors-in-mro_1","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#string","text":"class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Version ): value: str","title":"String"},{"location":"reference/arti/versions/#ancestors-in-mro_2","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_2","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#timestamp","text":"class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( Version ) : dt : datetime = Field ( default_factory = lambda : datetime . now ( tz = timezone . utc )) @validator ( \"dt\" , always = True ) @classmethod def _requires_timezone ( cls , dt : datetime ) -> datetime : if dt is not None and dt . tzinfo is None : raise ValueError ( \"Timestamp requires a timezone-aware datetime!\" ) return dt @property def fingerprint ( self ) -> Fingerprint : return Fingerprint . from_int ( round (( self . dt or datetime . utcnow ()). timestamp ()))","title":"Timestamp"},{"location":"reference/arti/versions/#ancestors-in-mro_3","text":"arti.versions.Version arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_3","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_3","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/versions/#version","text":"class Version ( __pydantic_self__ , ** data : Any ) View Source class Version ( Model ): _abstract_ = True","title":"Version"},{"location":"reference/arti/versions/#ancestors-in-mro_4","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/versions/#descendants","text":"arti.versions.GitCommit arti.versions.SemVer arti.versions.String arti.versions.Timestamp","title":"Descendants"},{"location":"reference/arti/versions/#class-variables_4","text":"Config","title":"Class variables"},{"location":"reference/arti/versions/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/versions/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/versions/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/versions/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/versions/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/versions/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/versions/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/versions/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/versions/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/versions/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/versions/#instance-variables_4","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/versions/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/versions/#copy_4","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/versions/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/versions/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/backends/","text":"Module arti.backends None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterator from contextlib import contextmanager from typing import TypeVar from arti.artifacts import Artifact from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.storage import InputFingerprints , StoragePartitions # TODO: Consider adding CRUD methods for \"everything\"? # # Likely worth making a distinction between lower level (\"CRUD\") methods vs higher level (\"RPC\" or # \"composing\") methods. Executors should operate on the high level methods, but those may have # defaults simply calling the lower level methods. If high level methods can be optimized (eg: not a # bunch of low level calls, each own network call), Backend can override. _Backend = TypeVar ( \"_Backend\" , bound = \"Backend\" ) class Backend ( Model ): \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ]: raise NotImplementedError () # Artifact partitions - independent of a specific Graph (snapshot) @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \"\"\"Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\"\" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () # Artifact partitions for a specific Graph (snapshot) @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) # Graph Snapshot Tagging @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \"\"\"Fetch the Snapshot ID for the named tag.\"\"\" raise NotImplementedError () @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Tag a Graph Snapshot ID with an arbitrary name.\"\"\" raise NotImplementedError () Sub-modules arti.backends.memory Classes Backend class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model ) : \" \"\" Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\" \" @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError () # Artifact partitions - independent of a specific Graph (snapshot) @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Artifact partitions for a specific Graph (snapshot) @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) # Graph Snapshot Tagging @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \" \"\" Fetch the Snapshot ID for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \" \"\" Tag a Graph Snapshot ID with an arbitrary name. \"\" \" raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.backends.memory.MemoryBackend Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods connect def connect ( self : ~ _Backend ) -> collections . abc . Iterator [ ~ _Backend ] View Source @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError () copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . read_artifact_partitions def read_artifact_partitions ( self , artifact : arti . artifacts . Artifact , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a Graph snapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError () read_graph_partitions def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact ) -> tuple [ arti . storage . StoragePartition , ... ] Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () read_graph_tag def read_graph_tag ( self , graph_name : str , tag : str ) -> arti . fingerprints . Fingerprint Fetch the Snapshot ID for the named tag. View Source @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \"\"\"Fetch the Snapshot ID for the named tag.\"\"\" raise NotImplementedError () write_artifact_and_graph_partitions def write_artifact_and_graph_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ], graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str ) -> None View Source def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) write_artifact_partitions def write_artifact_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () write_graph_partitions def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Link the Partitions to the named Artifact in a specific Graph snapshot. View Source @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () write_graph_tag def write_graph_tag ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , tag : str , overwrite : bool = False ) -> None Tag a Graph Snapshot ID with an arbitrary name. View Source @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Tag a Graph Snapshot ID with an arbitrary name.\"\"\" raise NotImplementedError ()","title":"Index"},{"location":"reference/arti/backends/#module-artibackends","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterator from contextlib import contextmanager from typing import TypeVar from arti.artifacts import Artifact from arti.fingerprints import Fingerprint from arti.internal.models import Model from arti.storage import InputFingerprints , StoragePartitions # TODO: Consider adding CRUD methods for \"everything\"? # # Likely worth making a distinction between lower level (\"CRUD\") methods vs higher level (\"RPC\" or # \"composing\") methods. Executors should operate on the high level methods, but those may have # defaults simply calling the lower level methods. If high level methods can be optimized (eg: not a # bunch of low level calls, each own network call), Backend can override. _Backend = TypeVar ( \"_Backend\" , bound = \"Backend\" ) class Backend ( Model ): \"\"\"Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\"\" @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ]: raise NotImplementedError () # Artifact partitions - independent of a specific Graph (snapshot) @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \"\"\"Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\"\" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError () # Artifact partitions for a specific Graph (snapshot) @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError () def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) # Graph Snapshot Tagging @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \"\"\"Fetch the Snapshot ID for the named tag.\"\"\" raise NotImplementedError () @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Tag a Graph Snapshot ID with an arbitrary name.\"\"\" raise NotImplementedError ()","title":"Module arti.backends"},{"location":"reference/arti/backends/#sub-modules","text":"arti.backends.memory","title":"Sub-modules"},{"location":"reference/arti/backends/#classes","text":"","title":"Classes"},{"location":"reference/arti/backends/#backend","text":"class Backend ( __pydantic_self__ , ** data : Any ) View Source class Backend ( Model ) : \" \"\" Backend represents a storage for internal Artigraph metadata. Backend storage is an addressable location (local path, database connection, etc) that tracks metadata for a collection of Graphs over time, including: - the Artifact(s)->Producer->Artifact(s) dependency graph - Artifact Annotations, Statistics, Partitions, and other metadata - Artifact and Producer Fingerprints - etc \"\" \" @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError () # Artifact partitions - independent of a specific Graph (snapshot) @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \" \"\" Add more partitions for a Storage spec. \"\" \" raise NotImplementedError () # Artifact partitions for a specific Graph (snapshot) @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \" \"\" Read the known Partitions for the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \" \"\" Link the Partitions to the named Artifact in a specific Graph snapshot. \"\" \" raise NotImplementedError () def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) # Graph Snapshot Tagging @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \" \"\" Fetch the Snapshot ID for the named tag. \"\" \" raise NotImplementedError () @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \" \"\" Tag a Graph Snapshot ID with an arbitrary name. \"\" \" raise NotImplementedError ()","title":"Backend"},{"location":"reference/arti/backends/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/backends/#descendants","text":"arti.backends.memory.MemoryBackend","title":"Descendants"},{"location":"reference/arti/backends/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/backends/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/backends/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/backends/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/backends/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/backends/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/backends/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/backends/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/backends/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/backends/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/backends/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/backends/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/backends/#methods","text":"","title":"Methods"},{"location":"reference/arti/backends/#connect","text":"def connect ( self : ~ _Backend ) -> collections . abc . Iterator [ ~ _Backend ] View Source @contextmanager def connect ( self : _Backend ) -> Iterator [ _Backend ] : raise NotImplementedError ()","title":"connect"},{"location":"reference/arti/backends/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/backends/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/backends/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/backends/#read_artifact_partitions","text":"def read_artifact_partitions ( self , artifact : arti . artifacts . Artifact , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . StoragePartition , ... ] Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a Graph snapshot. View Source @abstractmethod def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : \" \"\" Read all known Partitions for this Storage spec. If `input_fingerprints` is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless `input_fingerprints` is provided matching those for a Graph snapshot. \"\" \" raise NotImplementedError ()","title":"read_artifact_partitions"},{"location":"reference/arti/backends/#read_graph_partitions","text":"def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact ) -> tuple [ arti . storage . StoragePartition , ... ] Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source @abstractmethod def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError ()","title":"read_graph_partitions"},{"location":"reference/arti/backends/#read_graph_tag","text":"def read_graph_tag ( self , graph_name : str , tag : str ) -> arti . fingerprints . Fingerprint Fetch the Snapshot ID for the named tag. View Source @abstractmethod def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : \"\"\"Fetch the Snapshot ID for the named tag.\"\"\" raise NotImplementedError ()","title":"read_graph_tag"},{"location":"reference/arti/backends/#write_artifact_and_graph_partitions","text":"def write_artifact_and_graph_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ], graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str ) -> None View Source def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions )","title":"write_artifact_and_graph_partitions"},{"location":"reference/arti/backends/#write_artifact_partitions","text":"def write_artifact_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Add more partitions for a Storage spec. View Source @abstractmethod def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : \"\"\"Add more partitions for a Storage spec.\"\"\" raise NotImplementedError ()","title":"write_artifact_partitions"},{"location":"reference/arti/backends/#write_graph_partitions","text":"def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ] ) -> None Link the Partitions to the named Artifact in a specific Graph snapshot. View Source @abstractmethod def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : \"\"\"Link the Partitions to the named Artifact in a specific Graph snapshot.\"\"\" raise NotImplementedError ()","title":"write_graph_partitions"},{"location":"reference/arti/backends/#write_graph_tag","text":"def write_graph_tag ( self , graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , tag : str , overwrite : bool = False ) -> None Tag a Graph Snapshot ID with an arbitrary name. View Source @abstractmethod def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Tag a Graph Snapshot ID with an arbitrary name.\"\"\" raise NotImplementedError ()","title":"write_graph_tag"},{"location":"reference/arti/backends/memory/","text":"Module arti.backends.memory None None View Source from __future__ import annotations from collections import defaultdict from collections.abc import Iterator from contextlib import contextmanager from pydantic import PrivateAttr from arti.artifacts import Artifact from arti.backends import Backend from arti.fingerprints import Fingerprint from arti.storage import AnyStorage , InputFingerprints , StoragePartition , StoragePartitions def ensure_fingerprinted ( partitions : StoragePartitions ) -> Iterator [ StoragePartition ]: for partition in partitions : yield partition . with_content_fingerprint ( keep_existing = True ) _GraphSnapshotPartitions = dict [ str , dict [ Fingerprint , dict [ str , set [ StoragePartition ]]]] _StoragePartitions = dict [ AnyStorage , set [ StoragePartition ]] class MemoryBackend ( Backend ): # `_graph_snapshot_partitions` tracks all the partitions for a *specific* \"run\" of a graph. # `_storage_partitions` tracks all partitions, across all graphs. This separation is important # to allow for Literals to be used even after a snapshot_id change. _graph_snapshot_partitions : _GraphSnapshotPartitions = PrivateAttr ( default_factory = lambda : defaultdict ( lambda : defaultdict ( lambda : defaultdict ( set [ StoragePartition ])) ) ) _graph_tags : dict [ str , dict [ str , Fingerprint ]] = PrivateAttr ( default_factory = lambda : defaultdict ( dict ) ) _storage_partitions : _StoragePartitions = PrivateAttr ( default_factory = lambda : defaultdict ( set [ StoragePartition ]) ) @contextmanager def connect ( self ) -> Iterator [ MemoryBackend ]: yield self def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is (obviously) not persistent, so there may be external data we don't # know about. If we haven't seen this storage before, we'll attempt to \"warm\" the cache. if artifact . storage not in self . _storage_partitions : self . write_artifact_partitions ( artifact , artifact . discover_storage_partitions ( input_fingerprints ) ) partitions = self . _storage_partitions [ artifact . storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . _storage_partitions [ artifact . storage ] . update ( ensure_fingerprinted ( partitions )) def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ]) def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] . update ( ensure_fingerprinted ( partitions ) ) def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : if tag not in self . _graph_tags [ graph_name ]: raise ValueError ( f \"No known ` { tag } ` tag for Graph ` { graph_name } `\" ) return self . _graph_tags [ graph_name ][ tag ] def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" if ( existing := self . _graph_tags [ graph_name ] . get ( tag )) is not None and not overwrite : raise ValueError ( f \"Existing ` { tag } ` tag for Graph ` { graph_name } ` points to { existing } \" ) self . _graph_tags [ graph_name ][ tag ] = graph_snapshot_id Functions ensure_fingerprinted def ensure_fingerprinted ( partitions : 'StoragePartitions' ) -> 'Iterator[StoragePartition]' View Source def ensure_fingerprinted ( partitions : StoragePartitions ) -> Iterator [ StoragePartition ] : for partition in partitions : yield partition . with_content_fingerprint ( keep_existing = True ) Classes MemoryBackend class MemoryBackend ( __pydantic_self__ , ** data : Any ) View Source class MemoryBackend ( Backend ) : # ` _graph_snapshot_partitions ` tracks all the partitions for a * specific * \"run\" of a graph . # ` _storage_partitions ` tracks all partitions , across all graphs . This separation is important # to allow for Literals to be used even after a snapshot_id change . _graph_snapshot_partitions : _GraphSnapshotPartitions = PrivateAttr ( default_factory = lambda : defaultdict ( lambda : defaultdict ( lambda : defaultdict ( set [ StoragePartition ] )) ) ) _graph_tags : dict [ str, dict[str, Fingerprint ] ] = PrivateAttr ( default_factory = lambda : defaultdict ( dict ) ) _storage_partitions : _StoragePartitions = PrivateAttr ( default_factory = lambda : defaultdict ( set [ StoragePartition ] ) ) @contextmanager def connect ( self ) -> Iterator [ MemoryBackend ] : yield self def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we ' ll attempt to \"warm\" the cache . if artifact . storage not in self . _storage_partitions : self . write_artifact_partitions ( artifact , artifact . discover_storage_partitions ( input_fingerprints ) ) partitions = self . _storage_partitions [ artifact.storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . _storage_partitions [ artifact.storage ] . update ( ensure_fingerprinted ( partitions )) def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] ) def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] . update ( ensure_fingerprinted ( partitions ) ) def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : if tag not in self . _graph_tags [ graph_name ] : raise ValueError ( f \"No known `{tag}` tag for Graph `{graph_name}`\" ) return self . _graph_tags [ graph_name ][ tag ] def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" if ( existing : = self . _graph_tags [ graph_name ] . get ( tag )) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{graph_name}` points to {existing}\" ) self . _graph_tags [ graph_name ][ tag ] = graph_snapshot_id Ancestors (in MRO) arti.backends.Backend arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods connect def connect ( self ) -> 'Iterator[MemoryBackend]' View Source @contextmanager def connect ( self ) -> Iterator [ MemoryBackend ] : yield self copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . read_artifact_partitions def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = frozendict ({}) ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a Graph snapshot. View Source def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we 'll attempt to \" warm \" the cache. if artifact.storage not in self._storage_partitions: self.write_artifact_partitions( artifact, artifact.discover_storage_partitions(input_fingerprints) ) partitions = self._storage_partitions[artifact.storage] if input_fingerprints: partitions = { partition for partition in partitions if input_fingerprints.get(partition.keys) == partition.input_fingerprint } return tuple(partitions) read_graph_partitions def read_graph_partitions ( self , graph_name : 'str' , graph_snapshot_id : 'Fingerprint' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] ) read_graph_tag def read_graph_tag ( self , graph_name : 'str' , tag : 'str' ) -> 'Fingerprint' Fetch the Snapshot ID for the named tag. View Source def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : if tag not in self . _graph_tags [ graph_name ] : raise ValueError ( f \"No known `{tag}` tag for Graph `{graph_name}`\" ) return self . _graph_tags [ graph_name ][ tag ] write_artifact_and_graph_partitions def write_artifact_and_graph_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ], graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str ) -> None View Source def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions ) write_artifact_partitions def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . _storage_partitions [ artifact . storage ]. update ( ensure_fingerprinted ( partitions )) write_graph_partitions def write_graph_partitions ( self , graph_name : 'str' , graph_snapshot_id : 'Fingerprint' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific Graph snapshot. View Source def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] . update ( ensure_fingerprinted ( partitions ) ) write_graph_tag def write_graph_tag ( self , graph_name : 'str' , graph_snapshot_id : 'Fingerprint' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" if ( existing : = self . _graph_tags [ graph_name ] . get ( tag )) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{graph_name}` points to {existing}\" ) self . _graph_tags [ graph_name ][ tag ] = graph_snapshot_id","title":"Memory"},{"location":"reference/arti/backends/memory/#module-artibackendsmemory","text":"None None View Source from __future__ import annotations from collections import defaultdict from collections.abc import Iterator from contextlib import contextmanager from pydantic import PrivateAttr from arti.artifacts import Artifact from arti.backends import Backend from arti.fingerprints import Fingerprint from arti.storage import AnyStorage , InputFingerprints , StoragePartition , StoragePartitions def ensure_fingerprinted ( partitions : StoragePartitions ) -> Iterator [ StoragePartition ]: for partition in partitions : yield partition . with_content_fingerprint ( keep_existing = True ) _GraphSnapshotPartitions = dict [ str , dict [ Fingerprint , dict [ str , set [ StoragePartition ]]]] _StoragePartitions = dict [ AnyStorage , set [ StoragePartition ]] class MemoryBackend ( Backend ): # `_graph_snapshot_partitions` tracks all the partitions for a *specific* \"run\" of a graph. # `_storage_partitions` tracks all partitions, across all graphs. This separation is important # to allow for Literals to be used even after a snapshot_id change. _graph_snapshot_partitions : _GraphSnapshotPartitions = PrivateAttr ( default_factory = lambda : defaultdict ( lambda : defaultdict ( lambda : defaultdict ( set [ StoragePartition ])) ) ) _graph_tags : dict [ str , dict [ str , Fingerprint ]] = PrivateAttr ( default_factory = lambda : defaultdict ( dict ) ) _storage_partitions : _StoragePartitions = PrivateAttr ( default_factory = lambda : defaultdict ( set [ StoragePartition ]) ) @contextmanager def connect ( self ) -> Iterator [ MemoryBackend ]: yield self def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is (obviously) not persistent, so there may be external data we don't # know about. If we haven't seen this storage before, we'll attempt to \"warm\" the cache. if artifact . storage not in self . _storage_partitions : self . write_artifact_partitions ( artifact , artifact . discover_storage_partitions ( input_fingerprints ) ) partitions = self . _storage_partitions [ artifact . storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . _storage_partitions [ artifact . storage ] . update ( ensure_fingerprinted ( partitions )) def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ]) def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] . update ( ensure_fingerprinted ( partitions ) ) def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : if tag not in self . _graph_tags [ graph_name ]: raise ValueError ( f \"No known ` { tag } ` tag for Graph ` { graph_name } `\" ) return self . _graph_tags [ graph_name ][ tag ] def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" if ( existing := self . _graph_tags [ graph_name ] . get ( tag )) is not None and not overwrite : raise ValueError ( f \"Existing ` { tag } ` tag for Graph ` { graph_name } ` points to { existing } \" ) self . _graph_tags [ graph_name ][ tag ] = graph_snapshot_id","title":"Module arti.backends.memory"},{"location":"reference/arti/backends/memory/#functions","text":"","title":"Functions"},{"location":"reference/arti/backends/memory/#ensure_fingerprinted","text":"def ensure_fingerprinted ( partitions : 'StoragePartitions' ) -> 'Iterator[StoragePartition]' View Source def ensure_fingerprinted ( partitions : StoragePartitions ) -> Iterator [ StoragePartition ] : for partition in partitions : yield partition . with_content_fingerprint ( keep_existing = True )","title":"ensure_fingerprinted"},{"location":"reference/arti/backends/memory/#classes","text":"","title":"Classes"},{"location":"reference/arti/backends/memory/#memorybackend","text":"class MemoryBackend ( __pydantic_self__ , ** data : Any ) View Source class MemoryBackend ( Backend ) : # ` _graph_snapshot_partitions ` tracks all the partitions for a * specific * \"run\" of a graph . # ` _storage_partitions ` tracks all partitions , across all graphs . This separation is important # to allow for Literals to be used even after a snapshot_id change . _graph_snapshot_partitions : _GraphSnapshotPartitions = PrivateAttr ( default_factory = lambda : defaultdict ( lambda : defaultdict ( lambda : defaultdict ( set [ StoragePartition ] )) ) ) _graph_tags : dict [ str, dict[str, Fingerprint ] ] = PrivateAttr ( default_factory = lambda : defaultdict ( dict ) ) _storage_partitions : _StoragePartitions = PrivateAttr ( default_factory = lambda : defaultdict ( set [ StoragePartition ] ) ) @contextmanager def connect ( self ) -> Iterator [ MemoryBackend ] : yield self def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we ' ll attempt to \"warm\" the cache . if artifact . storage not in self . _storage_partitions : self . write_artifact_partitions ( artifact , artifact . discover_storage_partitions ( input_fingerprints ) ) partitions = self . _storage_partitions [ artifact.storage ] if input_fingerprints : partitions = { partition for partition in partitions if input_fingerprints . get ( partition . keys ) == partition . input_fingerprint } return tuple ( partitions ) def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . _storage_partitions [ artifact.storage ] . update ( ensure_fingerprinted ( partitions )) def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] ) def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] . update ( ensure_fingerprinted ( partitions ) ) def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : if tag not in self . _graph_tags [ graph_name ] : raise ValueError ( f \"No known `{tag}` tag for Graph `{graph_name}`\" ) return self . _graph_tags [ graph_name ][ tag ] def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" if ( existing : = self . _graph_tags [ graph_name ] . get ( tag )) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{graph_name}` points to {existing}\" ) self . _graph_tags [ graph_name ][ tag ] = graph_snapshot_id","title":"MemoryBackend"},{"location":"reference/arti/backends/memory/#ancestors-in-mro","text":"arti.backends.Backend arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/backends/memory/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/backends/memory/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/backends/memory/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/backends/memory/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/backends/memory/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/backends/memory/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/backends/memory/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/backends/memory/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/backends/memory/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/backends/memory/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/backends/memory/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/backends/memory/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/backends/memory/#methods","text":"","title":"Methods"},{"location":"reference/arti/backends/memory/#connect","text":"def connect ( self ) -> 'Iterator[MemoryBackend]' View Source @contextmanager def connect ( self ) -> Iterator [ MemoryBackend ] : yield self","title":"connect"},{"location":"reference/arti/backends/memory/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/backends/memory/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/backends/memory/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/backends/memory/#read_artifact_partitions","text":"def read_artifact_partitions ( self , artifact : 'Artifact' , input_fingerprints : 'InputFingerprints' = frozendict ({}) ) -> 'StoragePartitions' Read all known Partitions for this Storage spec. If input_fingerprints is provided, the returned partitions will be filtered accordingly. NOTE: The returned partitions may not be associated with any particular Graph, unless input_fingerprints is provided matching those for a Graph snapshot. View Source def read_artifact_partitions ( self , artifact : Artifact , input_fingerprints : InputFingerprints = InputFingerprints () ) -> StoragePartitions : # The MemoryBackend is ( obviously ) not persistent , so there may be external data we don 't # know about. If we haven' t seen this storage before , we 'll attempt to \" warm \" the cache. if artifact.storage not in self._storage_partitions: self.write_artifact_partitions( artifact, artifact.discover_storage_partitions(input_fingerprints) ) partitions = self._storage_partitions[artifact.storage] if input_fingerprints: partitions = { partition for partition in partitions if input_fingerprints.get(partition.keys) == partition.input_fingerprint } return tuple(partitions)","title":"read_artifact_partitions"},{"location":"reference/arti/backends/memory/#read_graph_partitions","text":"def read_graph_partitions ( self , graph_name : 'str' , graph_snapshot_id : 'Fingerprint' , artifact_key : 'str' , artifact : 'Artifact' ) -> 'StoragePartitions' Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source def read_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact ) -> StoragePartitions : return tuple ( self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] )","title":"read_graph_partitions"},{"location":"reference/arti/backends/memory/#read_graph_tag","text":"def read_graph_tag ( self , graph_name : 'str' , tag : 'str' ) -> 'Fingerprint' Fetch the Snapshot ID for the named tag. View Source def read_graph_tag ( self , graph_name : str , tag : str ) -> Fingerprint : if tag not in self . _graph_tags [ graph_name ] : raise ValueError ( f \"No known `{tag}` tag for Graph `{graph_name}`\" ) return self . _graph_tags [ graph_name ][ tag ]","title":"read_graph_tag"},{"location":"reference/arti/backends/memory/#write_artifact_and_graph_partitions","text":"def write_artifact_and_graph_partitions ( self , artifact : arti . artifacts . Artifact , partitions : tuple [ arti . storage . StoragePartition , ... ], graph_name : str , graph_snapshot_id : arti . fingerprints . Fingerprint , artifact_key : str ) -> None View Source def write_artifact_and_graph_partitions ( self , artifact : Artifact , partitions : StoragePartitions , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , ) -> None : self . write_artifact_partitions ( artifact , partitions ) self . write_graph_partitions ( graph_name , graph_snapshot_id , artifact_key , artifact , partitions )","title":"write_artifact_and_graph_partitions"},{"location":"reference/arti/backends/memory/#write_artifact_partitions","text":"def write_artifact_partitions ( self , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Add more partitions for a Storage spec. View Source def write_artifact_partitions ( self , artifact : Artifact , partitions : StoragePartitions ) -> None : self . _storage_partitions [ artifact . storage ]. update ( ensure_fingerprinted ( partitions ))","title":"write_artifact_partitions"},{"location":"reference/arti/backends/memory/#write_graph_partitions","text":"def write_graph_partitions ( self , graph_name : 'str' , graph_snapshot_id : 'Fingerprint' , artifact_key : 'str' , artifact : 'Artifact' , partitions : 'StoragePartitions' ) -> 'None' Link the Partitions to the named Artifact in a specific Graph snapshot. View Source def write_graph_partitions ( self , graph_name : str , graph_snapshot_id : Fingerprint , artifact_key : str , artifact : Artifact , partitions : StoragePartitions , ) -> None : self . _graph_snapshot_partitions [ graph_name ][ graph_snapshot_id ][ artifact_key ] . update ( ensure_fingerprinted ( partitions ) )","title":"write_graph_partitions"},{"location":"reference/arti/backends/memory/#write_graph_tag","text":"def write_graph_tag ( self , graph_name : 'str' , graph_snapshot_id : 'Fingerprint' , tag : 'str' , overwrite : 'bool' = False ) -> 'None' Read the known Partitions for the named Artifact in a specific Graph snapshot. View Source def write_graph_tag ( self , graph_name : str , graph_snapshot_id : Fingerprint , tag : str , overwrite : bool = False ) -> None : \"\"\"Read the known Partitions for the named Artifact in a specific Graph snapshot.\"\"\" if ( existing : = self . _graph_tags [ graph_name ] . get ( tag )) is not None and not overwrite : raise ValueError ( f \"Existing `{tag}` tag for Graph `{graph_name}` points to {existing}\" ) self . _graph_tags [ graph_name ][ tag ] = graph_snapshot_id","title":"write_graph_tag"},{"location":"reference/arti/executors/","text":"Module arti.executors None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc from arti.graphs import Graph from arti.internal.models import Model class Executor ( Model ): @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError () Sub-modules arti.executors.local Classes Executor class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError () Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.executors.local.LocalExecutor Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods build def build ( self , graph : 'Graph' ) -> 'None' View Source @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError () copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/executors/#module-artiexecutors","text":"None None View Source from __future__ import annotations __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc from arti.graphs import Graph from arti.internal.models import Model class Executor ( Model ): @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError ()","title":"Module arti.executors"},{"location":"reference/arti/executors/#sub-modules","text":"arti.executors.local","title":"Sub-modules"},{"location":"reference/arti/executors/#classes","text":"","title":"Classes"},{"location":"reference/arti/executors/#executor","text":"class Executor ( __pydantic_self__ , ** data : Any ) View Source class Executor ( Model ) : @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError ()","title":"Executor"},{"location":"reference/arti/executors/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/executors/#descendants","text":"arti.executors.local.LocalExecutor","title":"Descendants"},{"location":"reference/arti/executors/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/executors/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/executors/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/executors/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/executors/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/executors/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/executors/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/executors/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/executors/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/executors/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/executors/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/executors/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/executors/#methods","text":"","title":"Methods"},{"location":"reference/arti/executors/#build","text":"def build ( self , graph : 'Graph' ) -> 'None' View Source @abc . abstractmethod def build ( self , graph : Graph ) -> None : raise NotImplementedError ()","title":"build"},{"location":"reference/arti/executors/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/executors/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/executors/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/executors/local/","text":"Module arti.executors.local None None View Source import logging from graphlib import TopologicalSorter from itertools import chain from arti.artifacts import Artifact from arti.backends import Backend from arti.executors import Executor from arti.graphs import Graph from arti.producers import Producer from arti.storage import InputFingerprints # TODO: Factor this code out to reusable helpers/better homes. # - Perhaps a lot of the _build_producer logic can live in a few Producer methods. We # still want to expose each partition-to-build as a parallelizable unit of execution, # so maybe one Producer method kicks out all to-be-built partitions+metadata, and then # Executor can parallelize the `.build`. class LocalExecutor ( Executor ): # TODO: Separate .map and .build steps so we can: # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the .map phase in the future, if we wanted to support some sort of # iterated or cyclic Producers (eg: first pass output feeds into second run - in that case, # `.map` should describe how to \"converge\" by returning the same outputs as a prior call). def _build_producer ( self , graph : Graph , backend : Backend , producer : Producer , ) -> None : input_partitions = { name : backend . read_graph_partitions ( graph . name , graph . get_snapshot_id (), graph . artifact_to_key [ artifact ], artifact ) for name , artifact in producer . inputs . items () } partition_dependencies = producer . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in producer . _map_input_metadata_ } ) # TODO: Need to validate the partition_dependencies against the Producer's # partitioning scheme and such (basically, check user error). eg: if output is # not partitioned, we expect only 1 entry in partition_dependencies # (NotPartitioned). partition_input_fingerprints = InputFingerprints ( { composite_key : producer . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) output_artifacts = graph . producer_outputs [ producer ] # NOTE: The output partitions may be built, but not yet associated with this snapshot_id # (eg: raw input data changed, but no changes trickled into this specific Producer). Hence # we'll fetch all StoragePartitions for this Storage, filtered to the PKs and # input_fingerprints we've computed *are* for this Graph - and then link them to the graph. existing_output_partitions = { output : backend . read_artifact_partitions ( output , partition_input_fingerprints ) for output in output_artifacts } for artifact , partitions in existing_output_partitions . items (): backend . write_graph_partitions ( graph . name , graph . get_snapshot_id (), graph . artifact_to_key [ artifact ], artifact , partitions , ) # TODO: Guarantee all outputs have the same set of identified partitions. Currently, this # pretends a partition is built for all outputs if _any_ are built for that partition. existing_output_keys = { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } for output_partition_key , dependencies in partition_dependencies . items (): if output_partition_key in existing_output_keys : pk_str = f \" for: { dict ( output_partition_key ) } \" if output_partition_key else \".\" logging . info ( f \"Skipping existing { type ( producer ) . __name__ } output { pk_str } \" ) continue # TODO: Catch DispatchError and give a nicer error... maybe add this to our # @dispatch wrapper (eg: msg arg, or even fn that returns the message to # raise). arguments = { name : graph . read ( artifact = producer . inputs [ name ], storage_partitions = partition_dependencies [ output_partition_key ][ name ], # TODO: We'll probably need to update Producer._build_input_views_ # to hold *instances* (either objects set in Annotated or init # during Producer.__init_subclass__) view = view_class (), ) for name , view_class in producer . _build_input_views_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _output_metadata_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ): graph . write ( output , artifact = output_artifacts [ i ], input_fingerprint = partition_input_fingerprints [ output_partition_key ], keys = output_partition_key , view = producer . _output_metadata_ [ i ][ 1 ](), ) def build ( self , graph : Graph ) -> None : # NOTE: Raw Artifacts will already be discovered and linked in the backend to this graph # snapshot. assert graph . snapshot_id is not None with graph . backend . connect () as backend : for node in TopologicalSorter ( graph . dependencies ) . static_order (): if isinstance ( node , Artifact ): # TODO: Compute Statistics (if not already computed for the partition) and check # Thresholds (every time, as they may be changed, dynamic, or overridden). pass elif isinstance ( node , Producer ): logging . info ( f \"Building { node } ...\" ) self . _build_producer ( graph , backend , node ) else : raise NotImplementedError () logging . info ( \"Build finished.\" ) Classes LocalExecutor class LocalExecutor ( __pydantic_self__ , ** data : Any ) View Source class LocalExecutor ( Executor ) : # TODO : Separate . map and . build steps so we can : # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the . map phase in the future , if we wanted to support some sort of # iterated or cyclic Producers ( eg : first pass output feeds into second run - in that case , # ` . map ` should describe how to \"converge\" by returning the same outputs as a prior call ). def _build_producer ( self , graph : Graph , backend : Backend , producer : Producer , ) -> None : input_partitions = { name : backend . read_graph_partitions ( graph . name , graph . get_snapshot_id (), graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } partition_dependencies = producer . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in producer . _map_input_metadata_ } ) # TODO : Need to validate the partition_dependencies against the Producer 's # partitioning scheme and such (basically, check user error). eg: if output is # not partitioned, we expect only 1 entry in partition_dependencies # (NotPartitioned). partition_input_fingerprints = InputFingerprints( { composite_key: producer.compute_input_fingerprint(dependency_partitions) for composite_key, dependency_partitions in partition_dependencies.items() } ) output_artifacts = graph.producer_outputs[producer] # NOTE: The output partitions may be built, but not yet associated with this snapshot_id # (eg: raw input data changed, but no changes trickled into this specific Producer). Hence # we' ll fetch all StoragePartitions for this Storage , filtered to the PKs and # input_fingerprints we 've computed *are* for this Graph - and then link them to the graph. existing_output_partitions = { output: backend.read_artifact_partitions(output, partition_input_fingerprints) for output in output_artifacts } for artifact, partitions in existing_output_partitions.items(): backend.write_graph_partitions( graph.name, graph.get_snapshot_id(), graph.artifact_to_key[artifact], artifact, partitions, ) # TODO: Guarantee all outputs have the same set of identified partitions. Currently, this # pretends a partition is built for all outputs if _any_ are built for that partition. existing_output_keys = { partition.keys for partition in chain.from_iterable(existing_output_partitions.values()) } for output_partition_key, dependencies in partition_dependencies.items(): if output_partition_key in existing_output_keys: pk_str = f\" for: {dict(output_partition_key)}\" if output_partition_key else \".\" logging.info(f\"Skipping existing {type(producer).__name__} output{pk_str}\") continue # TODO: Catch DispatchError and give a nicer error... maybe add this to our # @dispatch wrapper (eg: msg arg, or even fn that returns the message to # raise). arguments = { name: graph.read( artifact=producer.inputs[name], storage_partitions=partition_dependencies[output_partition_key][name], # TODO: We' ll probably need to update Producer . _build_input_views_ # to hold * instances * ( either objects set in Annotated or init # during Producer . __init_subclass__ ) view = view_class (), ) for name , view_class in producer . _build_input_views_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _output_metadata_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : graph . write ( output , artifact = output_artifacts [ i ] , input_fingerprint = partition_input_fingerprints [ output_partition_key ] , keys = output_partition_key , view = producer . _output_metadata_ [ i ][ 1 ] (), ) def build ( self , graph : Graph ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this graph # snapshot . assert graph . snapshot_id is not None with graph . backend . connect () as backend : for node in TopologicalSorter ( graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) self . _build_producer ( graph , backend , node ) else : raise NotImplementedError () logging . info ( \"Build finished.\" ) Ancestors (in MRO) arti.executors.Executor arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods build def build ( self , graph : arti . graphs . Graph ) -> None View Source def build ( self , graph : Graph ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this graph # snapshot . assert graph . snapshot_id is not None with graph . backend . connect () as backend : for node in TopologicalSorter ( graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) self . _build_producer ( graph , backend , node ) else : raise NotImplementedError () logging . info ( \"Build finished.\" ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Local"},{"location":"reference/arti/executors/local/#module-artiexecutorslocal","text":"None None View Source import logging from graphlib import TopologicalSorter from itertools import chain from arti.artifacts import Artifact from arti.backends import Backend from arti.executors import Executor from arti.graphs import Graph from arti.producers import Producer from arti.storage import InputFingerprints # TODO: Factor this code out to reusable helpers/better homes. # - Perhaps a lot of the _build_producer logic can live in a few Producer methods. We # still want to expose each partition-to-build as a parallelizable unit of execution, # so maybe one Producer method kicks out all to-be-built partitions+metadata, and then # Executor can parallelize the `.build`. class LocalExecutor ( Executor ): # TODO: Separate .map and .build steps so we can: # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the .map phase in the future, if we wanted to support some sort of # iterated or cyclic Producers (eg: first pass output feeds into second run - in that case, # `.map` should describe how to \"converge\" by returning the same outputs as a prior call). def _build_producer ( self , graph : Graph , backend : Backend , producer : Producer , ) -> None : input_partitions = { name : backend . read_graph_partitions ( graph . name , graph . get_snapshot_id (), graph . artifact_to_key [ artifact ], artifact ) for name , artifact in producer . inputs . items () } partition_dependencies = producer . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in producer . _map_input_metadata_ } ) # TODO: Need to validate the partition_dependencies against the Producer's # partitioning scheme and such (basically, check user error). eg: if output is # not partitioned, we expect only 1 entry in partition_dependencies # (NotPartitioned). partition_input_fingerprints = InputFingerprints ( { composite_key : producer . compute_input_fingerprint ( dependency_partitions ) for composite_key , dependency_partitions in partition_dependencies . items () } ) output_artifacts = graph . producer_outputs [ producer ] # NOTE: The output partitions may be built, but not yet associated with this snapshot_id # (eg: raw input data changed, but no changes trickled into this specific Producer). Hence # we'll fetch all StoragePartitions for this Storage, filtered to the PKs and # input_fingerprints we've computed *are* for this Graph - and then link them to the graph. existing_output_partitions = { output : backend . read_artifact_partitions ( output , partition_input_fingerprints ) for output in output_artifacts } for artifact , partitions in existing_output_partitions . items (): backend . write_graph_partitions ( graph . name , graph . get_snapshot_id (), graph . artifact_to_key [ artifact ], artifact , partitions , ) # TODO: Guarantee all outputs have the same set of identified partitions. Currently, this # pretends a partition is built for all outputs if _any_ are built for that partition. existing_output_keys = { partition . keys for partition in chain . from_iterable ( existing_output_partitions . values ()) } for output_partition_key , dependencies in partition_dependencies . items (): if output_partition_key in existing_output_keys : pk_str = f \" for: { dict ( output_partition_key ) } \" if output_partition_key else \".\" logging . info ( f \"Skipping existing { type ( producer ) . __name__ } output { pk_str } \" ) continue # TODO: Catch DispatchError and give a nicer error... maybe add this to our # @dispatch wrapper (eg: msg arg, or even fn that returns the message to # raise). arguments = { name : graph . read ( artifact = producer . inputs [ name ], storage_partitions = partition_dependencies [ output_partition_key ][ name ], # TODO: We'll probably need to update Producer._build_input_views_ # to hold *instances* (either objects set in Annotated or init # during Producer.__init_subclass__) view = view_class (), ) for name , view_class in producer . _build_input_views_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _output_metadata_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ): graph . write ( output , artifact = output_artifacts [ i ], input_fingerprint = partition_input_fingerprints [ output_partition_key ], keys = output_partition_key , view = producer . _output_metadata_ [ i ][ 1 ](), ) def build ( self , graph : Graph ) -> None : # NOTE: Raw Artifacts will already be discovered and linked in the backend to this graph # snapshot. assert graph . snapshot_id is not None with graph . backend . connect () as backend : for node in TopologicalSorter ( graph . dependencies ) . static_order (): if isinstance ( node , Artifact ): # TODO: Compute Statistics (if not already computed for the partition) and check # Thresholds (every time, as they may be changed, dynamic, or overridden). pass elif isinstance ( node , Producer ): logging . info ( f \"Building { node } ...\" ) self . _build_producer ( graph , backend , node ) else : raise NotImplementedError () logging . info ( \"Build finished.\" )","title":"Module arti.executors.local"},{"location":"reference/arti/executors/local/#classes","text":"","title":"Classes"},{"location":"reference/arti/executors/local/#localexecutor","text":"class LocalExecutor ( __pydantic_self__ , ** data : Any ) View Source class LocalExecutor ( Executor ) : # TODO : Separate . map and . build steps so we can : # - add \"sync\" / \"dry run\" sort of things # - parallelize build # # We may still want to repeat the . map phase in the future , if we wanted to support some sort of # iterated or cyclic Producers ( eg : first pass output feeds into second run - in that case , # ` . map ` should describe how to \"converge\" by returning the same outputs as a prior call ). def _build_producer ( self , graph : Graph , backend : Backend , producer : Producer , ) -> None : input_partitions = { name : backend . read_graph_partitions ( graph . name , graph . get_snapshot_id (), graph . artifact_to_key [ artifact ] , artifact ) for name , artifact in producer . inputs . items () } partition_dependencies = producer . map ( ** { name : partitions for name , partitions in input_partitions . items () if name in producer . _map_input_metadata_ } ) # TODO : Need to validate the partition_dependencies against the Producer 's # partitioning scheme and such (basically, check user error). eg: if output is # not partitioned, we expect only 1 entry in partition_dependencies # (NotPartitioned). partition_input_fingerprints = InputFingerprints( { composite_key: producer.compute_input_fingerprint(dependency_partitions) for composite_key, dependency_partitions in partition_dependencies.items() } ) output_artifacts = graph.producer_outputs[producer] # NOTE: The output partitions may be built, but not yet associated with this snapshot_id # (eg: raw input data changed, but no changes trickled into this specific Producer). Hence # we' ll fetch all StoragePartitions for this Storage , filtered to the PKs and # input_fingerprints we 've computed *are* for this Graph - and then link them to the graph. existing_output_partitions = { output: backend.read_artifact_partitions(output, partition_input_fingerprints) for output in output_artifacts } for artifact, partitions in existing_output_partitions.items(): backend.write_graph_partitions( graph.name, graph.get_snapshot_id(), graph.artifact_to_key[artifact], artifact, partitions, ) # TODO: Guarantee all outputs have the same set of identified partitions. Currently, this # pretends a partition is built for all outputs if _any_ are built for that partition. existing_output_keys = { partition.keys for partition in chain.from_iterable(existing_output_partitions.values()) } for output_partition_key, dependencies in partition_dependencies.items(): if output_partition_key in existing_output_keys: pk_str = f\" for: {dict(output_partition_key)}\" if output_partition_key else \".\" logging.info(f\"Skipping existing {type(producer).__name__} output{pk_str}\") continue # TODO: Catch DispatchError and give a nicer error... maybe add this to our # @dispatch wrapper (eg: msg arg, or even fn that returns the message to # raise). arguments = { name: graph.read( artifact=producer.inputs[name], storage_partitions=partition_dependencies[output_partition_key][name], # TODO: We' ll probably need to update Producer . _build_input_views_ # to hold * instances * ( either objects set in Annotated or init # during Producer . __init_subclass__ ) view = view_class (), ) for name , view_class in producer . _build_input_views_ . items () } outputs = producer . build ( ** arguments ) if len ( producer . _output_metadata_ ) == 1 : outputs = ( outputs ,) validation_passed , validation_message = producer . validate_outputs ( * outputs ) if not validation_passed : raise ValueError ( validation_message ) for i , output in enumerate ( outputs ) : graph . write ( output , artifact = output_artifacts [ i ] , input_fingerprint = partition_input_fingerprints [ output_partition_key ] , keys = output_partition_key , view = producer . _output_metadata_ [ i ][ 1 ] (), ) def build ( self , graph : Graph ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this graph # snapshot . assert graph . snapshot_id is not None with graph . backend . connect () as backend : for node in TopologicalSorter ( graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) self . _build_producer ( graph , backend , node ) else : raise NotImplementedError () logging . info ( \"Build finished.\" )","title":"LocalExecutor"},{"location":"reference/arti/executors/local/#ancestors-in-mro","text":"arti.executors.Executor arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/executors/local/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/executors/local/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/executors/local/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/executors/local/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/executors/local/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/executors/local/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/executors/local/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/executors/local/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/executors/local/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/executors/local/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/executors/local/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/executors/local/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/executors/local/#methods","text":"","title":"Methods"},{"location":"reference/arti/executors/local/#build","text":"def build ( self , graph : arti . graphs . Graph ) -> None View Source def build ( self , graph : Graph ) -> None : # NOTE : Raw Artifacts will already be discovered and linked in the backend to this graph # snapshot . assert graph . snapshot_id is not None with graph . backend . connect () as backend : for node in TopologicalSorter ( graph . dependencies ). static_order () : if isinstance ( node , Artifact ) : # TODO : Compute Statistics ( if not already computed for the partition ) and check # Thresholds ( every time , as they may be changed , dynamic , or overridden ). pass elif isinstance ( node , Producer ) : logging . info ( f \"Building {node}...\" ) self . _build_producer ( graph , backend , node ) else : raise NotImplementedError () logging . info ( \"Build finished.\" )","title":"build"},{"location":"reference/arti/executors/local/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/executors/local/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/executors/local/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/formats/","text":"Module arti.formats None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import ClassVar , Optional from pydantic import Field , validator from arti.internal.models import Model from arti.types import Type , TypeSystem class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" type : Optional [ Type ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO: Check self.type_system supports the type. We can likely add a TypeSystem method # that will check for matching TypeAdapters. return type_ Sub-modules arti.formats.json arti.formats.pickle Classes Format class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ) : \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" type : Optional [ Type ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_ Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.formats.json.JSON arti.formats.pickle.Pickle Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/formats/#module-artiformats","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import ClassVar , Optional from pydantic import Field , validator from arti.internal.models import Model from arti.types import Type , TypeSystem class Format ( Model ): \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" type : Optional [ Type ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO: Check self.type_system supports the type. We can likely add a TypeSystem method # that will check for matching TypeAdapters. return type_","title":"Module arti.formats"},{"location":"reference/arti/formats/#sub-modules","text":"arti.formats.json arti.formats.pickle","title":"Sub-modules"},{"location":"reference/arti/formats/#classes","text":"","title":"Classes"},{"location":"reference/arti/formats/#format","text":"class Format ( __pydantic_self__ , ** data : Any ) View Source class Format ( Model ) : \"\"\"Format represents file formats such as CSV, Parquet, native (eg: databases), etc. Formats are associated with a type system that provides a bridge between the internal Artigraph types and any external type information. \"\"\" _abstract_ = True type_system : ClassVar [ TypeSystem ] extension : str = \"\" type : Optional [ Type ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_","title":"Format"},{"location":"reference/arti/formats/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/formats/#descendants","text":"arti.formats.json.JSON arti.formats.pickle.Pickle","title":"Descendants"},{"location":"reference/arti/formats/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/formats/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/formats/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/formats/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/formats/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/formats/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/formats/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/formats/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/formats/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/formats/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/formats/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/formats/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_","title":"validate_type"},{"location":"reference/arti/formats/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/formats/#methods","text":"","title":"Methods"},{"location":"reference/arti/formats/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/formats/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/formats/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/formats/json/","text":"Module arti.formats.json None None View Source from arti.formats import Format from arti.types.python import python_type_system class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system Classes JSON class JSON ( __pydantic_self__ , ** data : Any ) View Source class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system Ancestors (in MRO) arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config extension type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Json"},{"location":"reference/arti/formats/json/#module-artiformatsjson","text":"None None View Source from arti.formats import Format from arti.types.python import python_type_system class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system","title":"Module arti.formats.json"},{"location":"reference/arti/formats/json/#classes","text":"","title":"Classes"},{"location":"reference/arti/formats/json/#json","text":"class JSON ( __pydantic_self__ , ** data : Any ) View Source class JSON ( Format ): extension = \".json\" # Perhaps we narrow down a json_type_system with the subset of supported types + a way to hook # into json.JSON{De,En}coders. type_system = python_type_system","title":"JSON"},{"location":"reference/arti/formats/json/#ancestors-in-mro","text":"arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/formats/json/#class-variables","text":"Config extension type_system","title":"Class variables"},{"location":"reference/arti/formats/json/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/formats/json/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/formats/json/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/formats/json/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/formats/json/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/formats/json/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/formats/json/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/formats/json/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/formats/json/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/formats/json/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/formats/json/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_","title":"validate_type"},{"location":"reference/arti/formats/json/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/formats/json/#methods","text":"","title":"Methods"},{"location":"reference/arti/formats/json/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/formats/json/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/formats/json/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/formats/pickle/","text":"Module arti.formats.pickle None None View Source from arti.formats import Format from arti.types.python import python_type_system class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system Classes Pickle class Pickle ( __pydantic_self__ , ** data : Any ) View Source class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system Ancestors (in MRO) arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config extension type_system Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_ Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Pickle"},{"location":"reference/arti/formats/pickle/#module-artiformatspickle","text":"None None View Source from arti.formats import Format from arti.types.python import python_type_system class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system","title":"Module arti.formats.pickle"},{"location":"reference/arti/formats/pickle/#classes","text":"","title":"Classes"},{"location":"reference/arti/formats/pickle/#pickle","text":"class Pickle ( __pydantic_self__ , ** data : Any ) View Source class Pickle ( Format ): extension = \".pickle\" type_system = python_type_system","title":"Pickle"},{"location":"reference/arti/formats/pickle/#ancestors-in-mro","text":"arti.formats.Format arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/formats/pickle/#class-variables","text":"Config extension type_system","title":"Class variables"},{"location":"reference/arti/formats/pickle/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/formats/pickle/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/formats/pickle/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/formats/pickle/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/formats/pickle/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/formats/pickle/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/formats/pickle/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/formats/pickle/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/formats/pickle/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/formats/pickle/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/formats/pickle/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check self . type_system supports the type . We can likely add a TypeSystem method # that will check for matching TypeAdapters . return type_","title":"validate_type"},{"location":"reference/arti/formats/pickle/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/formats/pickle/#methods","text":"","title":"Methods"},{"location":"reference/arti/formats/pickle/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/formats/pickle/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/formats/pickle/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/internal/","text":"Module arti.internal None None View Source import importlib.metadata from collections.abc import Iterator from contextlib import contextmanager version = importlib . metadata . version ( \"arti\" ) @contextmanager def wrap_exc ( error_type : type [ Exception ], * , prefix : str ) -> Iterator [ None ]: \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - { msg } \" src = e error = error_type ( f \" { prefix }{ msg } \" ) error . wrapped = True # type: ignore raise error from src Sub-modules arti.internal.models arti.internal.patches arti.internal.type_hints arti.internal.utils Variables version Functions wrap_exc def wrap_exc ( error_type : type [ Exception ], * , prefix : str ) -> collections . abc . Iterator [ None ] Wrap exceptions of error_type and add a message prefix. error_type must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the caller of the generator will not be wrapped. View Source @contextmanager def wrap_exc ( error_type : type [ Exception ] , * , prefix : str ) -> Iterator [ None ] : \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - {msg}\" src = e error = error_type ( f \"{prefix}{msg}\" ) error . wrapped = True # type : ignore raise error from src","title":"Index"},{"location":"reference/arti/internal/#module-artiinternal","text":"None None View Source import importlib.metadata from collections.abc import Iterator from contextlib import contextmanager version = importlib . metadata . version ( \"arti\" ) @contextmanager def wrap_exc ( error_type : type [ Exception ], * , prefix : str ) -> Iterator [ None ]: \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - { msg } \" src = e error = error_type ( f \" { prefix }{ msg } \" ) error . wrapped = True # type: ignore raise error from src","title":"Module arti.internal"},{"location":"reference/arti/internal/#sub-modules","text":"arti.internal.models arti.internal.patches arti.internal.type_hints arti.internal.utils","title":"Sub-modules"},{"location":"reference/arti/internal/#variables","text":"version","title":"Variables"},{"location":"reference/arti/internal/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/#wrap_exc","text":"def wrap_exc ( error_type : type [ Exception ], * , prefix : str ) -> collections . abc . Iterator [ None ] Wrap exceptions of error_type and add a message prefix. error_type must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the caller of the generator will not be wrapped. View Source @contextmanager def wrap_exc ( error_type : type [ Exception ] , * , prefix : str ) -> Iterator [ None ] : \"\"\"Wrap exceptions of `error_type` and add a message prefix. `error_type` must be initializable with a single string message argument. NOTE: When used inside a generator, any exceptions raised by the *caller of the generator* will **not** be wrapped. \"\"\" try : yield except error_type as e : msg = str ( e ) if getattr ( e , \"wrapped\" , False ) and e . __cause__ is not None : src = e . __cause__ # Shorten exception chains to the root and last wrapped only else : msg = f \" - {msg}\" src = e error = error_type ( f \"{prefix}{msg}\" ) error . wrapped = True # type : ignore raise error from src","title":"wrap_exc"},{"location":"reference/arti/internal/models/","text":"Module arti.internal.models None None View Source from collections.abc import Generator , Mapping , Sequence from copy import deepcopy from functools import cached_property from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Literal , Optional , TypeVar , get_args , get_origin , ) from pydantic import BaseModel , Extra , root_validator , validator from pydantic.fields import ModelField , Undefined from pydantic.json import pydantic_encoder as pydantic_json_encoder from arti.internal.type_hints import is_union , lenient_issubclass from arti.internal.utils import class_name , classproperty , frozendict if TYPE_CHECKING : from arti.fingerprints import Fingerprint from arti.types import Type def _check_types ( value : Any , type_ : type ) -> Any : # noqa: C901 mismatch_error = ValueError ( f \"expected an instance of { type_ } , got: { value } \" ) if type_ is Any : return value origin = get_origin ( type_ ) if origin is not None : args = get_args ( type_ ) if origin is Annotated : return _check_types ( value , args [ 0 ]) if origin is Literal : return _check_types ( value , type ( args [ 0 ])) # NOTE: Optional[t] -> Union[t, NoneType] if is_union ( origin ): for subtype in args : try : return _check_types ( value , subtype ) except ValueError : pass raise mismatch_error if issubclass ( origin , ( dict , Mapping )): value = _check_types ( value , origin ) for k , v in value . items (): _check_types ( k , args [ 0 ]) _check_types ( v , args [ 1 ]) return value # Variadic tuples will be handled below if issubclass ( origin , tuple ) and ... not in args : value = _check_types ( value , origin ) if len ( value ) != len ( args ): raise mismatch_error for i , subtype in enumerate ( args ): _check_types ( value [ i ], subtype ) return value for t in ( tuple , list , set , frozenset , Sequence ): if issubclass ( origin , t ): value = _check_types ( value , origin ) for subvalue in value : _check_types ( subvalue , args [ 0 ]) return value if issubclass ( origin , type ): if not lenient_issubclass ( value , args [ 0 ]): raise ValueError ( f \"expected a subclass of { args [ 0 ] } , got: { value } \" ) return value if set ( args ) == { Any }: return _check_types ( value , origin ) raise NotImplementedError ( f \"Missing handler for { type_ } with { value } !\" ) if isinstance ( value , Mapping ) and not isinstance ( value , frozendict ): value = frozendict ( value ) if not lenient_issubclass ( type ( value ), type_ ): raise mismatch_error return value _Model = TypeVar ( \"_Model\" , bound = \"Model\" ) class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. setattr ( cls , \"_abstract_\" , cls . __dict__ . get ( \"_abstract_\" , False )) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes := cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): { unknown_excludes } \" ) if cls . _fingerprint_includes_ and ( unknown_includes := cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): { unknown_includes } \" ) @root_validator ( pre = True ) @classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \" { cls } cannot be instantiated directly!\" ) return values @validator ( \"*\" , pre = True ) @classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True keep_untouched = ( cached_property , classproperty ) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @staticmethod def _fingerprint_json_encoder ( obj : Any ) -> Any : from arti.fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint return pydantic_json_encoder ( obj ) @property def fingerprint ( self ) -> \"Fingerprint\" : from arti.fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = self . _fingerprint_json_encoder , ) return Fingerprint . from_string ( f \" { self . _class_key_ } : { json_repr } \" ) # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : \"Type\" , * , name : str , required : bool ) -> \"Type\" : return type_ Variables TYPE_CHECKING Classes Model class Model ( __pydantic_self__ , ** data : Any ) View Source class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @ classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. setattr ( cls , \"_abstract_\" , cls . __dict__ . get ( \"_abstract_\" , False )) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes : = cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): {unknown_excludes}\" ) if cls . _fingerprint_includes_ and ( unknown_includes : = cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): {unknown_includes}\" ) @ root_validator ( pre = True ) @ classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \"{cls} cannot be instantiated directly!\" ) return values @ validator ( \"*\" , pre = True ) @ classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True keep_untouched = ( cached_property , classproperty ) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value : = getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @ staticmethod def _fingerprint_json_encoder ( obj : Any ) -> Any : from arti . fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint return pydantic_json_encoder ( obj ) @ property def fingerprint ( self ) -> \"Fingerprint\" : from arti . fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = self . _fingerprint_json_encoder , ) return Fingerprint . from_string ( f \"{self._class_key_}:{json_repr}\" ) # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @ classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : \"Type\" , * , name : str , required : bool ) -> \"Type\" : return type_ Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.annotations.Annotation arti.types.Type arti.types._NamedMixin arti.types.TypeSystem arti.formats.Format arti.partitions.PartitionKey arti.fingerprints.Fingerprint arti.storage._StorageMixin arti.storage.StoragePartition arti.storage.Storage arti.artifacts.BaseArtifact arti.versions.Version arti.views.View arti.producers.Producer arti.producers.ProducerOutput arti.backends.Backend arti.graphs.Graph arti.executors.Executor Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Models"},{"location":"reference/arti/internal/models/#module-artiinternalmodels","text":"None None View Source from collections.abc import Generator , Mapping , Sequence from copy import deepcopy from functools import cached_property from typing import ( TYPE_CHECKING , Annotated , Any , ClassVar , Literal , Optional , TypeVar , get_args , get_origin , ) from pydantic import BaseModel , Extra , root_validator , validator from pydantic.fields import ModelField , Undefined from pydantic.json import pydantic_encoder as pydantic_json_encoder from arti.internal.type_hints import is_union , lenient_issubclass from arti.internal.utils import class_name , classproperty , frozendict if TYPE_CHECKING : from arti.fingerprints import Fingerprint from arti.types import Type def _check_types ( value : Any , type_ : type ) -> Any : # noqa: C901 mismatch_error = ValueError ( f \"expected an instance of { type_ } , got: { value } \" ) if type_ is Any : return value origin = get_origin ( type_ ) if origin is not None : args = get_args ( type_ ) if origin is Annotated : return _check_types ( value , args [ 0 ]) if origin is Literal : return _check_types ( value , type ( args [ 0 ])) # NOTE: Optional[t] -> Union[t, NoneType] if is_union ( origin ): for subtype in args : try : return _check_types ( value , subtype ) except ValueError : pass raise mismatch_error if issubclass ( origin , ( dict , Mapping )): value = _check_types ( value , origin ) for k , v in value . items (): _check_types ( k , args [ 0 ]) _check_types ( v , args [ 1 ]) return value # Variadic tuples will be handled below if issubclass ( origin , tuple ) and ... not in args : value = _check_types ( value , origin ) if len ( value ) != len ( args ): raise mismatch_error for i , subtype in enumerate ( args ): _check_types ( value [ i ], subtype ) return value for t in ( tuple , list , set , frozenset , Sequence ): if issubclass ( origin , t ): value = _check_types ( value , origin ) for subvalue in value : _check_types ( subvalue , args [ 0 ]) return value if issubclass ( origin , type ): if not lenient_issubclass ( value , args [ 0 ]): raise ValueError ( f \"expected a subclass of { args [ 0 ] } , got: { value } \" ) return value if set ( args ) == { Any }: return _check_types ( value , origin ) raise NotImplementedError ( f \"Missing handler for { type_ } with { value } !\" ) if isinstance ( value , Mapping ) and not isinstance ( value , frozendict ): value = frozendict ( value ) if not lenient_issubclass ( type ( value ), type_ ): raise mismatch_error return value _Model = TypeVar ( \"_Model\" , bound = \"Model\" ) class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. setattr ( cls , \"_abstract_\" , cls . __dict__ . get ( \"_abstract_\" , False )) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes := cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): { unknown_excludes } \" ) if cls . _fingerprint_includes_ and ( unknown_includes := cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): { unknown_includes } \" ) @root_validator ( pre = True ) @classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \" { cls } cannot be instantiated directly!\" ) return values @validator ( \"*\" , pre = True ) @classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True keep_untouched = ( cached_property , classproperty ) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @staticmethod def _fingerprint_json_encoder ( obj : Any ) -> Any : from arti.fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint return pydantic_json_encoder ( obj ) @property def fingerprint ( self ) -> \"Fingerprint\" : from arti.fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = self . _fingerprint_json_encoder , ) return Fingerprint . from_string ( f \" { self . _class_key_ } : { json_repr } \" ) # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : \"Type\" , * , name : str , required : bool ) -> \"Type\" : return type_","title":"Module arti.internal.models"},{"location":"reference/arti/internal/models/#variables","text":"TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/internal/models/#classes","text":"","title":"Classes"},{"location":"reference/arti/internal/models/#model","text":"class Model ( __pydantic_self__ , ** data : Any ) View Source class Model ( BaseModel ): # A model can be marked _abstract_ to prevent direct instantiation, such as when it is intended # as a base class for other models with arbitrary data. As the subclasses of an _abstract_ model # have unknown fields (varying per subclass), we don't have targets to mark abstract with # abc.ABC nor typing.Protocol. See [1] for more context. # # 1: https://github.com/artigraph/artigraph/pull/60#discussion_r669089086 _abstract_ : ClassVar [ bool ] = True _class_key_ : ClassVar [ str ] = class_name () _fingerprint_excludes_ : ClassVar [ Optional [ frozenset [ str ]]] = None _fingerprint_includes_ : ClassVar [ Optional [ frozenset [ str ]]] = None @ classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) # Default _abstract_ to False if not set explicitly on the class. __dict__ is read-only. setattr ( cls , \"_abstract_\" , cls . __dict__ . get ( \"_abstract_\" , False )) field_names = set ( cls . __fields__ ) if cls . _fingerprint_excludes_ and ( unknown_excludes : = cls . _fingerprint_excludes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_excludes_` field(s): {unknown_excludes}\" ) if cls . _fingerprint_includes_ and ( unknown_includes : = cls . _fingerprint_includes_ - field_names ): raise ValueError ( f \"Unknown `_fingerprint_includes_` field(s): {unknown_includes}\" ) @ root_validator ( pre = True ) @ classmethod def _block_abstract_instance ( cls , values : dict [ str , Any ]) -> dict [ str , Any ]: if cls . _abstract_ : raise ValueError ( f \"{cls} cannot be instantiated directly!\" ) return values @ validator ( \"*\" , pre = True ) @ classmethod def _strict_types ( cls , value : Any , field : ModelField ) -> Any : \"\"\"Check that the value is a stricter instance of the declared type annotation. Pydantic will attempt to *parse* values (eg: \"5\" -> 5), but we'd prefer stricter values for clarity and to avoid silent precision loss (eg: 5.3 -> 5). \"\"\" # `field.type_` points to the *inner* type (eg: `int`->`int`; `tuple[int, ...]` -> `int`) # while `field.outer_type_` will (mostly) include the full spec and match the `value` we # received. The caveat is the `field.outer_type_` will never be wrapped in `Optional` # (though nested fields like `tuple[tuple[Optional[int]]]` would). Hence, we pull the # `field.outer_type_`, but add back the `Optional` wrapping if necessary. type_ = field . outer_type_ if field . allow_none : type_ = Optional [ type_ ] return _check_types ( value , type_ ) # By default, pydantic just compares models by their dict representation, causing models of # different types but same fields (eg: Int8 and Int16) to be equivalent. This can be removed if # [1] is merged+released. # # 1: https://github.com/samuelcolvin/pydantic/pull/3066 def __eq__ ( self , other : Any ) -> bool : return self . __class__ == other . __class__ and tuple ( self . _iter ()) == tuple ( other . _iter ()) # Omitting unpassed args in repr by default def __repr_args__ ( self ) -> Sequence [ tuple [ Optional [ str ], Any ]]: return [( k , v ) for k , v in super () . __repr_args__ () if k in self . __fields_set__ ] def __str__ ( self ) -> str : return repr ( self ) class Config : extra = Extra . forbid frozen = True keep_untouched = ( cached_property , classproperty ) smart_union = True validate_assignment = True # Unused with frozen, unless that is overridden in subclass. def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super () . copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value : = getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy @ staticmethod def _fingerprint_json_encoder ( obj : Any ) -> Any : from arti . fingerprints import Fingerprint if isinstance ( obj , Fingerprint ): return obj . key if isinstance ( obj , Model ): return obj . fingerprint return pydantic_json_encoder ( obj ) @ property def fingerprint ( self ) -> \"Fingerprint\" : from arti . fingerprints import Fingerprint # `.json` cannot be used, even with a custom encoder, because it calls `.dict`, which # converts the sub-models to dicts. Instead, we want to access `.fingerprint` (in the # decoder). data = dict ( sorted ( # Sort to ensure stability self . _iter ( exclude = self . _fingerprint_excludes_ , include = self . _fingerprint_includes_ , ), key = lambda kv : kv [ 0 ], ) ) json_repr = self . __config__ . json_dumps ( data , default = self . _fingerprint_json_encoder , ) return Fingerprint . from_string ( f \"{self._class_key_}:{json_repr}\" ) # Filter out non-fields from ._iter (and thus .dict, .json, etc), such as `@cached_property` # after access (which just gets cached in .__dict__). def _iter ( self , * args : Any , ** kwargs : Any ) -> Generator [ tuple [ str , Any ], None , None ]: for key , value in super () . _iter ( * args , ** kwargs ): if key in self . __fields__ : yield key , value @ classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : \"Type\" , * , name : str , required : bool ) -> \"Type\" : return type_","title":"Model"},{"location":"reference/arti/internal/models/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/models/#descendants","text":"arti.annotations.Annotation arti.types.Type arti.types._NamedMixin arti.types.TypeSystem arti.formats.Format arti.partitions.PartitionKey arti.fingerprints.Fingerprint arti.storage._StorageMixin arti.storage.StoragePartition arti.storage.Storage arti.artifacts.BaseArtifact arti.versions.Version arti.views.View arti.producers.Producer arti.producers.ProducerOutput arti.backends.Backend arti.graphs.Graph arti.executors.Executor","title":"Descendants"},{"location":"reference/arti/internal/models/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/internal/models/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/internal/models/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/internal/models/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/internal/models/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/internal/models/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/internal/models/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/internal/models/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/internal/models/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/internal/models/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/internal/models/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/internal/models/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/internal/models/#methods","text":"","title":"Methods"},{"location":"reference/arti/internal/models/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/internal/models/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/internal/models/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/internal/patches/","text":"Module arti.internal.patches None None View Source def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. Pending: https://github.com/python/cpython/pull/28714 \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias ) # type: ignore Functions patch_TopologicalSorter_class_getitem def patch_TopologicalSorter_class_getitem ( ) -> None Patch adding TopologicalSorter. class_getitem to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. Pending: https://github.com/python/cpython/pull/28714 View Source def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. Pending: https://github.com/python/cpython/pull/28714 \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias ) # type: ignore","title":"Patches"},{"location":"reference/arti/internal/patches/#module-artiinternalpatches","text":"None None View Source def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. Pending: https://github.com/python/cpython/pull/28714 \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias ) # type: ignore","title":"Module arti.internal.patches"},{"location":"reference/arti/internal/patches/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/patches/#patch_topologicalsorter_class_getitem","text":"def patch_TopologicalSorter_class_getitem ( ) -> None Patch adding TopologicalSorter. class_getitem to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. Pending: https://github.com/python/cpython/pull/28714 View Source def patch_TopologicalSorter_class_getitem () -> None : \"\"\"Patch adding TopologicalSorter.__class_getitem__ to support subscription. TopologicalSorter is considered Generic in typeshed (hence mypy expects a type arg), but is not at runtime. Pending: https://github.com/python/cpython/pull/28714 \"\"\" from graphlib import TopologicalSorter from types import GenericAlias if not hasattr ( TopologicalSorter , \"__class_getitem__\" ): # pragma: no cover TopologicalSorter . __class_getitem__ = classmethod ( GenericAlias ) # type: ignore","title":"patch_TopologicalSorter_class_getitem"},{"location":"reference/arti/internal/type_hints/","text":"Module arti.internal.type_hints None None View Source import inspect import sys import types from collections.abc import Callable from datetime import date , datetime from typing import _AnnotatedAlias # type: ignore from typing import ( TYPE_CHECKING , Annotated , Any , TypeVar , Union , cast , get_args , get_origin , get_type_hints , no_type_check , ) NoneType = cast ( type , type ( None )) # mypy otherwise treats type(None) as an object def _check_issubclass ( klass : Any , check_type : type ) -> bool : # If a hint is Annotated, we want to unwrap the underlying type and discard the rest of the # annotations. klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) if klass_origin is Annotated : klass = klass_args [ 0 ] klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) if isinstance ( klass , TypeVar ): klass = cast ( type , Any ) if klass . __bound__ is None else klass . __bound__ klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if check_type_origin is Annotated : check_type = check_type_args [ 0 ] check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if isinstance ( check_type , TypeVar ): check_type = cast ( type , Any ) if check_type . __bound__ is None else check_type . __bound__ check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if klass is Any : return check_type is Any if check_type is Any : return True # eg: issubclass(tuple, tuple) if klass_origin is None and check_type_origin is None : return issubclass ( klass , check_type ) # eg: issubclass(tuple[int], tuple) if klass_origin is not None and check_type_origin is None : return issubclass ( klass_origin , check_type ) # eg: issubclass(tuple, tuple[int]) if klass_origin is None and check_type_origin is not None : return issubclass ( klass , check_type_origin ) and not check_type_args # eg: issubclass(tuple[int], tuple[int]) if klass_origin is not None and check_type_origin is not None : # NOTE: Considering all container types covariant for simplicity (mypy may be more strict). # # The builtin mutable containers (list, dict, etc) are invariant (klass_args == # check_type_args), but the interfaces (Mapping, Sequence, etc) and immutable containers are # covariant. if check_type_args : if not ( len ( klass_args ) == len ( check_type_args ) and all ( # check subclass OR things like \"...\" lenient_issubclass ( klass_arg , check_type_arg ) or klass_arg is check_type_arg for ( klass_arg , check_type_arg ) in zip ( klass_args , check_type_args ) ) ): return False return lenient_issubclass ( klass_origin , check_type_origin ) # Shouldn't happen, but need to explicitly say \"x is not None\" to narrow mypy types. raise NotImplementedError ( \"The origin conditions don't cover all cases!\" ) def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" if is_generic_alias ( klass ): bases = ( klass ,) else : bases = klass . __orig_bases__ # type: ignore for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \" { klass . __name__ } must subclass a subscripted Generic\" ) def get_annotation_from_value ( value : Any ) -> Any : if isinstance ( value , ( NoneType , bool , bytes , date , datetime , float , int , str )): return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )): first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ): if isinstance ( value , tuple ): return tuple [ first_type , ... ] # type: ignore return type ( value )[ first_type ] # type: ignore if isinstance ( value , dict ): items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items )[ 0 ]) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ): return dict [ first_key_type , first_value_type ] # type: ignore # TODO: Implement with TypedDict to support Struct types...? raise NotImplementedError ( f \"Unable to determine type of { value } \" ) def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ... ]]) -> bool : if not ( isinstance ( klass , ( type , TypeVar )) or is_Annotated ( klass ) or klass is Any ): return False if isinstance ( class_or_tuple , tuple ): return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE: py 3.10 supports issubclass with Unions (eg: `issubclass(str, str | int)`) if is_union_hint ( check_type ): return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type ) def _tidy_return ( return_annotation : Any , * , force_tuple_return : bool ) -> Any : if not force_tuple_return : return return_annotation if lenient_issubclass ( get_origin ( return_annotation ), tuple ): return get_args ( return_annotation ) return ( return_annotation ,) def tidy_signature ( fn : Callable [ ... , Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), ) def signature ( fn : Callable [ ... , Any ], * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \"\"\"Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\"\" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , ) ############################################# # Helpers for typing across python versions # ############################################# # # Focusing on 3.9+ (for now) if sys . version_info < ( 3 , 10 ): # pragma: no cover def is_union ( type_ : Any ) -> bool : return type_ is Union def is_typeddict ( type_ : Any ) -> bool : # mypy doesn't know of typing._TypedDictMeta, but `type: ignore` would be \"unused\" (and error) # on other python versions. if TYPE_CHECKING : from typing import _TypedDict as _TypedDictMeta else : from typing import _TypedDictMeta return isinstance ( type_ , _TypedDictMeta ) else : # pragma: no cover from typing import is_typeddict as is_typeddict # noqa: F401 # mypy doesn't know of types.UnionType yet, but `type: ignore` would be \"unused\" # (and error) on other python versions. @no_type_check def is_union ( type_ : Any ) -> bool : # `Union[int, str]` or `int | str` return type_ is Union or type_ is types . UnionType # noqa: E721 def is_Annotated ( type_ : Any ) -> bool : return isinstance ( type_ , _AnnotatedAlias ) def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore return isinstance ( type_ , _GenericAlias ) def is_optional_hint ( type_ : Any ) -> bool : # Optional[x] is represented as Union[x, NoneType] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ ) def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union Variables TYPE_CHECKING Functions get_annotation_from_value def get_annotation_from_value ( value : Any ) -> Any View Source def get_annotation_from_value ( value : Any ) -> Any : if isinstance ( value , ( NoneType , bool , bytes , date , datetime , float , int , str )) : return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )) : first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ) : if isinstance ( value , tuple ) : return tuple [ first_type, ... ] # type : ignore return type ( value ) [ first_type ] # type : ignore if isinstance ( value , dict ) : items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items ) [ 0 ] ) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ) : return dict [ first_key_type, first_value_type ] # type : ignore # TODO : Implement with TypedDict to support Struct types ... ? raise NotImplementedError ( f \"Unable to determine type of {value}\" ) get_class_type_vars def get_class_type_vars ( klass : type ) -> tuple [ type , ... ] Get the bound type variables from a class NOTE: Only vars from the first Generic in the mro with all variables bound will be returned. View Source def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" if is_generic_alias ( klass ): bases = ( klass ,) else : bases = klass . __orig_bases__ # type: ignore for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \"{klass.__name__} must subclass a subscripted Generic\" ) is_Annotated def is_Annotated ( type_ : Any ) -> bool View Source def is_Annotated ( type_ : Any ) -> bool : return isinstance ( type_ , _AnnotatedAlias ) is_generic_alias def is_generic_alias ( type_ : Any ) -> bool View Source def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore return isinstance ( type_ , _GenericAlias ) is_optional_hint def is_optional_hint ( type_ : Any ) -> bool View Source def is_optional_hint ( type_ : Any ) -> bool : # Optional [ x ] is represented as Union [ x, NoneType ] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ ) is_union def is_union ( type_ : Any ) -> bool View Source @no_type_check def is_union ( type_ : Any ) -> bool : # ` Union [ int, str ] ` or ` int | str ` return type_ is Union or type_ is types . UnionType # noqa : E721 is_union_hint def is_union_hint ( type_ : Any ) -> bool View Source def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union lenient_issubclass def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ... ]] ) -> bool View Source def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ...]]) -> bool : if not ( isinstance ( klass , ( type , TypeVar )) or is_Annotated ( klass ) or klass is Any ) : return False if isinstance ( class_or_tuple , tuple ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE : py 3.10 supports issubclass with Unions ( eg : ` issubclass ( str , str | int ) ` ) if is_union_hint ( check_type ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type ) signature def signature ( fn : collections . abc . Callable [ ... , typing . Any ], * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True ) -> inspect . Signature Convenience wrapper around inspect.signature . The returned Signature will have cls / self parameters removed if remove_owner is True and tuple[...] converted to tuple(...) in the return_annotation . View Source def signature ( fn : Callable [ ..., Any ] , * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \" \"\" Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\" \" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , ) tidy_signature def tidy_signature ( fn : collections . abc . Callable [ ... , typing . Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False ) -> inspect . Signature View Source def tidy_signature ( fn : Callable [..., Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), ) Classes NoneType class NoneType ( / , * args , ** kwargs )","title":"Type Hints"},{"location":"reference/arti/internal/type_hints/#module-artiinternaltype_hints","text":"None None View Source import inspect import sys import types from collections.abc import Callable from datetime import date , datetime from typing import _AnnotatedAlias # type: ignore from typing import ( TYPE_CHECKING , Annotated , Any , TypeVar , Union , cast , get_args , get_origin , get_type_hints , no_type_check , ) NoneType = cast ( type , type ( None )) # mypy otherwise treats type(None) as an object def _check_issubclass ( klass : Any , check_type : type ) -> bool : # If a hint is Annotated, we want to unwrap the underlying type and discard the rest of the # annotations. klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) if klass_origin is Annotated : klass = klass_args [ 0 ] klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) if isinstance ( klass , TypeVar ): klass = cast ( type , Any ) if klass . __bound__ is None else klass . __bound__ klass_origin , klass_args = get_origin ( klass ), get_args ( klass ) check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if check_type_origin is Annotated : check_type = check_type_args [ 0 ] check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if isinstance ( check_type , TypeVar ): check_type = cast ( type , Any ) if check_type . __bound__ is None else check_type . __bound__ check_type_origin , check_type_args = get_origin ( check_type ), get_args ( check_type ) if klass is Any : return check_type is Any if check_type is Any : return True # eg: issubclass(tuple, tuple) if klass_origin is None and check_type_origin is None : return issubclass ( klass , check_type ) # eg: issubclass(tuple[int], tuple) if klass_origin is not None and check_type_origin is None : return issubclass ( klass_origin , check_type ) # eg: issubclass(tuple, tuple[int]) if klass_origin is None and check_type_origin is not None : return issubclass ( klass , check_type_origin ) and not check_type_args # eg: issubclass(tuple[int], tuple[int]) if klass_origin is not None and check_type_origin is not None : # NOTE: Considering all container types covariant for simplicity (mypy may be more strict). # # The builtin mutable containers (list, dict, etc) are invariant (klass_args == # check_type_args), but the interfaces (Mapping, Sequence, etc) and immutable containers are # covariant. if check_type_args : if not ( len ( klass_args ) == len ( check_type_args ) and all ( # check subclass OR things like \"...\" lenient_issubclass ( klass_arg , check_type_arg ) or klass_arg is check_type_arg for ( klass_arg , check_type_arg ) in zip ( klass_args , check_type_args ) ) ): return False return lenient_issubclass ( klass_origin , check_type_origin ) # Shouldn't happen, but need to explicitly say \"x is not None\" to narrow mypy types. raise NotImplementedError ( \"The origin conditions don't cover all cases!\" ) def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" if is_generic_alias ( klass ): bases = ( klass ,) else : bases = klass . __orig_bases__ # type: ignore for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \" { klass . __name__ } must subclass a subscripted Generic\" ) def get_annotation_from_value ( value : Any ) -> Any : if isinstance ( value , ( NoneType , bool , bytes , date , datetime , float , int , str )): return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )): first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ): if isinstance ( value , tuple ): return tuple [ first_type , ... ] # type: ignore return type ( value )[ first_type ] # type: ignore if isinstance ( value , dict ): items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items )[ 0 ]) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ): return dict [ first_key_type , first_value_type ] # type: ignore # TODO: Implement with TypedDict to support Struct types...? raise NotImplementedError ( f \"Unable to determine type of { value } \" ) def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ... ]]) -> bool : if not ( isinstance ( klass , ( type , TypeVar )) or is_Annotated ( klass ) or klass is Any ): return False if isinstance ( class_or_tuple , tuple ): return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE: py 3.10 supports issubclass with Unions (eg: `issubclass(str, str | int)`) if is_union_hint ( check_type ): return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type ) def _tidy_return ( return_annotation : Any , * , force_tuple_return : bool ) -> Any : if not force_tuple_return : return return_annotation if lenient_issubclass ( get_origin ( return_annotation ), tuple ): return get_args ( return_annotation ) return ( return_annotation ,) def tidy_signature ( fn : Callable [ ... , Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), ) def signature ( fn : Callable [ ... , Any ], * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \"\"\"Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\"\" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , ) ############################################# # Helpers for typing across python versions # ############################################# # # Focusing on 3.9+ (for now) if sys . version_info < ( 3 , 10 ): # pragma: no cover def is_union ( type_ : Any ) -> bool : return type_ is Union def is_typeddict ( type_ : Any ) -> bool : # mypy doesn't know of typing._TypedDictMeta, but `type: ignore` would be \"unused\" (and error) # on other python versions. if TYPE_CHECKING : from typing import _TypedDict as _TypedDictMeta else : from typing import _TypedDictMeta return isinstance ( type_ , _TypedDictMeta ) else : # pragma: no cover from typing import is_typeddict as is_typeddict # noqa: F401 # mypy doesn't know of types.UnionType yet, but `type: ignore` would be \"unused\" # (and error) on other python versions. @no_type_check def is_union ( type_ : Any ) -> bool : # `Union[int, str]` or `int | str` return type_ is Union or type_ is types . UnionType # noqa: E721 def is_Annotated ( type_ : Any ) -> bool : return isinstance ( type_ , _AnnotatedAlias ) def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore return isinstance ( type_ , _GenericAlias ) def is_optional_hint ( type_ : Any ) -> bool : # Optional[x] is represented as Union[x, NoneType] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ ) def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union","title":"Module arti.internal.type_hints"},{"location":"reference/arti/internal/type_hints/#variables","text":"TYPE_CHECKING","title":"Variables"},{"location":"reference/arti/internal/type_hints/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/type_hints/#get_annotation_from_value","text":"def get_annotation_from_value ( value : Any ) -> Any View Source def get_annotation_from_value ( value : Any ) -> Any : if isinstance ( value , ( NoneType , bool , bytes , date , datetime , float , int , str )) : return type ( value ) if isinstance ( value , ( tuple , list , set , frozenset )) : first , * tail = tuple ( value ) first_type = type ( first ) if all ( isinstance ( v , first_type ) for v in tail ) : if isinstance ( value , tuple ) : return tuple [ first_type, ... ] # type : ignore return type ( value ) [ first_type ] # type : ignore if isinstance ( value , dict ) : items = value . items () first_key_type , first_value_type = ( type ( v ) for v in tuple ( items ) [ 0 ] ) if all ( isinstance ( k , first_key_type ) and isinstance ( v , first_value_type ) for ( k , v ) in items ) : return dict [ first_key_type, first_value_type ] # type : ignore # TODO : Implement with TypedDict to support Struct types ... ? raise NotImplementedError ( f \"Unable to determine type of {value}\" )","title":"get_annotation_from_value"},{"location":"reference/arti/internal/type_hints/#get_class_type_vars","text":"def get_class_type_vars ( klass : type ) -> tuple [ type , ... ] Get the bound type variables from a class NOTE: Only vars from the first Generic in the mro with all variables bound will be returned. View Source def get_class_type_vars ( klass : type ) -> tuple [ type , ... ]: \"\"\"Get the bound type variables from a class NOTE: Only vars from the *first* Generic in the mro *with all variables bound* will be returned. \"\"\" if is_generic_alias ( klass ): bases = ( klass ,) else : bases = klass . __orig_bases__ # type: ignore for base in bases : base_origin = get_origin ( base ) if base_origin is None : continue args = get_args ( base ) if any ( isinstance ( arg , TypeVar ) for arg in args ): continue return args raise TypeError ( f \"{klass.__name__} must subclass a subscripted Generic\" )","title":"get_class_type_vars"},{"location":"reference/arti/internal/type_hints/#is_annotated","text":"def is_Annotated ( type_ : Any ) -> bool View Source def is_Annotated ( type_ : Any ) -> bool : return isinstance ( type_ , _AnnotatedAlias )","title":"is_Annotated"},{"location":"reference/arti/internal/type_hints/#is_generic_alias","text":"def is_generic_alias ( type_ : Any ) -> bool View Source def is_generic_alias ( type_ : Any ) -> bool : from typing import _GenericAlias # type: ignore return isinstance ( type_ , _GenericAlias )","title":"is_generic_alias"},{"location":"reference/arti/internal/type_hints/#is_optional_hint","text":"def is_optional_hint ( type_ : Any ) -> bool View Source def is_optional_hint ( type_ : Any ) -> bool : # Optional [ x ] is represented as Union [ x, NoneType ] return is_union ( get_origin ( type_ )) and NoneType in get_args ( type_ )","title":"is_optional_hint"},{"location":"reference/arti/internal/type_hints/#is_union","text":"def is_union ( type_ : Any ) -> bool View Source @no_type_check def is_union ( type_ : Any ) -> bool : # ` Union [ int, str ] ` or ` int | str ` return type_ is Union or type_ is types . UnionType # noqa : E721","title":"is_union"},{"location":"reference/arti/internal/type_hints/#is_union_hint","text":"def is_union_hint ( type_ : Any ) -> bool View Source def is_union_hint ( type_ : Any ) -> bool : return get_origin ( type_ ) is Union","title":"is_union_hint"},{"location":"reference/arti/internal/type_hints/#lenient_issubclass","text":"def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ... ]] ) -> bool View Source def lenient_issubclass ( klass : Any , class_or_tuple : Union [ type , tuple [ type , ...]]) -> bool : if not ( isinstance ( klass , ( type , TypeVar )) or is_Annotated ( klass ) or klass is Any ) : return False if isinstance ( class_or_tuple , tuple ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in class_or_tuple ) check_type = class_or_tuple # NOTE : py 3.10 supports issubclass with Unions ( eg : ` issubclass ( str , str | int ) ` ) if is_union_hint ( check_type ) : return any ( lenient_issubclass ( klass , subtype ) for subtype in get_args ( check_type )) return _check_issubclass ( klass , check_type )","title":"lenient_issubclass"},{"location":"reference/arti/internal/type_hints/#signature","text":"def signature ( fn : collections . abc . Callable [ ... , typing . Any ], * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True ) -> inspect . Signature Convenience wrapper around inspect.signature . The returned Signature will have cls / self parameters removed if remove_owner is True and tuple[...] converted to tuple(...) in the return_annotation . View Source def signature ( fn : Callable [ ..., Any ] , * , follow_wrapped : bool = True , force_tuple_return : bool = False , remove_owner : bool = True , ) -> inspect . Signature : \" \"\" Convenience wrapper around `inspect.signature`. The returned Signature will have `cls`/`self` parameters removed if `remove_owner` is `True` and `tuple[...]` converted to `tuple(...)` in the `return_annotation`. \"\" \" return tidy_signature ( fn = fn , sig = inspect . signature ( fn , follow_wrapped = follow_wrapped ), force_tuple_return = force_tuple_return , remove_owner = remove_owner , )","title":"signature"},{"location":"reference/arti/internal/type_hints/#tidy_signature","text":"def tidy_signature ( fn : collections . abc . Callable [ ... , typing . Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False ) -> inspect . Signature View Source def tidy_signature ( fn : Callable [..., Any ], sig : inspect . Signature , * , force_tuple_return : bool = False , remove_owner : bool = False , ) -> inspect . Signature : type_hints = get_type_hints ( fn , include_extras = True ) sig = sig . replace ( return_annotation = type_hints . get ( \"return\" , sig . return_annotation )) return sig . replace ( parameters = [ p . replace ( annotation = type_hints . get ( p . name , p . annotation )) for p in sig . parameters . values () if ( p . name not in ( \"cls\" , \"self\" ) if remove_owner else True ) ], return_annotation = ( sig . empty if sig . return_annotation is sig . empty else _tidy_return ( sig . return_annotation , force_tuple_return = force_tuple_return ) ), )","title":"tidy_signature"},{"location":"reference/arti/internal/type_hints/#classes","text":"","title":"Classes"},{"location":"reference/arti/internal/type_hints/#nonetype","text":"class NoneType ( / , * args , ** kwargs )","title":"NoneType"},{"location":"reference/arti/internal/utils/","text":"Module arti.internal.utils None None View Source import importlib import inspect import os.path import pkgutil import threading from collections.abc import Callable , Generator , Iterator , MutableMapping from contextlib import contextmanager from tempfile import TemporaryDirectory from types import GenericAlias , ModuleType from typing import ( IO , Any , ClassVar , Generic , Optional , SupportsIndex , TypeVar , Union , cast , overload , ) from box import Box from frozendict.core import frozendict as _frozendict from multimethod import multidispatch from arti.internal.type_hints import lenient_issubclass , tidy_signature from arti.internal.vendored.setuptools import find_namespace_packages _K = TypeVar ( \"_K\" ) _V = TypeVar ( \"_V\" ) class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ]) -> str : return type_ . __name__ class_name = cast ( Callable [[], str ], ClassName ) PropReturn = TypeVar ( \"PropReturn\" ) class classproperty ( Generic [ PropReturn ]): \"\"\"Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc). \"\"\" def __init__ ( self , f : Callable [ ... , PropReturn ]) -> None : self . f = f if isinstance ( self . f , classmethod ): self . f = lambda type_ : f . __get__ ( None , type_ )() def __get__ ( self , obj : Any , type_ : Any ) -> PropReturn : return self . f ( type_ ) RETURN = TypeVar ( \"RETURN\" ) REGISTERED = TypeVar ( \"REGISTERED\" , bound = Callable [ ... , Any ]) # This may be less useful once mypy supports ParamSpecs - after that, we might be able to define # multidispatch with a ParamSpec and have mypy check the handlers' arguments are covariant. class dispatch ( multidispatch [ RETURN ]): \"\"\"Multiple dispatch for a set of functions based on parameter type. Usage is similar to `@functools.singledispatch`. The original definition defines the \"spec\" that subsequent handlers must follow, namely the name and (base)class of parameters. \"\"\" def __init__ ( self , func : Callable [ ... , RETURN ]) -> None : super () . __init__ ( func ) self . clean_signature = tidy_signature ( func , self . signature ) @overload def register ( self , __func : REGISTERED ) -> REGISTERED : ... @overload def register ( self , * args : type ) -> Callable [[ REGISTERED ], REGISTERED ]: ... def register ( self , * args : Any ) -> Callable [[ REGISTERED ], REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ], \"__annotations__\" ): func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ): raise TypeError ( f \"Expected ` { func . __name__ } ` to have { sorted ( set ( spec . parameters )) } parameters, got { sorted ( set ( sig . parameters )) } \" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ], spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be { spec_param . kind } , got { sig_param . kind } \" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be a subclass of { spec_param . annotation } , got { sig_param . annotation } \" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } ` return to match { spec . return_annotation } , got { sig . return_annotation } \" ) return super () . register ( * args ) # type: ignore # frozendict is useful for models to preserve hashability (eg: key in a dict). Unfortunately, there # are issues deepcopying types.GenericAlias[1,2] (eg: using in a pydantic model). We can work around # this by replacing types.GenericAlias with typing.Generic (which returns a typing._GenericAlias). # # NOTE: The GenericAlias deepcopy issue has been resolved in 3.9.8 and 3.10.1. # # 1: https://github.com/Marco-Sulla/python-frozendict/issues/29 # 2: https://bugs.python.org/issue45167 class frozendict ( Generic [ _K , _V ], _frozendict [ _K , _V ]): pass def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { os . sep . join ([ path , * name . split ( \".\" )]): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) } _int_sub = TypeVar ( \"_int_sub\" , bound = \"_int\" ) class _int ( int ): def __repr__ ( self ) -> str : return f \" { qname ( self ) } ( { int ( self ) } )\" def __str__ ( self ) -> str : return str ( int ( self )) # Stock magics. # # Using \"self: TypeVar\" so mypy will detect the returned subclass (rather than _int). def __add__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __add__ ( x )) def __and__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __and__ ( n )) def __ceil__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __ceil__ ()) def __floor__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __floor__ ()) def __floordiv__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __floordiv__ ( x )) def __invert__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __invert__ ()) def __lshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __lshift__ ( n )) def __mod__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __mod__ ( x )) def __mul__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __mul__ ( x )) def __neg__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __neg__ ()) def __or__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __or__ ( n )) def __pos__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __pos__ ()) def __radd__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __radd__ ( x )) def __rand__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rand__ ( n )) def __rfloordiv__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rfloordiv__ ( x )) def __rlshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rlshift__ ( n )) def __rmod__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rmod__ ( x )) def __rmul__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rmul__ ( x )) def __ror__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __ror__ ( n )) def __round__ ( self : _int_sub , ndigits : SupportsIndex = 0 ) -> _int_sub : return type ( self )( super () . __round__ ( ndigits )) def __rrshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rrshift__ ( n )) def __rshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rshift__ ( n )) def __rsub__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rsub__ ( x )) def __rxor__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rxor__ ( n )) def __sub__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __sub__ ( x )) def __trunc__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __trunc__ ()) def __xor__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __xor__ ( n )) class int64 ( _int ): _min , _max = - ( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i : Union [ int , \"int64\" , \"uint64\" ]) -> \"int64\" : if i > cls . _max : if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else : raise ValueError ( f \" { i } is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min : raise ValueError ( f \" { i } is too small for int64.\" ) return super () . __new__ ( cls , i ) class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i : Union [ int , int64 , \"uint64\" ]) -> \"uint64\" : if i > cls . _max : raise ValueError ( f \" { i } is too large for uint64.\" ) if i < cls . _min : if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else : raise ValueError ( f \" { i } is negative. Hint: cast to int64 first.\" ) return super () . __new__ ( cls , i ) @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO [ Any ], None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d : with open ( os . path . join ( d , \"contents\" ), mode = mode ) as f : yield f def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix def register ( registry : dict [ _K , _V ], key : _K , value : _V , get_priority : Optional [ Callable [[ _V ], int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \" { key } is already registered with: { existing } !\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \" { key } with matching priority ( { existing_priority } ) is already registered with: { existing } !\" ) registry [ key ] = value return value def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ): return val . __qualname__ return type ( val ) . __qualname__ _K_str = TypeVar ( \"_K_str\" ) class TypedBox ( Box , MutableMapping [ _K_str , Union [ _V , MutableMapping [ _K_str , _V ]]]): \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type [ _V ]] # type: ignore @classmethod def __class_getitem__ ( cls , item : tuple [ type [ _K_str ], type [ _V ]]) -> GenericAlias : if not isinstance ( item , tuple ) or len ( item ) != 2 : raise TypeError ( f \" { cls . __name__ } expects a key and value type\" ) key_type , value_type = item if key_type is not str : raise TypeError ( f \" { cls . __name__ } key must be `str`\" ) return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : __name__ , \"__target_type__\" : value_type , }, ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__, so preempt Box from storing that (or # erroring if frozen). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super () . __setattr__ ( key , value ) def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ): return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ): casted = cast ( Any , self . __target_type__ ) . cast ( value ) if isinstance ( casted , self . __target_type__ ): return casted raise TypeError ( f \"Expected { tgt_name } .cast( { value } ) to return an instance of { tgt_name } , got: { casted } \" ) raise TypeError ( f \"Expected an instance of { tgt_name } , got: { value } \" ) # NOTE: Box uses name mangling (double __) to prevent conflicts with contained values. def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ): super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member elif item in self : raise ValueError ( f \" { item } is already set!\" ) else : super () . _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ _K_str , ... ] = ()) -> Iterator [ tuple [ _K_str , _V ]]: for k , v in self . items (): subroot = root + ( k ,) if isinstance ( v , TypedBox ): yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type: ignore Variables PropReturn REGISTERED RETURN Functions import_submodules def import_submodules ( path : list [ str ], name : str , * , lock : < built - in function allocate_lock > = < unlocked _thread . lock object at 0x7f664b4480c0 > ) -> dict [ str , module ] Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. path and name are usually provided from an existing module's __path__ and __name__ . This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. View Source def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { os . sep . join ([ path , * name . split ( \".\" )]): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) } named_temporary_file def named_temporary_file ( mode : str = 'w+b' ) -> collections . abc . Generator [ typing . IO [ typing . Any ], None , None ] Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows. View Source @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO[Any ] , None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d : with open ( os . path . join ( d , \"contents\" ), mode = mode ) as f : yield f ordinal def ordinal ( n : int ) -> str Convert an integer into its ordinal representation. View Source def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix qname def qname ( val : Union [ object , type ] ) -> str View Source def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ) : return val . __qualname__ return type ( val ). __qualname__ register def register ( registry : dict [ ~ _K , ~ _V ], key : ~ _K , value : ~ _V , get_priority : Optional [ collections . abc . Callable [[ ~ _V ], int ]] = None ) -> ~ _V View Source def register ( registry : dict [ _K, _V ] , key : _K , value : _V , get_priority : Optional [ Callable[[_V ] , int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \"{key} is already registered with: {existing}!\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \"{key} with matching priority ({existing_priority}) is already registered with: {existing}!\" ) registry [ key ] = value return value Classes ClassName class ClassName ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__ TypedBox class TypedBox ( * args : Any , default_box : bool = False , default_box_attr : Any = < object object at 0x7f664d253ed0 > , default_box_none_transform : bool = True , frozen_box : bool = False , camel_killer_box : bool = False , conversion_box : bool = True , modify_tuples_box : bool = False , box_safe_prefix : str = 'x' , box_duplicates : str = 'ignore' , box_intact_types : Union [ Tuple , List ] = (), box_recast : Dict = None , box_dots : bool = False , box_class : Union [ Dict , ForwardRef ( 'Box' )] = None , ** kwargs : Any ) View Source class TypedBox ( Box , MutableMapping [ _K_str, Union[_V, MutableMapping[_K_str, _V ] ]] ) : \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type[_V ] ] # type : ignore @classmethod def __class_getitem__ ( cls , item : tuple [ type[_K_str ] , type [ _V ] ] ) -> GenericAlias : if not isinstance ( item , tuple ) or len ( item ) != 2 : raise TypeError ( f \"{cls.__name__} expects a key and value type\" ) key_type , value_type = item if key_type is not str : raise TypeError ( f \"{cls.__name__} key must be `str`\" ) return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : __name__ , \"__target_type__\" : value_type , } , ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__ , so preempt Box from storing that ( or # erroring if frozen ). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super (). __setattr__ ( key , value ) def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ) : return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ) : casted = cast ( Any , self . __target_type__ ). cast ( value ) if isinstance ( casted , self . __target_type__ ) : return casted raise TypeError ( f \"Expected {tgt_name}.cast({value}) to return an instance of {tgt_name}, got: {casted}\" ) raise TypeError ( f \"Expected an instance of {tgt_name}, got: {value}\" ) # NOTE : Box uses name mangling ( double __ ) to prevent conflicts with contained values . def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ) : super (). _Box__convert_and_store ( item , value ) # pylint : disable = no - member elif item in self : raise ValueError ( f \"{item} is already set!\" ) else : super (). _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ _K_str, ... ] = ()) -> Iterator [ tuple[_K_str, _V ] ]: for k , v in self . items () : subroot = root + ( k ,) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore Ancestors (in MRO) box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container Descendants arti.internal.utils.TypedBox Static methods from_json def from_json ( json_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) from_msgpack def from_msgpack ( msgpack_bytes : bytes = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : bytes = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' ) from_toml def from_toml ( toml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args ) from_yaml def from_yaml ( yaml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args ) Methods clear def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) super () . clear () self . _box_config [ \" __safe_keys \" ]. clear () copy def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : return Box ( super (). copy (), ** self . __box_config ()) fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default =< object object at 0x7f664d253ed0 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ] items def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ] keys def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x )) merge_update def merge_update ( self , _Box__m = None , ** kwargs ) View Source def merge_update ( self , __m = None , ** kwargs ) : def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ()) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ()) merge_type = kwargs . get ( \"box_merge_lists\" ) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if __m : if hasattr ( __m , \"keys\" ) : for key in __m : convert_and_set ( key , __m [ key ] ) else : for key , value in __m : convert_and_set ( key , value ) for key in kwargs : convert_and_set ( key , kwargs [ key ] ) pop def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item popitem def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \" Empty box \" ) from None return key , self . pop ( key ) setdefault def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ()) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ()) self [ item ] = default return self [ item ] to_dict def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict to_json def to_json ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** json_kwargs ) : \"\"\" Transform the Box object into a JSON string . : param filename : If provided will save to file : param encoding : File encoding : param errors : How to handle encoding errors : param json_kwargs : additional arguments to pass to json . dump ( s ) : return : string of JSON ( if no filename provided ) \"\"\" return _to_json ( self . to_dict () , filename = filename , encoding = encoding , errors = errors , ** json_kwargs ) to_msgpack def to_msgpack ( self , filename : Union [ str , os . PathLike ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Union[str, PathLike] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package') to_toml def to_toml ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" ) : \"\"\" Transform the Box object into a toml string . : param filename : File to write toml object too : param encoding : File encoding : param errors : How to handle encoding errors : return : string of TOML ( if no filename provided ) \"\"\" return _to_toml ( self . to_dict () , filename = filename , encoding = encoding , errors = errors ) to_yaml def to_yaml ( self , filename : Union [ str , os . PathLike ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Union [ str , PathLike ] = None , default_flow_style : bool = False , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** yaml_kwargs , ) : \"\"\" Transform the Box object into a YAML string . : param filename : If provided will save to file : param default_flow_style : False will recursively dump dicts : param encoding : File encoding : param errors : How to handle encoding errors : param yaml_kwargs : additional arguments to pass to yaml . dump : return : string of YAML ( if no filename provided ) \"\"\" return _to_yaml ( self . to_dict () , filename = filename , default_flow_style = default_flow_style , encoding = encoding , errors = errors , ** yaml_kwargs , ) update def update ( self , _Box__m = None , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , __m = None , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if __m : if hasattr ( __m , \"keys\" ) : for k in __m : self . __convert_and_store ( k , __m [ k ] ) else : for k , v in __m : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] ) values def values ( ... ) D.values() -> an object providing a view on D's values walk def walk ( self , root : tuple [ ~ _K_str , ... ] = () ) -> collections . abc . Iterator [ tuple [ ~ _K_str , ~ _V ]] View Source def walk ( self , root : tuple [ _K_str , ...] = ()) -> Iterator [ tuple [ _K_str , _V ]] : for k , v in self . items () : subroot = root + ( k ,) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore class_name class class_name ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__ classproperty class classproperty ( f : collections . abc . Callable [ ... , ~ PropReturn ] ) View Source class classproperty ( Generic [ PropReturn ] ) : \"\"\"Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc). \"\"\" def __init__ ( self , f : Callable [ ..., PropReturn ] ) -> None : self . f = f if isinstance ( self . f , classmethod ) : self . f = lambda type_ : f . __get__ ( None , type_ )() def __get__ ( self , obj : Any , type_ : Any ) -> PropReturn : return self . f ( type_ ) Ancestors (in MRO) typing.Generic dispatch class dispatch ( func : collections . abc . Callable [ ... , ~ RETURN ] ) View Source class dispatch ( multidispatch [ RETURN ]): \"\"\"Multiple dispatch for a set of functions based on parameter type. Usage is similar to `@functools.singledispatch`. The original definition defines the \"spec\" that subsequent handlers must follow, namely the name and (base)class of parameters. \"\"\" def __init__ ( self , func : Callable [ ... , RETURN ]) -> None : super () . __init__ ( func ) self . clean_signature = tidy_signature ( func , self . signature ) @ overload def register ( self , __func : REGISTERED ) -> REGISTERED : ... @ overload def register ( self , * args : type ) -> Callable [[ REGISTERED ], REGISTERED ]: ... def register ( self , * args : Any ) -> Callable [[ REGISTERED ], REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ], \"__annotations__\" ): func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ): raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ], spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ): raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ): raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super () . register ( * args ) # type: ignore Ancestors (in MRO) multimethod.multidispatch multimethod.multimethod builtins.dict typing.Generic Instance variables docstring a descriptive docstring of all registered functions Methods clean def clean ( self ) Empty the cache. View Source def clean ( self ) : \"\"\" Empty the cache. \"\"\" for key in list ( self ) : if not isinstance ( key , signature ) : super () . __delitem__ ( key ) clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( self ) Return a new multimethod with the same methods. View Source def copy ( self ) : \"\"\" Return a new multimethod with the same methods. \"\"\" other = dict . __new__ ( type ( self )) other . update ( self ) return other evaluate def evaluate ( self ) Evaluate any pending forward references. This can be called explicitly when using forward references, otherwise cache misses will evaluate. View Source def evaluate ( self ): \"\"\"Evaluate any pending forward references. This can be called explicitly when using forward references, otherwise cache misses will evaluate. \"\"\" while self . pending : func = self . pending . pop () self [ get_types ( func )] = func fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys parents def parents ( self , types : tuple ) -> set Find immediate parents of potential key. View Source def parents ( self , types : tuple ) -> set : \"\"\"Find immediate parents of potential key.\"\"\" parents = { key for key in self if isinstance ( key , signature ) and key < types } return parents - { ancestor for parent in parents for ancestor in parent . parents } pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. register def register ( self , * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values frozendict class frozendict ( * args , ** kwargs ) View Source class frozendict ( Generic [ _K , _V ], _frozendict [ _K , _V ]): pass Ancestors (in MRO) typing.Generic frozendict.core.frozendict builtins.dict Static methods fromkeys def fromkeys ( * args , ** kwargs ) Identical to dict.fromkeys(). View Source @classmethod def fromkeys ( cls , * args , ** kwargs ) : r \"\"\" Identical to dict.fromkeys(). \"\"\" return cls ( dict . fromkeys ( * args , ** kwargs )) Methods clear def clear ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" ) copy def copy ( self ) Return the object itself, as it's an immutable. View Source def copy ( self ) : r \"\"\" Return the object itself , as it ' s an immutable. \"\"\" return self get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" ) popitem def popitem ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" ) setdefault def setdefault ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" ) update def update ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" ) values def values ( ... ) D.values() -> an object providing a view on D's values int64 class int64 ( / , * args , ** kwargs ) View Source class int64 ( _int ): _min , _max = -( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i: Union [ int , \"int64\" , \"uint64\" ]) -> \"int64\" : if i > cls . _max: if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else: raise ValueError ( f \"{i} is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min: raise ValueError ( f \"{i} is too small for int64.\" ) return super (). __new__ ( cls , i ) Ancestors (in MRO) arti.internal.utils._int builtins.int Class variables denominator imag numerator real Methods as_integer_ratio def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1) bit_count def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3 bit_length def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6 conjugate def conjugate ( ... ) Returns self, the complex conjugate of any int. from_bytes def from_bytes ( bytes , byteorder , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Indicates whether two's complement is used to represent the integer. to_bytes def to_bytes ( self , / , length , byteorder , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised. uint64 class uint64 ( / , * args , ** kwargs ) View Source class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i: Union [ int , int64 , \"uint64\" ]) -> \"uint64\" : if i > cls . _max: raise ValueError ( f \"{i} is too large for uint64.\" ) if i < cls . _min: if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else: raise ValueError ( f \"{i} is negative. Hint: cast to int64 first.\" ) return super (). __new__ ( cls , i ) Ancestors (in MRO) arti.internal.utils._int builtins.int Class variables denominator imag numerator real Methods as_integer_ratio def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1) bit_count def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3 bit_length def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6 conjugate def conjugate ( ... ) Returns self, the complex conjugate of any int. from_bytes def from_bytes ( bytes , byteorder , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Indicates whether two's complement is used to represent the integer. to_bytes def to_bytes ( self , / , length , byteorder , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised.","title":"Utils"},{"location":"reference/arti/internal/utils/#module-artiinternalutils","text":"None None View Source import importlib import inspect import os.path import pkgutil import threading from collections.abc import Callable , Generator , Iterator , MutableMapping from contextlib import contextmanager from tempfile import TemporaryDirectory from types import GenericAlias , ModuleType from typing import ( IO , Any , ClassVar , Generic , Optional , SupportsIndex , TypeVar , Union , cast , overload , ) from box import Box from frozendict.core import frozendict as _frozendict from multimethod import multidispatch from arti.internal.type_hints import lenient_issubclass , tidy_signature from arti.internal.vendored.setuptools import find_namespace_packages _K = TypeVar ( \"_K\" ) _V = TypeVar ( \"_V\" ) class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ]) -> str : return type_ . __name__ class_name = cast ( Callable [[], str ], ClassName ) PropReturn = TypeVar ( \"PropReturn\" ) class classproperty ( Generic [ PropReturn ]): \"\"\"Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc). \"\"\" def __init__ ( self , f : Callable [ ... , PropReturn ]) -> None : self . f = f if isinstance ( self . f , classmethod ): self . f = lambda type_ : f . __get__ ( None , type_ )() def __get__ ( self , obj : Any , type_ : Any ) -> PropReturn : return self . f ( type_ ) RETURN = TypeVar ( \"RETURN\" ) REGISTERED = TypeVar ( \"REGISTERED\" , bound = Callable [ ... , Any ]) # This may be less useful once mypy supports ParamSpecs - after that, we might be able to define # multidispatch with a ParamSpec and have mypy check the handlers' arguments are covariant. class dispatch ( multidispatch [ RETURN ]): \"\"\"Multiple dispatch for a set of functions based on parameter type. Usage is similar to `@functools.singledispatch`. The original definition defines the \"spec\" that subsequent handlers must follow, namely the name and (base)class of parameters. \"\"\" def __init__ ( self , func : Callable [ ... , RETURN ]) -> None : super () . __init__ ( func ) self . clean_signature = tidy_signature ( func , self . signature ) @overload def register ( self , __func : REGISTERED ) -> REGISTERED : ... @overload def register ( self , * args : type ) -> Callable [[ REGISTERED ], REGISTERED ]: ... def register ( self , * args : Any ) -> Callable [[ REGISTERED ], REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ], \"__annotations__\" ): func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ): raise TypeError ( f \"Expected ` { func . __name__ } ` to have { sorted ( set ( spec . parameters )) } parameters, got { sorted ( set ( sig . parameters )) } \" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ], spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be { spec_param . kind } , got { sig_param . kind } \" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } . { name } ` parameter to be a subclass of { spec_param . annotation } , got { sig_param . annotation } \" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ): raise TypeError ( f \"Expected the ` { func . __name__ } ` return to match { spec . return_annotation } , got { sig . return_annotation } \" ) return super () . register ( * args ) # type: ignore # frozendict is useful for models to preserve hashability (eg: key in a dict). Unfortunately, there # are issues deepcopying types.GenericAlias[1,2] (eg: using in a pydantic model). We can work around # this by replacing types.GenericAlias with typing.Generic (which returns a typing._GenericAlias). # # NOTE: The GenericAlias deepcopy issue has been resolved in 3.9.8 and 3.10.1. # # 1: https://github.com/Marco-Sulla/python-frozendict/issues/29 # 2: https://bugs.python.org/issue45167 class frozendict ( Generic [ _K , _V ], _frozendict [ _K , _V ]): pass def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { os . sep . join ([ path , * name . split ( \".\" )]): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) } _int_sub = TypeVar ( \"_int_sub\" , bound = \"_int\" ) class _int ( int ): def __repr__ ( self ) -> str : return f \" { qname ( self ) } ( { int ( self ) } )\" def __str__ ( self ) -> str : return str ( int ( self )) # Stock magics. # # Using \"self: TypeVar\" so mypy will detect the returned subclass (rather than _int). def __add__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __add__ ( x )) def __and__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __and__ ( n )) def __ceil__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __ceil__ ()) def __floor__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __floor__ ()) def __floordiv__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __floordiv__ ( x )) def __invert__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __invert__ ()) def __lshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __lshift__ ( n )) def __mod__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __mod__ ( x )) def __mul__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __mul__ ( x )) def __neg__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __neg__ ()) def __or__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __or__ ( n )) def __pos__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __pos__ ()) def __radd__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __radd__ ( x )) def __rand__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rand__ ( n )) def __rfloordiv__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rfloordiv__ ( x )) def __rlshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rlshift__ ( n )) def __rmod__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rmod__ ( x )) def __rmul__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rmul__ ( x )) def __ror__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __ror__ ( n )) def __round__ ( self : _int_sub , ndigits : SupportsIndex = 0 ) -> _int_sub : return type ( self )( super () . __round__ ( ndigits )) def __rrshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rrshift__ ( n )) def __rshift__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rshift__ ( n )) def __rsub__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __rsub__ ( x )) def __rxor__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __rxor__ ( n )) def __sub__ ( self : _int_sub , x : int ) -> _int_sub : return type ( self )( super () . __sub__ ( x )) def __trunc__ ( self : _int_sub ) -> _int_sub : return type ( self )( super () . __trunc__ ()) def __xor__ ( self : _int_sub , n : int ) -> _int_sub : return type ( self )( super () . __xor__ ( n )) class int64 ( _int ): _min , _max = - ( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i : Union [ int , \"int64\" , \"uint64\" ]) -> \"int64\" : if i > cls . _max : if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else : raise ValueError ( f \" { i } is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min : raise ValueError ( f \" { i } is too small for int64.\" ) return super () . __new__ ( cls , i ) class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i : Union [ int , int64 , \"uint64\" ]) -> \"uint64\" : if i > cls . _max : raise ValueError ( f \" { i } is too large for uint64.\" ) if i < cls . _min : if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else : raise ValueError ( f \" { i } is negative. Hint: cast to int64 first.\" ) return super () . __new__ ( cls , i ) @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO [ Any ], None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d : with open ( os . path . join ( d , \"contents\" ), mode = mode ) as f : yield f def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix def register ( registry : dict [ _K , _V ], key : _K , value : _V , get_priority : Optional [ Callable [[ _V ], int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \" { key } is already registered with: { existing } !\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \" { key } with matching priority ( { existing_priority } ) is already registered with: { existing } !\" ) registry [ key ] = value return value def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ): return val . __qualname__ return type ( val ) . __qualname__ _K_str = TypeVar ( \"_K_str\" ) class TypedBox ( Box , MutableMapping [ _K_str , Union [ _V , MutableMapping [ _K_str , _V ]]]): \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type [ _V ]] # type: ignore @classmethod def __class_getitem__ ( cls , item : tuple [ type [ _K_str ], type [ _V ]]) -> GenericAlias : if not isinstance ( item , tuple ) or len ( item ) != 2 : raise TypeError ( f \" { cls . __name__ } expects a key and value type\" ) key_type , value_type = item if key_type is not str : raise TypeError ( f \" { cls . __name__ } key must be `str`\" ) return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : __name__ , \"__target_type__\" : value_type , }, ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__, so preempt Box from storing that (or # erroring if frozen). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super () . __setattr__ ( key , value ) def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ): return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ): casted = cast ( Any , self . __target_type__ ) . cast ( value ) if isinstance ( casted , self . __target_type__ ): return casted raise TypeError ( f \"Expected { tgt_name } .cast( { value } ) to return an instance of { tgt_name } , got: { casted } \" ) raise TypeError ( f \"Expected an instance of { tgt_name } , got: { value } \" ) # NOTE: Box uses name mangling (double __) to prevent conflicts with contained values. def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ): super () . _Box__convert_and_store ( item , value ) # pylint: disable=no-member elif item in self : raise ValueError ( f \" { item } is already set!\" ) else : super () . _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ _K_str , ... ] = ()) -> Iterator [ tuple [ _K_str , _V ]]: for k , v in self . items (): subroot = root + ( k ,) if isinstance ( v , TypedBox ): yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type: ignore","title":"Module arti.internal.utils"},{"location":"reference/arti/internal/utils/#variables","text":"PropReturn REGISTERED RETURN","title":"Variables"},{"location":"reference/arti/internal/utils/#functions","text":"","title":"Functions"},{"location":"reference/arti/internal/utils/#import_submodules","text":"def import_submodules ( path : list [ str ], name : str , * , lock : < built - in function allocate_lock > = < unlocked _thread . lock object at 0x7f664b4480c0 > ) -> dict [ str , module ] Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. path and name are usually provided from an existing module's __path__ and __name__ . This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. View Source def import_submodules ( path : list [ str ], # module.__path__ is a list[str] name : str , * , lock : threading . Lock = threading . Lock (), ) -> dict [ str , ModuleType ]: \"\"\"Recursively import submodules. This can be useful with registry patterns to automatically discover and import submodules defining additional implementations. `path` and `name` are usually provided from an existing module's `__path__` and `__name__`. This function is thread-safe and supports namespace modules. NOTE: This inherently triggers eager imports, which has performance impacts and may cause import cycles. To reduce these issues, avoid calling during module definition. \"\"\" # pkgutil.iter_modules is not recursive and pkgutil.walk_packages does not handle namespace # packages... however we can leverage setuptools.find_namespace_packages, which was built for # exactly this. path_names = { p : name for p in path } path_names . update ( { os . sep . join ([ path , * name . split ( \".\" )]): f \" { root_name } . { name } \" for path , root_name in path_names . items () for name in find_namespace_packages ( path ) } ) with lock : return { name : importlib . import_module ( name ) for path , name in path_names . items () for _ , name , _ in pkgutil . iter_modules ([ path ], prefix = f \" { name } .\" ) }","title":"import_submodules"},{"location":"reference/arti/internal/utils/#named_temporary_file","text":"def named_temporary_file ( mode : str = 'w+b' ) -> collections . abc . Generator [ typing . IO [ typing . Any ], None , None ] Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows. View Source @contextmanager def named_temporary_file ( mode : str = \"w+b\" ) -> Generator [ IO[Any ] , None , None ]: \"\"\"Minimal alternative to tempfile.NamedTemporaryFile that can be re-opened on Windows.\"\"\" with TemporaryDirectory () as d : with open ( os . path . join ( d , \"contents\" ), mode = mode ) as f : yield f","title":"named_temporary_file"},{"location":"reference/arti/internal/utils/#ordinal","text":"def ordinal ( n : int ) -> str Convert an integer into its ordinal representation. View Source def ordinal ( n : int ) -> str : \"\"\"Convert an integer into its ordinal representation.\"\"\" n = int ( n ) suffix = [ \"th\" , \"st\" , \"nd\" , \"rd\" , \"th\" ][ min ( n % 10 , 4 )] if 11 <= ( n % 100 ) <= 13 : suffix = \"th\" return str ( n ) + suffix","title":"ordinal"},{"location":"reference/arti/internal/utils/#qname","text":"def qname ( val : Union [ object , type ] ) -> str View Source def qname ( val : Union [ object , type ]) -> str : if isinstance ( val , type ) : return val . __qualname__ return type ( val ). __qualname__","title":"qname"},{"location":"reference/arti/internal/utils/#register","text":"def register ( registry : dict [ ~ _K , ~ _V ], key : ~ _K , value : ~ _V , get_priority : Optional [ collections . abc . Callable [[ ~ _V ], int ]] = None ) -> ~ _V View Source def register ( registry : dict [ _K, _V ] , key : _K , value : _V , get_priority : Optional [ Callable[[_V ] , int ]] = None , ) -> _V : if key in registry : existing = registry [ key ] if get_priority is None : raise ValueError ( f \"{key} is already registered with: {existing}!\" ) existing_priority , new_priority = get_priority ( existing ), get_priority ( value ) if existing_priority > new_priority : return value if existing_priority == new_priority : raise ValueError ( f \"{key} with matching priority ({existing_priority}) is already registered with: {existing}!\" ) registry [ key ] = value return value","title":"register"},{"location":"reference/arti/internal/utils/#classes","text":"","title":"Classes"},{"location":"reference/arti/internal/utils/#classname","text":"class ClassName ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__","title":"ClassName"},{"location":"reference/arti/internal/utils/#typedbox","text":"class TypedBox ( * args : Any , default_box : bool = False , default_box_attr : Any = < object object at 0x7f664d253ed0 > , default_box_none_transform : bool = True , frozen_box : bool = False , camel_killer_box : bool = False , conversion_box : bool = True , modify_tuples_box : bool = False , box_safe_prefix : str = 'x' , box_duplicates : str = 'ignore' , box_intact_types : Union [ Tuple , List ] = (), box_recast : Dict = None , box_dots : bool = False , box_class : Union [ Dict , ForwardRef ( 'Box' )] = None , ** kwargs : Any ) View Source class TypedBox ( Box , MutableMapping [ _K_str, Union[_V, MutableMapping[_K_str, _V ] ]] ) : \"\"\"TypedBox holds a collection of typed values. Subclasses must set the __target_type__ to a base class for the contained values. \"\"\" __target_type__ : ClassVar [ type[_V ] ] # type : ignore @classmethod def __class_getitem__ ( cls , item : tuple [ type[_K_str ] , type [ _V ] ] ) -> GenericAlias : if not isinstance ( item , tuple ) or len ( item ) != 2 : raise TypeError ( f \"{cls.__name__} expects a key and value type\" ) key_type , value_type = item if key_type is not str : raise TypeError ( f \"{cls.__name__} key must be `str`\" ) return GenericAlias ( type ( cls . __name__ , ( cls ,), { \"__module__\" : __name__ , \"__target_type__\" : value_type , } , ), item , ) def __setattr__ ( self , key : str , value : Any ) -> None : # GenericAlias sets __orig_class__ after __init__ , so preempt Box from storing that ( or # erroring if frozen ). if key == \"__orig_class__\" : return object . __setattr__ ( self , key , value ) super (). __setattr__ ( key , value ) def __cast_value ( self , item : str , value : Any ) -> _V : if isinstance ( value , self . __target_type__ ) : return value tgt_name = self . __target_type__ . __name__ if hasattr ( self . __target_type__ , \"cast\" ) : casted = cast ( Any , self . __target_type__ ). cast ( value ) if isinstance ( casted , self . __target_type__ ) : return casted raise TypeError ( f \"Expected {tgt_name}.cast({value}) to return an instance of {tgt_name}, got: {casted}\" ) raise TypeError ( f \"Expected an instance of {tgt_name}, got: {value}\" ) # NOTE : Box uses name mangling ( double __ ) to prevent conflicts with contained values . def _Box__convert_and_store ( self , item : str , value : _V ) -> None : if isinstance ( value , dict ) : super (). _Box__convert_and_store ( item , value ) # pylint : disable = no - member elif item in self : raise ValueError ( f \"{item} is already set!\" ) else : super (). _Box__convert_and_store ( item , self . __cast_value ( item , value )) def walk ( self , root : tuple [ _K_str, ... ] = ()) -> Iterator [ tuple[_K_str, _V ] ]: for k , v in self . items () : subroot = root + ( k ,) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore","title":"TypedBox"},{"location":"reference/arti/internal/utils/#ancestors-in-mro","text":"box.box.Box builtins.dict collections.abc.MutableMapping collections.abc.Mapping collections.abc.Collection collections.abc.Sized collections.abc.Iterable collections.abc.Container","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#descendants","text":"arti.internal.utils.TypedBox","title":"Descendants"},{"location":"reference/arti/internal/utils/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/internal/utils/#from_json","text":"def from_json ( json_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. Parameters: Name Type Description Default json_string None string to pass to json.loads None filename None filename to open and pass to json.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or json.loads None Returns: Type Description None Box object from json data View Source @classmethod def from_json ( cls , json_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a json object string into a Box object. If the incoming json is a list, you must use BoxList.from_json. :param json_string: string to pass to `json.loads` :param filename: filename to open and pass to `json.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `json.loads` :return: Box object from json data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_json ( json_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not isinstance ( data , dict ) : raise BoxError ( f \"json data not returned as a dictionary, but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_json"},{"location":"reference/arti/internal/utils/#from_msgpack","text":"def from_msgpack ( msgpack_bytes : bytes = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' View Source @classmethod def from_msgpack ( cls , msgpack_bytes : bytes = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : raise BoxError ( 'msgpack is unavailable on this system, please install the \"msgpack\" package' )","title":"from_msgpack"},{"location":"reference/arti/internal/utils/#from_toml","text":"def from_toml ( toml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transforms a toml string or file into a Box object Parameters: Name Type Description Default toml_string None string to pass to toml.load None filename None filename to open and pass to toml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() None Returns: Type Description None Box object View Source @classmethod def from_toml ( cls , toml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transforms a toml string or file into a Box object :param toml_string: string to pass to `toml.load` :param filename: filename to open and pass to `toml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` :return: Box object \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_toml ( toml_string = toml_string , filename = filename , encoding = encoding , errors = errors ) return cls ( data , ** box_args )","title":"from_toml"},{"location":"reference/arti/internal/utils/#from_yaml","text":"def from_yaml ( yaml_string : str = None , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** kwargs ) -> 'Box' Transform a yaml object string into a Box object. By default will use SafeLoader. Parameters: Name Type Description Default yaml_string None string to pass to yaml.load None filename None filename to open and pass to yaml.load None encoding None File encoding None errors None How to handle encoding errors None kwargs None parameters to pass to Box() or yaml.load None Returns: Type Description None Box object from yaml data View Source @classmethod def from_yaml ( cls , yaml_string : str = None , filename : Union [ str, PathLike ] = None , encoding : str = \"utf-8\" , errors : str = \"strict\" , ** kwargs , ) -> \"Box\" : \"\"\" Transform a yaml object string into a Box object. By default will use SafeLoader. :param yaml_string: string to pass to `yaml.load` :param filename: filename to open and pass to `yaml.load` :param encoding: File encoding :param errors: How to handle encoding errors :param kwargs: parameters to pass to `Box()` or `yaml.load` :return: Box object from yaml data \"\"\" box_args = {} for arg in kwargs . copy () : if arg in BOX_PARAMETERS : box_args [ arg ] = kwargs . pop ( arg ) data = _from_yaml ( yaml_string = yaml_string , filename = filename , encoding = encoding , errors = errors , ** kwargs ) if not data : return cls ( ** box_args ) if not isinstance ( data , dict ) : raise BoxError ( f \"yaml data not returned as a dictionary but rather a {type(data).__name__}\" ) return cls ( data , ** box_args )","title":"from_yaml"},{"location":"reference/arti/internal/utils/#methods","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#clear","text":"def clear ( self ) D.clear() -> None. Remove all items from D. View Source def clear ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) super () . clear () self . _box_config [ \" __safe_keys \" ]. clear ()","title":"clear"},{"location":"reference/arti/internal/utils/#copy","text":"def copy ( self ) -> 'Box' D.copy() -> a shallow copy of D View Source def copy ( self ) -> \"Box\" : return Box ( super (). copy (), ** self . __box_config ())","title":"copy"},{"location":"reference/arti/internal/utils/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/arti/internal/utils/#get","text":"def get ( self , key , default =< object object at 0x7f664d253ed0 > ) Return the value for key if key is in the dictionary, else default. View Source def get ( self , key , default = NO_DEFAULT ) : if key not in self : if default is NO_DEFAULT : if self . _box_config [ \"default_box\" ] and self . _box_config [ \"default_box_none_transform\" ] : return self . __get_default ( key ) else : return None if isinstance ( default , dict ) and not isinstance ( default , Box ) : return Box ( default ) if isinstance ( default , list ) and not isinstance ( default , box . BoxList ) : return box . BoxList ( default ) return default return self [ key ]","title":"get"},{"location":"reference/arti/internal/utils/#items","text":"def items ( self , dotted : bool = False ) D.items() -> a set-like object providing a view on D's items View Source def items ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). items () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) return [ (k, self[k ] ) for k in self . keys ( dotted = True ) ]","title":"items"},{"location":"reference/arti/internal/utils/#keys","text":"def keys ( self , dotted : bool = False ) D.keys() -> a set-like object providing a view on D's keys View Source def keys ( self , dotted : Union [ bool ] = False ) : if not dotted : return super (). keys () if not self . _box_config [ \"box_dots\" ] : raise BoxError ( \"Cannot return dotted keys as this Box does not have `box_dots` enabled\" ) keys = set () for key , value in self . items () : added = False if isinstance ( key , str ) : if isinstance ( value , Box ) : for sub_key in value . keys ( dotted = True ) : keys . add ( f \"{key}.{sub_key}\" ) added = True elif isinstance ( value , box . BoxList ) : for pos in value . _dotted_helper () : keys . add ( f \"{key}{pos}\" ) added = True if not added : keys . add ( key ) return sorted ( keys , key = lambda x : str ( x ))","title":"keys"},{"location":"reference/arti/internal/utils/#merge_update","text":"def merge_update ( self , _Box__m = None , ** kwargs ) View Source def merge_update ( self , __m = None , ** kwargs ) : def convert_and_set ( k , v ) : intact_type = self . _box_config [ \"box_intact_types\" ] and isinstance ( v , self . _box_config [ \"box_intact_types\" ] ) if isinstance ( v , dict ) and not intact_type : # Box objects must be created in case they are already # in the ` converted ` box_config set v = self . _box_config [ \"box_class\" ] ( v , ** self . __box_config ()) if k in self and isinstance ( self [ k ] , dict ) : self [ k ] . merge_update ( v ) return if isinstance ( v , list ) and not intact_type : v = box . BoxList ( v , ** self . __box_config ()) merge_type = kwargs . get ( \"box_merge_lists\" ) if merge_type == \"extend\" and k in self and isinstance ( self [ k ] , list ) : self [ k ] . extend ( v ) return if merge_type == \"unique\" and k in self and isinstance ( self [ k ] , list ) : for item in v : if item not in self [ k ] : self [ k ] . append ( item ) return self . __setitem__ ( k , v ) if __m : if hasattr ( __m , \"keys\" ) : for key in __m : convert_and_set ( key , __m [ key ] ) else : for key , value in __m : convert_and_set ( key , value ) for key in kwargs : convert_and_set ( key , kwargs [ key ] )","title":"merge_update"},{"location":"reference/arti/internal/utils/#pop","text":"def pop ( self , key , * args ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError. View Source def pop ( self , key , * args ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if args : if len ( args ) != 1 : raise BoxError ( 'pop() takes only one optional argument \"default\"' ) try : item = self [ key ] except KeyError : return args [ 0 ] else : del self [ key ] return item try : item = self [ key ] except KeyError : raise BoxKeyError ( f \"{key}\" ) from None else : del self [ key ] return item","title":"pop"},{"location":"reference/arti/internal/utils/#popitem","text":"def popitem ( self ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. View Source def popitem ( self ) : if self . _box_config [ \" frozen_box \" ]: raise BoxError ( \" Box is frozen \" ) try : key = next ( self . __iter__ ()) except StopIteration : raise BoxKeyError ( \" Empty box \" ) from None return key , self . pop ( key )","title":"popitem"},{"location":"reference/arti/internal/utils/#setdefault","text":"def setdefault ( self , item , default = None ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. View Source def setdefault ( self , item , default = None ) : if item in self : return self [ item ] if self . _box_config [ \"box_dots\" ] : if item in _get_dot_paths ( self ) : return self [ item ] if isinstance ( default , dict ) : default = self . _box_config [ \"box_class\" ] ( default , ** self . __box_config ()) if isinstance ( default , list ) : default = box . BoxList ( default , ** self . __box_config ()) self [ item ] = default return self [ item ]","title":"setdefault"},{"location":"reference/arti/internal/utils/#to_dict","text":"def to_dict ( self ) -> Dict Turn the Box and sub Boxes back into a native python dictionary. Returns: Type Description None python dictionary of this Box View Source def to_dict ( self ) -> Dict : \"\"\" Turn the Box and sub Boxes back into a native python dictionary. :return: python dictionary of this Box \"\"\" out_dict = dict ( self ) for k , v in out_dict . items () : if v is self : out_dict [ k ] = out_dict elif isinstance ( v , Box ) : out_dict [ k ] = v . to_dict () elif isinstance ( v , box . BoxList ) : out_dict [ k ] = v . to_list () return out_dict","title":"to_dict"},{"location":"reference/arti/internal/utils/#to_json","text":"def to_json ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' , ** json_kwargs ) Transform the Box object into a JSON string. Parameters: Name Type Description Default filename None If provided will save to file None encoding None File encoding None errors None How to handle encoding errors None json_kwargs None additional arguments to pass to json.dump(s) None Returns: Type Description None string of JSON (if no filename provided) View Source def to_json ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** json_kwargs ) : \"\"\" Transform the Box object into a JSON string . : param filename : If provided will save to file : param encoding : File encoding : param errors : How to handle encoding errors : param json_kwargs : additional arguments to pass to json . dump ( s ) : return : string of JSON ( if no filename provided ) \"\"\" return _to_json ( self . to_dict () , filename = filename , encoding = encoding , errors = errors , ** json_kwargs )","title":"to_json"},{"location":"reference/arti/internal/utils/#to_msgpack","text":"def to_msgpack ( self , filename : Union [ str , os . PathLike ] = None , ** kwargs ) View Source def to_msgpack(self, filename: Union[str, PathLike] = None, **kwargs): raise BoxError('msgpack is unavailable on this system, please install the \"msgpack\" package')","title":"to_msgpack"},{"location":"reference/arti/internal/utils/#to_toml","text":"def to_toml ( self , filename : Union [ str , os . PathLike ] = None , encoding : str = 'utf-8' , errors : str = 'strict' ) Transform the Box object into a toml string. Parameters: Name Type Description Default filename None File to write toml object too None encoding None File encoding None errors None How to handle encoding errors None Returns: Type Description None string of TOML (if no filename provided) View Source def to_toml ( self , filename : Union [ str , PathLike ] = None , encoding : str = \" utf-8 \" , errors : str = \" strict \" ) : \"\"\" Transform the Box object into a toml string . : param filename : File to write toml object too : param encoding : File encoding : param errors : How to handle encoding errors : return : string of TOML ( if no filename provided ) \"\"\" return _to_toml ( self . to_dict () , filename = filename , encoding = encoding , errors = errors )","title":"to_toml"},{"location":"reference/arti/internal/utils/#to_yaml","text":"def to_yaml ( self , filename : Union [ str , os . PathLike ] = None , default_flow_style : bool = False , encoding : str = 'utf-8' , errors : str = 'strict' , ** yaml_kwargs ) Transform the Box object into a YAML string. Parameters: Name Type Description Default filename None If provided will save to file None default_flow_style None False will recursively dump dicts None encoding None File encoding None errors None How to handle encoding errors None yaml_kwargs None additional arguments to pass to yaml.dump None Returns: Type Description None string of YAML (if no filename provided) View Source def to_yaml ( self , filename : Union [ str , PathLike ] = None , default_flow_style : bool = False , encoding : str = \" utf-8 \" , errors : str = \" strict \" , ** yaml_kwargs , ) : \"\"\" Transform the Box object into a YAML string . : param filename : If provided will save to file : param default_flow_style : False will recursively dump dicts : param encoding : File encoding : param errors : How to handle encoding errors : param yaml_kwargs : additional arguments to pass to yaml . dump : return : string of YAML ( if no filename provided ) \"\"\" return _to_yaml ( self . to_dict () , filename = filename , default_flow_style = default_flow_style , encoding = encoding , errors = errors , ** yaml_kwargs , )","title":"to_yaml"},{"location":"reference/arti/internal/utils/#update","text":"def update ( self , _Box__m = None , ** kwargs ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] View Source def update ( self , __m = None , ** kwargs ) : if self . _box_config [ \"frozen_box\" ] : raise BoxError ( \"Box is frozen\" ) if __m : if hasattr ( __m , \"keys\" ) : for k in __m : self . __convert_and_store ( k , __m [ k ] ) else : for k , v in __m : self . __convert_and_store ( k , v ) for k in kwargs : self . __convert_and_store ( k , kwargs [ k ] )","title":"update"},{"location":"reference/arti/internal/utils/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/internal/utils/#walk","text":"def walk ( self , root : tuple [ ~ _K_str , ... ] = () ) -> collections . abc . Iterator [ tuple [ ~ _K_str , ~ _V ]] View Source def walk ( self , root : tuple [ _K_str , ...] = ()) -> Iterator [ tuple [ _K_str , _V ]] : for k , v in self . items () : subroot = root + ( k ,) if isinstance ( v , TypedBox ) : yield from v . walk ( root = subroot ) else : yield \".\" . join ( subroot ), v # type : ignore","title":"walk"},{"location":"reference/arti/internal/utils/#class_name","text":"class class_name ( / , * args , ** kwargs ) View Source class ClassName : def __get__ ( self , obj : Any , type_ : type [ Any ] ) -> str : return type_ . __name__","title":"class_name"},{"location":"reference/arti/internal/utils/#classproperty","text":"class classproperty ( f : collections . abc . Callable [ ... , ~ PropReturn ] ) View Source class classproperty ( Generic [ PropReturn ] ) : \"\"\"Access a @classmethod like a @property. Can be stacked above @classmethod (to satisfy pylint, mypy, etc). \"\"\" def __init__ ( self , f : Callable [ ..., PropReturn ] ) -> None : self . f = f if isinstance ( self . f , classmethod ) : self . f = lambda type_ : f . __get__ ( None , type_ )() def __get__ ( self , obj : Any , type_ : Any ) -> PropReturn : return self . f ( type_ )","title":"classproperty"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_1","text":"typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#dispatch","text":"class dispatch ( func : collections . abc . Callable [ ... , ~ RETURN ] ) View Source class dispatch ( multidispatch [ RETURN ]): \"\"\"Multiple dispatch for a set of functions based on parameter type. Usage is similar to `@functools.singledispatch`. The original definition defines the \"spec\" that subsequent handlers must follow, namely the name and (base)class of parameters. \"\"\" def __init__ ( self , func : Callable [ ... , RETURN ]) -> None : super () . __init__ ( func ) self . clean_signature = tidy_signature ( func , self . signature ) @ overload def register ( self , __func : REGISTERED ) -> REGISTERED : ... @ overload def register ( self , * args : type ) -> Callable [[ REGISTERED ], REGISTERED ]: ... def register ( self , * args : Any ) -> Callable [[ REGISTERED ], REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ], \"__annotations__\" ): func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ): raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ], spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ): raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ): raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super () . register ( * args ) # type: ignore","title":"dispatch"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_2","text":"multimethod.multidispatch multimethod.multimethod builtins.dict typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#instance-variables","text":"docstring a descriptive docstring of all registered functions","title":"Instance variables"},{"location":"reference/arti/internal/utils/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#clean","text":"def clean ( self ) Empty the cache. View Source def clean ( self ) : \"\"\" Empty the cache. \"\"\" for key in list ( self ) : if not isinstance ( key , signature ) : super () . __delitem__ ( key )","title":"clean"},{"location":"reference/arti/internal/utils/#clear_1","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/arti/internal/utils/#copy_1","text":"def copy ( self ) Return a new multimethod with the same methods. View Source def copy ( self ) : \"\"\" Return a new multimethod with the same methods. \"\"\" other = dict . __new__ ( type ( self )) other . update ( self ) return other","title":"copy"},{"location":"reference/arti/internal/utils/#evaluate","text":"def evaluate ( self ) Evaluate any pending forward references. This can be called explicitly when using forward references, otherwise cache misses will evaluate. View Source def evaluate ( self ): \"\"\"Evaluate any pending forward references. This can be called explicitly when using forward references, otherwise cache misses will evaluate. \"\"\" while self . pending : func = self . pending . pop () self [ get_types ( func )] = func","title":"evaluate"},{"location":"reference/arti/internal/utils/#fromkeys_1","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/arti/internal/utils/#get_1","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/arti/internal/utils/#items_1","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/arti/internal/utils/#keys_1","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/arti/internal/utils/#parents","text":"def parents ( self , types : tuple ) -> set Find immediate parents of potential key. View Source def parents ( self , types : tuple ) -> set : \"\"\"Find immediate parents of potential key.\"\"\" parents = { key for key in self if isinstance ( key , signature ) and key < types } return parents - { ancestor for parent in parents for ancestor in parent . parents }","title":"parents"},{"location":"reference/arti/internal/utils/#pop_1","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If the key is not found, return the default if given; otherwise, raise a KeyError.","title":"pop"},{"location":"reference/arti/internal/utils/#popitem_1","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/arti/internal/utils/#register_1","text":"def register ( self , * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore","title":"register"},{"location":"reference/arti/internal/utils/#setdefault_1","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/arti/internal/utils/#update_1","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/arti/internal/utils/#values_1","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/internal/utils/#frozendict","text":"class frozendict ( * args , ** kwargs ) View Source class frozendict ( Generic [ _K , _V ], _frozendict [ _K , _V ]): pass","title":"frozendict"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_3","text":"typing.Generic frozendict.core.frozendict builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/internal/utils/#fromkeys_2","text":"def fromkeys ( * args , ** kwargs ) Identical to dict.fromkeys(). View Source @classmethod def fromkeys ( cls , * args , ** kwargs ) : r \"\"\" Identical to dict.fromkeys(). \"\"\" return cls ( dict . fromkeys ( * args , ** kwargs ))","title":"fromkeys"},{"location":"reference/arti/internal/utils/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#clear_2","text":"def clear ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" )","title":"clear"},{"location":"reference/arti/internal/utils/#copy_2","text":"def copy ( self ) Return the object itself, as it's an immutable. View Source def copy ( self ) : r \"\"\" Return the object itself , as it ' s an immutable. \"\"\" return self","title":"copy"},{"location":"reference/arti/internal/utils/#get_2","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/arti/internal/utils/#items_2","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/arti/internal/utils/#keys_2","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/arti/internal/utils/#pop_2","text":"def pop ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" )","title":"pop"},{"location":"reference/arti/internal/utils/#popitem_2","text":"def popitem ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" )","title":"popitem"},{"location":"reference/arti/internal/utils/#setdefault_2","text":"def setdefault ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" )","title":"setdefault"},{"location":"reference/arti/internal/utils/#update_2","text":"def update ( self , * args , ** kwargs ) Function for not implemented method since the object is immutable View Source def immutable ( self , * args , ** kwargs ) : r \"\"\" Function for not implemented method since the object is immutable \"\"\" raise AttributeError ( f \" '{self.__class__.__name__}' object is read-only \" )","title":"update"},{"location":"reference/arti/internal/utils/#values_2","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/arti/internal/utils/#int64","text":"class int64 ( / , * args , ** kwargs ) View Source class int64 ( _int ): _min , _max = -( 2 ** 63 ), ( 2 ** 63 ) - 1 def __new__ ( cls , i: Union [ int , \"int64\" , \"uint64\" ]) -> \"int64\" : if i > cls . _max: if isinstance ( i , uint64 ): i = int ( i ) - uint64 . _max - 1 else: raise ValueError ( f \"{i} is too large for int64. Hint: cast to uint64 first.\" ) if i < cls . _min: raise ValueError ( f \"{i} is too small for int64.\" ) return super (). __new__ ( cls , i )","title":"int64"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_4","text":"arti.internal.utils._int builtins.int","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#class-variables","text":"denominator imag numerator real","title":"Class variables"},{"location":"reference/arti/internal/utils/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#as_integer_ratio","text":"def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1)","title":"as_integer_ratio"},{"location":"reference/arti/internal/utils/#bit_count","text":"def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3","title":"bit_count"},{"location":"reference/arti/internal/utils/#bit_length","text":"def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6","title":"bit_length"},{"location":"reference/arti/internal/utils/#conjugate","text":"def conjugate ( ... ) Returns self, the complex conjugate of any int.","title":"conjugate"},{"location":"reference/arti/internal/utils/#from_bytes","text":"def from_bytes ( bytes , byteorder , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Indicates whether two's complement is used to represent the integer.","title":"from_bytes"},{"location":"reference/arti/internal/utils/#to_bytes","text":"def to_bytes ( self , / , length , byteorder , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised.","title":"to_bytes"},{"location":"reference/arti/internal/utils/#uint64","text":"class uint64 ( / , * args , ** kwargs ) View Source class uint64 ( _int ): _min , _max = 0 , ( 2 ** 64 ) - 1 def __new__ ( cls , i: Union [ int , int64 , \"uint64\" ]) -> \"uint64\" : if i > cls . _max: raise ValueError ( f \"{i} is too large for uint64.\" ) if i < cls . _min: if isinstance ( i , int64 ): i = int ( i ) + cls . _max + 1 else: raise ValueError ( f \"{i} is negative. Hint: cast to int64 first.\" ) return super (). __new__ ( cls , i )","title":"uint64"},{"location":"reference/arti/internal/utils/#ancestors-in-mro_5","text":"arti.internal.utils._int builtins.int","title":"Ancestors (in MRO)"},{"location":"reference/arti/internal/utils/#class-variables_1","text":"denominator imag numerator real","title":"Class variables"},{"location":"reference/arti/internal/utils/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/internal/utils/#as_integer_ratio_1","text":"def as_integer_ratio ( self , / ) Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original int and with a positive denominator. (10).as_integer_ratio() (10, 1) (-10).as_integer_ratio() (-10, 1) (0).as_integer_ratio() (0, 1)","title":"as_integer_ratio"},{"location":"reference/arti/internal/utils/#bit_count_1","text":"def bit_count ( self , / ) Number of ones in the binary representation of the absolute value of self. Also known as the population count. bin(13) '0b1101' (13).bit_count() 3","title":"bit_count"},{"location":"reference/arti/internal/utils/#bit_length_1","text":"def bit_length ( self , / ) Number of bits necessary to represent self in binary. bin(37) '0b100101' (37).bit_length() 6","title":"bit_length"},{"location":"reference/arti/internal/utils/#conjugate_1","text":"def conjugate ( ... ) Returns self, the complex conjugate of any int.","title":"conjugate"},{"location":"reference/arti/internal/utils/#from_bytes_1","text":"def from_bytes ( bytes , byteorder , * , signed = False ) Return the integer represented by the given array of bytes. bytes Holds the array of bytes to convert. The argument must either support the buffer protocol or be an iterable object producing bytes. Bytes and bytearray are examples of built-in objects that support the buffer protocol. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Indicates whether two's complement is used to represent the integer.","title":"from_bytes"},{"location":"reference/arti/internal/utils/#to_bytes_1","text":"def to_bytes ( self , / , length , byteorder , * , signed = False ) Return an array of bytes representing an integer. length Length of bytes object to use. An OverflowError is raised if the integer is not representable with the given number of bytes. byteorder The byte order used to represent the integer. If byteorder is 'big', the most significant byte is at the beginning of the byte array. If byteorder is 'little', the most significant byte is at the end of the byte array. To request the native byte order of the host system, use `sys.byteorder' as the byte order value. signed Determines whether two's complement is used to represent the integer. If signed is False and a negative integer is given, an OverflowError is raised.","title":"to_bytes"},{"location":"reference/arti/io/","text":"Module arti.io None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections.abc import Sequence from types import ModuleType from typing import Any , Optional from arti.formats import Format from arti.internal.utils import dispatch , import_submodules from arti.storage import StoragePartition , _StoragePartition from arti.types import Collection , Type from arti.views import View _submodules : Optional [ dict [ str , ModuleType ]] = None def _discover () -> None : global _submodules if _submodules is None : _submodules = import_submodules ( __path__ , __name__ ) @dispatch def _read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : raise NotImplementedError ( f \"Reading { type ( storage_partitions [ 0 ]) } storage in { type ( format ) } format to { type ( view ) } view is not implemented.\" ) register_reader = _read . register def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : if not storage_partitions : # NOTE: Aside from simplifying this check up front, multiple dispatch with unknown list # element type can be ambiguous/error. raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not ( isinstance ( type_ , Collection ) and type_ . is_partitioned ): raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not { type_ } \" ) _discover () # TODO Checks that the returned data matches the Type/View # # Likely add a View method that can handle this type + schema checking, filtering to column/row subsets if necessary, etc return _read ( type_ , format , storage_partitions , view ) @dispatch # type: ignore def _write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> Optional [ _StoragePartition ]: raise NotImplementedError ( f \"Writing { type ( view ) } view into { type ( format ) } format in { type ( storage_partition ) } storage is not implemented.\" ) register_writer = _write . register def write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> _StoragePartition : _discover () if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition Sub-modules arti.io.json_localfile_python arti.io.json_stringliteral_python arti.io.pickle_localfile_python Functions read def read ( type_ : arti . types . Type , format : arti . formats . Format , storage_partitions : collections . abc . Sequence [ arti . storage . StoragePartition ], view : arti . views . View ) -> Any View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not ( isinstance ( type_ , Collection ) and type_ . is_partitioned ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) _discover () # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view ) register_reader def register_reader ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore register_writer def register_writer ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore write def write ( data : Any , type_ : arti . types . Type , format : arti . formats . Format , storage_partition : ~ _StoragePartition , view : arti . views . View ) -> ~ _StoragePartition View Source def write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> _StoragePartition : _discover () if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition","title":"Index"},{"location":"reference/arti/io/#module-artiio","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from collections.abc import Sequence from types import ModuleType from typing import Any , Optional from arti.formats import Format from arti.internal.utils import dispatch , import_submodules from arti.storage import StoragePartition , _StoragePartition from arti.types import Collection , Type from arti.views import View _submodules : Optional [ dict [ str , ModuleType ]] = None def _discover () -> None : global _submodules if _submodules is None : _submodules = import_submodules ( __path__ , __name__ ) @dispatch def _read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : raise NotImplementedError ( f \"Reading { type ( storage_partitions [ 0 ]) } storage in { type ( format ) } format to { type ( view ) } view is not implemented.\" ) register_reader = _read . register def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ], view : View ) -> Any : if not storage_partitions : # NOTE: Aside from simplifying this check up front, multiple dispatch with unknown list # element type can be ambiguous/error. raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not ( isinstance ( type_ , Collection ) and type_ . is_partitioned ): raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not { type_ } \" ) _discover () # TODO Checks that the returned data matches the Type/View # # Likely add a View method that can handle this type + schema checking, filtering to column/row subsets if necessary, etc return _read ( type_ , format , storage_partitions , view ) @dispatch # type: ignore def _write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> Optional [ _StoragePartition ]: raise NotImplementedError ( f \"Writing { type ( view ) } view into { type ( format ) } format in { type ( storage_partition ) } storage is not implemented.\" ) register_writer = _write . register def write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> _StoragePartition : _discover () if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition","title":"Module arti.io"},{"location":"reference/arti/io/#sub-modules","text":"arti.io.json_localfile_python arti.io.json_stringliteral_python arti.io.pickle_localfile_python","title":"Sub-modules"},{"location":"reference/arti/io/#functions","text":"","title":"Functions"},{"location":"reference/arti/io/#read","text":"def read ( type_ : arti . types . Type , format : arti . formats . Format , storage_partitions : collections . abc . Sequence [ arti . storage . StoragePartition ], view : arti . views . View ) -> Any View Source def read ( type_ : Type , format : Format , storage_partitions : Sequence [ StoragePartition ] , view : View ) -> Any : if not storage_partitions : # NOTE : Aside from simplifying this check up front , multiple dispatch with unknown list # element type can be ambiguous / error . raise FileNotFoundError ( \"No data\" ) if len ( storage_partitions ) > 1 and not ( isinstance ( type_ , Collection ) and type_ . is_partitioned ) : raise ValueError ( f \"Multiple partitions can only be read into a partitioned Collection, not {type_}\" ) _discover () # TODO Checks that the returned data matches the Type / View # # Likely add a View method that can handle this type + schema checking , filtering to column / row subsets if necessary , etc return _read ( type_ , format , storage_partitions , view )","title":"read"},{"location":"reference/arti/io/#register_reader","text":"def register_reader ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore","title":"register_reader"},{"location":"reference/arti/io/#register_writer","text":"def register_writer ( * args : Any ) -> collections . abc . Callable [[ ~ REGISTERED ], ~ REGISTERED ] Decorator for registering a function. Optionally call with types to return a decorator for unannotated functions. View Source def register ( self , * args : Any ) -> Callable [ [REGISTERED ] , REGISTERED ]: if len ( args ) == 1 and hasattr ( args [ 0 ] , \"__annotations__\" ) : func = args [ 0 ] sig = tidy_signature ( func , inspect . signature ( func )) spec = self . clean_signature if set ( sig . parameters ) != set ( spec . parameters ) : raise TypeError ( f \"Expected `{func.__name__}` to have {sorted(set(spec.parameters))} parameters, got {sorted(set(sig.parameters))}\" ) for name in sig . parameters : sig_param , spec_param = sig . parameters [ name ] , spec . parameters [ name ] if sig_param . kind != spec_param . kind : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be {spec_param.kind}, got {sig_param.kind}\" ) if sig_param . annotation is not Any and not lenient_issubclass ( sig_param . annotation , spec_param . annotation ) : raise TypeError ( f \"Expected the `{func.__name__}.{name}` parameter to be a subclass of {spec_param.annotation}, got {sig_param.annotation}\" ) if not lenient_issubclass ( sig . return_annotation , spec . return_annotation ) : raise TypeError ( f \"Expected the `{func.__name__}` return to match {spec.return_annotation}, got {sig.return_annotation}\" ) return super (). register ( * args ) # type : ignore","title":"register_writer"},{"location":"reference/arti/io/#write","text":"def write ( data : Any , type_ : arti . types . Type , format : arti . formats . Format , storage_partition : ~ _StoragePartition , view : arti . views . View ) -> ~ _StoragePartition View Source def write ( data : Any , type_ : Type , format : Format , storage_partition : _StoragePartition , view : View ) -> _StoragePartition : _discover () if ( updated := _write ( data , type_ , format , storage_partition , view )) is not None : return updated return storage_partition","title":"write"},{"location":"reference/arti/io/json_localfile_python/","text":"Module arti.io.json_localfile_python None None View Source import json from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Collection , Type from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_json_file ( path : str ) -> Any : with open ( path ) as file : return json . load ( file ) @register_reader def _read_json_localfile_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if isinstance ( type_ , Collection ) and type_ . is_partitioned : return list ( chain . from_iterable ( _read_json_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) else : assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_json_localfile_python ( data : Any , type_ : Type , format : JSON , storage_partition : LocalFilePartition , view : PythonBuiltin ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"w\" ) as file : json . dump ( data , file )","title":"Json Localfile Python"},{"location":"reference/arti/io/json_localfile_python/#module-artiiojson_localfile_python","text":"None None View Source import json from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Collection , Type from arti.views.python import PythonBuiltin # TODO: Do I need to inject the partition keys into the returned data? Likely useful... # Maybe a View option? def _read_json_file ( path : str ) -> Any : with open ( path ) as file : return json . load ( file ) @register_reader def _read_json_localfile_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if isinstance ( type_ , Collection ) and type_ . is_partitioned : return list ( chain . from_iterable ( _read_json_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) else : assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_json_localfile_python ( data : Any , type_ : Type , format : JSON , storage_partition : LocalFilePartition , view : PythonBuiltin ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"w\" ) as file : json . dump ( data , file )","title":"Module arti.io.json_localfile_python"},{"location":"reference/arti/io/json_stringliteral_python/","text":"Module arti.io.json_stringliteral_python None None View Source import json from collections.abc import Sequence from itertools import chain from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.literal import StringLiteralPartition , _not_written_err from arti.types import Collection , Type from arti.views.python import PythonBuiltin def _read_json_literal ( partition : StringLiteralPartition ) -> Any : if partition . value is None : raise _not_written_err return json . loads ( partition . value ) @register_reader def _read_json_stringliteral_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ StringLiteralPartition ], view : PythonBuiltin , ) -> Any : if isinstance ( type_ , Collection ) and type_ . is_partitioned : return list ( chain . from_iterable ( _read_json_literal ( storage_partition ) for storage_partition in storage_partitions ) ) else : assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_literal ( storage_partitions [ 0 ]) @register_writer def _write_json_stringliteral_python ( data : Any , type_ : Type , format : JSON , storage_partition : StringLiteralPartition , view : PythonBuiltin , ) -> StringLiteralPartition : if storage_partition . value is not None : # We can't overwrite the original value stored in LiteralStorage - on subsequent # `.discover_partitions`, a partition with the original value will still be used. raise ValueError ( \"Literals with a value already set cannot be written\" ) return storage_partition . copy ( update = { \"value\" : json . dumps ( data )})","title":"Json Stringliteral Python"},{"location":"reference/arti/io/json_stringliteral_python/#module-artiiojson_stringliteral_python","text":"None None View Source import json from collections.abc import Sequence from itertools import chain from typing import Any from arti.formats.json import JSON from arti.io import register_reader , register_writer from arti.storage.literal import StringLiteralPartition , _not_written_err from arti.types import Collection , Type from arti.views.python import PythonBuiltin def _read_json_literal ( partition : StringLiteralPartition ) -> Any : if partition . value is None : raise _not_written_err return json . loads ( partition . value ) @register_reader def _read_json_stringliteral_python ( type_ : Type , format : JSON , storage_partitions : Sequence [ StringLiteralPartition ], view : PythonBuiltin , ) -> Any : if isinstance ( type_ , Collection ) and type_ . is_partitioned : return list ( chain . from_iterable ( _read_json_literal ( storage_partition ) for storage_partition in storage_partitions ) ) else : assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_json_literal ( storage_partitions [ 0 ]) @register_writer def _write_json_stringliteral_python ( data : Any , type_ : Type , format : JSON , storage_partition : StringLiteralPartition , view : PythonBuiltin , ) -> StringLiteralPartition : if storage_partition . value is not None : # We can't overwrite the original value stored in LiteralStorage - on subsequent # `.discover_partitions`, a partition with the original value will still be used. raise ValueError ( \"Literals with a value already set cannot be written\" ) return storage_partition . copy ( update = { \"value\" : json . dumps ( data )})","title":"Module arti.io.json_stringliteral_python"},{"location":"reference/arti/io/pickle_localfile_python/","text":"Module arti.io.pickle_localfile_python None None View Source import pickle from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.pickle import Pickle from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Collection , Type from arti.views.python import PythonBuiltin def _read_pickle_file ( path : str ) -> Any : with open ( path , \"rb\" ) as file : return pickle . load ( file ) # nosec # User opted into pickle, ignore bandit check @register_reader def _read_pickle_localfile_python ( type_ : Type , format : Pickle , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if isinstance ( type_ , Collection ) and type_ . is_partitioned : return list ( chain . from_iterable ( _read_pickle_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) else : assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_pickle_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_pickle_localfile_python ( data : Any , type_ : Type , format : Pickle , storage_partition : LocalFilePartition , view : PythonBuiltin , ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"wb\" ) as file : pickle . dump ( data , file )","title":"Pickle Localfile Python"},{"location":"reference/arti/io/pickle_localfile_python/#module-artiiopickle_localfile_python","text":"None None View Source import pickle from collections.abc import Sequence from itertools import chain from pathlib import Path from typing import Any from arti.formats.pickle import Pickle from arti.io import register_reader , register_writer from arti.storage.local import LocalFilePartition from arti.types import Collection , Type from arti.views.python import PythonBuiltin def _read_pickle_file ( path : str ) -> Any : with open ( path , \"rb\" ) as file : return pickle . load ( file ) # nosec # User opted into pickle, ignore bandit check @register_reader def _read_pickle_localfile_python ( type_ : Type , format : Pickle , storage_partitions : Sequence [ LocalFilePartition ], view : PythonBuiltin , ) -> Any : if isinstance ( type_ , Collection ) and type_ . is_partitioned : return list ( chain . from_iterable ( _read_pickle_file ( storage_partition . path ) for storage_partition in storage_partitions ) ) else : assert len ( storage_partitions ) == 1 # Better error handled in base read return _read_pickle_file ( storage_partitions [ 0 ] . path ) @register_writer def _write_pickle_localfile_python ( data : Any , type_ : Type , format : Pickle , storage_partition : LocalFilePartition , view : PythonBuiltin , ) -> None : path = Path ( storage_partition . path ) path . parent . mkdir ( exist_ok = True , parents = True ) with path . open ( \"wb\" ) as file : pickle . dump ( data , file )","title":"Module arti.io.pickle_localfile_python"},{"location":"reference/arti/storage/","text":"Module arti.storage None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc import os from typing import Any , ClassVar , Generic , Optional , TypeVar from pydantic import Field , validator from arti.fingerprints import Fingerprint from arti.formats import Format from arti.internal.models import Model from arti.internal.type_hints import get_class_type_vars , lenient_issubclass from arti.internal.utils import frozendict from arti.partitions import CompositeKey , CompositeKeyTypes , PartitionKey from arti.storage._internal import InputFingerprints as InputFingerprints from arti.storage._internal import partial_format , strip_partition_indexes from arti.types import Type class _StorageMixin ( Model ): @property def key_types ( self ) -> CompositeKeyTypes : if self . type is None : # type: ignore raise ValueError ( f \" { self } .type is not set\" ) return PartitionKey . types_from ( self . type ) # type: ignore @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO: Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys { tuple ( key_types ) } but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: { keys } \" ) class StoragePartition ( _StorageMixin , Model ): type : Type = Field ( repr = False ) format : Format = Field ( repr = False ) keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str , Any ]) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ]), keys ) return keys def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) _StoragePartition = TypeVar ( \"_StoragePartition\" , bound = StoragePartition ) StoragePartitions = tuple [ StoragePartition , ... ] class Storage ( _StorageMixin , Model , Generic [ _StoragePartition ]): \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True # These separators are used in the default resolve_* helpers to format metadata into # the storage fields. # # The defaults are tailored for \"path\"-like fields. key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep storage_partition_type : ClassVar [ type [ _StoragePartition ]] # type: ignore type : Optional [ Type ] = Field ( None , repr = False ) format : Optional [ Format ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO: Check support for the types and partitioning on the specified field(s). return type_ @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls )[ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \" { cls . __name__ } fields must match { cls . storage_partition_type . __name__ } ( { expected_field_types } ), got: { fields } \" ) @property def _format_fields ( self ) -> frozendict [ str , str ]: return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value := getattr ( self , name )), str ) } ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition , ... ]: raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> _StoragePartition : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any , Any ]( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \" { self } requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \" { self } does not specify a {{ input_fingerprint }} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ) . format ( ** format_kwargs ) if lenient_issubclass ( type ( original := getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \" {input_fingerprint} \" in val for val in self . _format_fields . values ()) def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str , str ]) -> str : for placeholder , value in placeholder_values . items (): if not value : # Strip placeholder *and* any trailing self.segment_sep. trim = \"{\" + placeholder + \"}\" if f \" { trim }{ self . segment_sep } \" in spec : trim = f \" { trim }{ self . segment_sep } \" # Also strip any trailing separators, eg: if the placeholder was at the end. spec = spec . replace ( trim , \"\" ) . rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \" { self } . { name } was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ frozendict [ str , str ]] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ - 1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \" { tag }{ self . key_value_sep }{ value } \" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \" { name }{ self . partition_name_component_sep }{ component_name } \" : f \" {{ { name } . { component_spec } }} \" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \" { name }{ self . key_value_sep }{ spec } \" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity (which # only shows \"set\" fields by default). if ( new := self . _resolve_field ( name , original , values )) != original } ) AnyStorage = Storage [ Any ] # mypy doesn't (yet?) support nested TypeVars[1], so mark internal as Any. # # 1: https://github.com/python/mypy/issues/2756 _Storage = TypeVar ( \"_Storage\" , bound = AnyStorage ) Sub-modules arti.storage.literal arti.storage.local Variables AnyStorage Classes Storage class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( _StorageMixin , Model , Generic [ _StoragePartition ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep storage_partition_type : ClassVar [ type[_StoragePartition ] ] # type : ignore type : Optional [ Type ] = Field ( None , repr = False ) format : Optional [ Format ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> _StoragePartition : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) Ancestors (in MRO) arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Descendants arti.storage.local.LocalFile arti.storage.literal.StringLiteral Class variables Config key_value_sep partition_name_component_sep segment_sep Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ ~ _StoragePartition , ... ] View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError () generate_partition def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve_templates def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) StoragePartition class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( _StorageMixin , Model ) : type : Type = Field ( repr = False ) format : Format = Field ( repr = False ) keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) Ancestors (in MRO) arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_keys def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys Instance variables fingerprint key_types Methods compute_content_fingerprint def compute_content_fingerprint ( self ) -> arti . fingerprints . Fingerprint View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) StoragePartitions class StoragePartitions ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.tuple Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"Index"},{"location":"reference/arti/storage/#module-artistorage","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) import abc import os from typing import Any , ClassVar , Generic , Optional , TypeVar from pydantic import Field , validator from arti.fingerprints import Fingerprint from arti.formats import Format from arti.internal.models import Model from arti.internal.type_hints import get_class_type_vars , lenient_issubclass from arti.internal.utils import frozendict from arti.partitions import CompositeKey , CompositeKeyTypes , PartitionKey from arti.storage._internal import InputFingerprints as InputFingerprints from arti.storage._internal import partial_format , strip_partition_indexes from arti.types import Type class _StorageMixin ( Model ): @property def key_types ( self ) -> CompositeKeyTypes : if self . type is None : # type: ignore raise ValueError ( f \" { self } .type is not set\" ) return PartitionKey . types_from ( self . type ) # type: ignore @classmethod def _check_keys ( cls , key_types : CompositeKeyTypes , keys : CompositeKey ) -> None : # TODO: Confirm the key names and types align if key_types and not keys : raise ValueError ( f \"Expected partition keys { tuple ( key_types ) } but none were passed\" ) if keys and not key_types : raise ValueError ( f \"Expected no partition keys but got: { keys } \" ) class StoragePartition ( _StorageMixin , Model ): type : Type = Field ( repr = False ) format : Format = Field ( repr = False ) keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str , Any ]) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ]), keys ) return keys def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()}) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" ) _StoragePartition = TypeVar ( \"_StoragePartition\" , bound = StoragePartition ) StoragePartitions = tuple [ StoragePartition , ... ] class Storage ( _StorageMixin , Model , Generic [ _StoragePartition ]): \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True # These separators are used in the default resolve_* helpers to format metadata into # the storage fields. # # The defaults are tailored for \"path\"-like fields. key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep storage_partition_type : ClassVar [ type [ _StoragePartition ]] # type: ignore type : Optional [ Type ] = Field ( None , repr = False ) format : Optional [ Format ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO: Check support for the types and partitioning on the specified field(s). return type_ @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls )[ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \" { cls . __name__ } fields must match { cls . storage_partition_type . __name__ } ( { expected_field_types } ), got: { fields } \" ) @property def _format_fields ( self ) -> frozendict [ str , str ]: return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value := getattr ( self , name )), str ) } ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition , ... ]: raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> _StoragePartition : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any , Any ]( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \" { self } requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \" { self } does not specify a {{ input_fingerprint }} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ) . format ( ** format_kwargs ) if lenient_issubclass ( type ( original := getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \" {input_fingerprint} \" in val for val in self . _format_fields . values ()) def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str , str ]) -> str : for placeholder , value in placeholder_values . items (): if not value : # Strip placeholder *and* any trailing self.segment_sep. trim = \"{\" + placeholder + \"}\" if f \" { trim }{ self . segment_sep } \" in spec : trim = f \" { trim }{ self . segment_sep } \" # Also strip any trailing separators, eg: if the placeholder was at the end. spec = spec . replace ( trim , \"\" ) . rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \" { self } . { name } was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ frozendict [ str , str ]] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ - 1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \" { tag }{ self . key_value_sep }{ value } \" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \" { name }{ self . partition_name_component_sep }{ component_name } \" : f \" {{ { name } . { component_spec } }} \" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \" { name }{ self . key_value_sep }{ spec } \" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity (which # only shows \"set\" fields by default). if ( new := self . _resolve_field ( name , original , values )) != original } ) AnyStorage = Storage [ Any ] # mypy doesn't (yet?) support nested TypeVars[1], so mark internal as Any. # # 1: https://github.com/python/mypy/issues/2756 _Storage = TypeVar ( \"_Storage\" , bound = AnyStorage )","title":"Module arti.storage"},{"location":"reference/arti/storage/#sub-modules","text":"arti.storage.literal arti.storage.local","title":"Sub-modules"},{"location":"reference/arti/storage/#variables","text":"AnyStorage","title":"Variables"},{"location":"reference/arti/storage/#classes","text":"","title":"Classes"},{"location":"reference/arti/storage/#storage","text":"class Storage ( __pydantic_self__ , ** data : Any ) View Source class Storage ( _StorageMixin , Model , Generic [ _StoragePartition ] ) : \"\"\"Storage is a data reference identifying 1 or more partitions of data. Storage fields should have defaults set with placeholders for tags and partition keys. This allows automatic injection of the tags and partition keys for simple cases. \"\"\" _abstract_ = True # These separators are used in the default resolve_ * helpers to format metadata into # the storage fields . # # The defaults are tailored for \"path\" - like fields . key_value_sep : ClassVar [ str ] = \"=\" partition_name_component_sep : ClassVar [ str ] = \"_\" segment_sep : ClassVar [ str ] = os . sep storage_partition_type : ClassVar [ type[_StoragePartition ] ] # type : ignore type : Optional [ Type ] = Field ( None , repr = False ) format : Optional [ Format ] = Field ( None , repr = False ) @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if cls . _abstract_ : return cls . storage_partition_type = get_class_type_vars ( cls ) [ 0 ] expected_field_types = { name : info . outer_type_ for name , info in cls . storage_partition_type . __fields__ . items () if name not in StoragePartition . __fields__ } fields = { name : info . outer_type_ for name , info in cls . __fields__ . items () if name not in Storage . __fields__ } if fields != expected_field_types : raise TypeError ( f \"{cls.__name__} fields must match {cls.storage_partition_type.__name__} ({expected_field_types}), got: {fields}\" ) @property def _format_fields ( self ) -> frozendict [ str, str ] : return frozendict ( { name : value for name in self . __fields__ if lenient_issubclass ( type ( value : = getattr ( self , name )), str ) } ) @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError () def generate_partition ( self , keys : CompositeKey = CompositeKey (), input_fingerprint : Fingerprint = Fingerprint . empty (), with_content_fingerprint : bool = True , ) -> _StoragePartition : self . _check_keys ( self . key_types , keys ) format_kwargs = dict [ Any, Any ] ( keys ) if input_fingerprint . is_empty : if self . includes_input_fingerprint_template : raise ValueError ( f \"{self} requires an input_fingerprint, but none was provided\" ) else : if not self . includes_input_fingerprint_template : raise ValueError ( f \"{self} does not specify a {{input_fingerprint}} template\" ) format_kwargs [ \"input_fingerprint\" ] = str ( input_fingerprint . key ) field_values = { name : ( strip_partition_indexes ( original ). format ( ** format_kwargs ) if lenient_issubclass ( type ( original : = getattr ( self , name )), str ) else original ) for name in self . __fields__ if name in self . storage_partition_type . __fields__ } partition = self . storage_partition_type ( input_fingerprint = input_fingerprint , keys = keys , ** field_values ) if with_content_fingerprint : partition = partition . with_content_fingerprint () return partition @property def includes_input_fingerprint_template ( self ) -> bool : return any ( \"{input_fingerprint}\" in val for val in self . _format_fields . values ()) def _resolve_field ( self , name : str , spec : str , placeholder_values : dict [ str, str ] ) -> str : for placeholder , value in placeholder_values . items () : if not value : # Strip placeholder * and * any trailing self . segment_sep . trim = \"{\" + placeholder + \"}\" if f \"{trim}{self.segment_sep}\" in spec : trim = f \"{trim}{self.segment_sep}\" # Also strip any trailing separators , eg : if the placeholder was at the end . spec = spec . replace ( trim , \"\" ). rstrip ( self . segment_sep ) if not spec : raise ValueError ( f \"{self}.{name} was empty after removing unused templates\" ) return partial_format ( spec , ** placeholder_values ) def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"Storage"},{"location":"reference/arti/storage/#ancestors-in-mro","text":"arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/#descendants","text":"arti.storage.local.LocalFile arti.storage.literal.StringLiteral","title":"Descendants"},{"location":"reference/arti/storage/#class-variables","text":"Config key_value_sep partition_name_component_sep segment_sep","title":"Class variables"},{"location":"reference/arti/storage/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/storage/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/#validate_format","text":"def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format","title":"validate_format"},{"location":"reference/arti/storage/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_","title":"validate_type"},{"location":"reference/arti/storage/#instance-variables","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/storage/#methods","text":"","title":"Methods"},{"location":"reference/arti/storage/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ ~ _StoragePartition , ... ] View Source @abc . abstractmethod def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ _StoragePartition, ... ] : raise NotImplementedError ()","title":"discover_partitions"},{"location":"reference/arti/storage/#generate_partition","text":"def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/storage/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/#resolve_templates","text":"def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"resolve_templates"},{"location":"reference/arti/storage/#storagepartition","text":"class StoragePartition ( __pydantic_self__ , ** data : Any ) View Source class StoragePartition ( _StorageMixin , Model ) : type : Type = Field ( repr = False ) format : Format = Field ( repr = False ) keys : CompositeKey = CompositeKey () input_fingerprint : Fingerprint = Fingerprint . empty () content_fingerprint : Fingerprint = Fingerprint . empty () @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint () } ) @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"StoragePartition"},{"location":"reference/arti/storage/#ancestors-in-mro_1","text":"arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/#descendants_1","text":"arti.storage.local.LocalFilePartition arti.storage.literal.StringLiteralPartition","title":"Descendants"},{"location":"reference/arti/storage/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/storage/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/storage/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/#validate_keys","text":"def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys","title":"validate_keys"},{"location":"reference/arti/storage/#instance-variables_1","text":"fingerprint key_types","title":"Instance variables"},{"location":"reference/arti/storage/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/storage/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self ) -> arti . fingerprints . Fingerprint View Source @abc . abstractmethod def compute_content_fingerprint ( self ) -> Fingerprint : raise NotImplementedError ( \"{type(self).__name__}.compute_content_fingerprint is not implemented!\" )","title":"compute_content_fingerprint"},{"location":"reference/arti/storage/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/#with_content_fingerprint","text":"def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/storage/#storagepartitions","text":"class StoragePartitions ( / , * args , ** kwargs )","title":"StoragePartitions"},{"location":"reference/arti/storage/#ancestors-in-mro_2","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/storage/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/arti/storage/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/arti/storage/literal/","text":"Module arti.storage.literal None None View Source from typing import Optional from arti.fingerprints import Fingerprint from arti.partitions import CompositeKey from arti.storage import InputFingerprints , Storage , StoragePartition _not_written_err = FileNotFoundError ( \"Literal has not been written yet\" ) class StringLiteralPartition ( StoragePartition ): id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) class StringLiteral ( Storage [ StringLiteralPartition ]): \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \" {graph_name} / {path_tags} / {names} / {partition_key_spec} / {input_fingerprint} / {name}{extension} \" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ]: if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ( { self . value } ) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey (): Fingerprint . empty ()} ) . items () ) Classes StringLiteral class StringLiteral ( __pydantic_self__ , ** data : Any ) View Source class StringLiteral ( Storage [ StringLiteralPartition ] ) : \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \"{graph_name}/{path_tags}/{names}/{partition_key_spec}/{input_fingerprint}/{name}{extension}\" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition, ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won 't know what partitions to lookup. raise ValueError(\"Literal storage can only be partitioned if generated by a Producer.\") # Existing StringLiteralPartitions may be stored in the Graph' s backend , however we don ' t # have access here to lookup . if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () ) Ancestors (in MRO) arti.storage.Storage arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Class variables Config key_value_sep partition_name_component_sep segment_sep storage_partition_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . literal . StringLiteralPartition , ... ] View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () ) generate_partition def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve_templates def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) StringLiteralPartition class StringLiteralPartition ( __pydantic_self__ , ** data : Any ) View Source class StringLiteralPartition ( StoragePartition ) : id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) Ancestors (in MRO) arti.storage.StoragePartition arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_keys def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys Instance variables fingerprint key_types Methods compute_content_fingerprint def compute_content_fingerprint ( self ) -> arti . fingerprints . Fingerprint View Source def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"Literal"},{"location":"reference/arti/storage/literal/#module-artistorageliteral","text":"None None View Source from typing import Optional from arti.fingerprints import Fingerprint from arti.partitions import CompositeKey from arti.storage import InputFingerprints , Storage , StoragePartition _not_written_err = FileNotFoundError ( \"Literal has not been written yet\" ) class StringLiteralPartition ( StoragePartition ): id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value ) class StringLiteral ( Storage [ StringLiteralPartition ]): \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \" {graph_name} / {path_tags} / {names} / {partition_key_spec} / {input_fingerprint} / {name}{extension} \" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ]: if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ( { self . value } ) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey (): Fingerprint . empty ()} ) . items () )","title":"Module arti.storage.literal"},{"location":"reference/arti/storage/literal/#classes","text":"","title":"Classes"},{"location":"reference/arti/storage/literal/#stringliteral","text":"class StringLiteral ( __pydantic_self__ , ** data : Any ) View Source class StringLiteral ( Storage [ StringLiteralPartition ] ) : \"\"\"StringLiteral stores a literal String value directly in the Backend.\"\"\" id : str = \"{graph_name}/{path_tags}/{names}/{partition_key_spec}/{input_fingerprint}/{name}{extension}\" value : Optional [ str ] def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition, ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won 't know what partitions to lookup. raise ValueError(\"Literal storage can only be partitioned if generated by a Producer.\") # Existing StringLiteralPartitions may be stored in the Graph' s backend , however we don ' t # have access here to lookup . if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () )","title":"StringLiteral"},{"location":"reference/arti/storage/literal/#ancestors-in-mro","text":"arti.storage.Storage arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/literal/#class-variables","text":"Config key_value_sep partition_name_component_sep segment_sep storage_partition_type","title":"Class variables"},{"location":"reference/arti/storage/literal/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/storage/literal/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/literal/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/literal/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/literal/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/literal/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/literal/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/literal/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/literal/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/literal/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/literal/#validate_format","text":"def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format","title":"validate_format"},{"location":"reference/arti/storage/literal/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_","title":"validate_type"},{"location":"reference/arti/storage/literal/#instance-variables","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/storage/literal/#methods","text":"","title":"Methods"},{"location":"reference/arti/storage/literal/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/literal/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/literal/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : arti . internal . utils . frozendict [ arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], arti . fingerprints . Fingerprint ] = frozendict ({}) ) -> tuple [ arti . storage . literal . StringLiteralPartition , ... ] View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ StringLiteralPartition , ... ] : if input_fingerprints and self . value is not None : raise ValueError ( f \"Literal storage cannot have a `value` preset ({self.value}) for a Producer output\" ) if self . key_types and not input_fingerprints : # We won't know what partitions to lookup. raise ValueError ( \"Literal storage can only be partitioned if generated by a Producer.\" ) # Existing StringLiteralPartitions may be stored in the Graph's backend, however we don't # have access here to lookup. if self . value is None : return () return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for keys , input_fingerprint in ( input_fingerprints or { CompositeKey () : Fingerprint . empty () } ). items () )","title":"discover_partitions"},{"location":"reference/arti/storage/literal/#generate_partition","text":"def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/storage/literal/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/literal/#resolve_templates","text":"def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"resolve_templates"},{"location":"reference/arti/storage/literal/#stringliteralpartition","text":"class StringLiteralPartition ( __pydantic_self__ , ** data : Any ) View Source class StringLiteralPartition ( StoragePartition ) : id : str value : Optional [ str ] def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value )","title":"StringLiteralPartition"},{"location":"reference/arti/storage/literal/#ancestors-in-mro_1","text":"arti.storage.StoragePartition arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/literal/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/storage/literal/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/storage/literal/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/literal/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/literal/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/literal/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/literal/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/literal/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/literal/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/literal/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/literal/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/literal/#validate_keys","text":"def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys","title":"validate_keys"},{"location":"reference/arti/storage/literal/#instance-variables_1","text":"fingerprint key_types","title":"Instance variables"},{"location":"reference/arti/storage/literal/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/storage/literal/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self ) -> arti . fingerprints . Fingerprint View Source def compute_content_fingerprint ( self ) -> Fingerprint : if self . value is None : raise _not_written_err return Fingerprint . from_string ( self . value )","title":"compute_content_fingerprint"},{"location":"reference/arti/storage/literal/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/literal/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/literal/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/literal/#with_content_fingerprint","text":"def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/storage/local/","text":"Module arti.storage.local None None View Source from __future__ import annotations import hashlib import tempfile from glob import glob from pathlib import Path from typing import Optional , Union from arti.fingerprints import Fingerprint from arti.storage import InputFingerprints , Storage , StoragePartition from arti.storage._internal import parse_spec , spec_to_wildcard class LocalFilePartition ( StoragePartition ): path : str def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () data = f . read ( buffer_size ) while len ( data ) > 0 : sha . update ( data ) data = f . read ( buffer_size ) return Fingerprint . from_string ( sha . hexdigest ()) class LocalFile ( Storage [ LocalFilePartition ]): # `_DEFAULT_PATH_TEMPLATE` and `rooted_at` ease testing, where we often want to just override # the tempdir, but keep the rest of the template. Eventually, we should introduce Resources and # implement a MockFS (to be used in `io.*`). _DEFAULT_PATH_TEMPLATE = str ( Path ( \" {graph_name} \" ) / \" {path_tags} \" / \" {names} \" / \" {partition_key_spec} \" / \" {input_fingerprint} \" / \" {name}{extension} \" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ... ]: wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str , Path ], path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path )) Classes LocalFile class LocalFile ( __pydantic_self__ , ** data : Any ) View Source class LocalFile ( Storage [ LocalFilePartition ] ) : # ` _DEFAULT_PATH_TEMPLATE ` and ` rooted_at ` ease testing , where we often want to just override # the tempdir , but keep the rest of the template . Eventually , we should introduce Resources and # implement a MockFS ( to be used in ` io . * ` ). _DEFAULT_PATH_TEMPLATE = str ( Path ( \"{graph_name}\" ) / \"{path_tags}\" / \"{names}\" / \"{partition_key_spec}\" / \"{input_fingerprint}\" / \"{name}{extension}\" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition, ... ] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path )) Ancestors (in MRO) arti.storage.Storage arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic Class variables Config key_value_sep partition_name_component_sep segment_sep storage_partition_type Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' rooted_at def rooted_at ( root : 'Union[str, Path]' , path : 'Optional[str]' = None ) -> 'LocalFile' View Source @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path )) schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_format def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format validate_type def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_ Instance variables fingerprint includes_input_fingerprint_template key_types Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. discover_partitions def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = frozendict ({}) ) -> 'tuple[LocalFilePartition, ...]' View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ...] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) generate_partition def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . resolve_templates def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } ) LocalFilePartition class LocalFilePartition ( __pydantic_self__ , ** data : Any ) View Source class LocalFilePartition ( StoragePartition ): path: str def compute_content_fingerprint ( self , buffer_size: int = 1024 * 1024 ) -> Fingerprint: with open ( self . path , mode = \"rb\" ) as f: sha = hashlib . sha256 () data = f . read ( buffer_size ) while len ( data ) > 0 : sha . update ( data ) data = f . read ( buffer_size ) return Fingerprint . from_string ( sha . hexdigest ()) Ancestors (in MRO) arti.storage.StoragePartition arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' validate_keys def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys Instance variables fingerprint key_types Methods compute_content_fingerprint def compute_content_fingerprint ( self , buffer_size : 'int' = 1048576 ) -> 'Fingerprint' View Source def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () data = f . read ( buffer_size ) while len ( data ) > 0 : sha . update ( data ) data = f . read ( buffer_size ) return Fingerprint . from_string ( sha . hexdigest ()) copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . with_content_fingerprint def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"Local"},{"location":"reference/arti/storage/local/#module-artistoragelocal","text":"None None View Source from __future__ import annotations import hashlib import tempfile from glob import glob from pathlib import Path from typing import Optional , Union from arti.fingerprints import Fingerprint from arti.storage import InputFingerprints , Storage , StoragePartition from arti.storage._internal import parse_spec , spec_to_wildcard class LocalFilePartition ( StoragePartition ): path : str def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () data = f . read ( buffer_size ) while len ( data ) > 0 : sha . update ( data ) data = f . read ( buffer_size ) return Fingerprint . from_string ( sha . hexdigest ()) class LocalFile ( Storage [ LocalFilePartition ]): # `_DEFAULT_PATH_TEMPLATE` and `rooted_at` ease testing, where we often want to just override # the tempdir, but keep the rest of the template. Eventually, we should introduce Resources and # implement a MockFS (to be used in `io.*`). _DEFAULT_PATH_TEMPLATE = str ( Path ( \" {graph_name} \" ) / \" {path_tags} \" / \" {names} \" / \" {partition_key_spec} \" / \" {input_fingerprint} \" / \" {name}{extension} \" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ... ]: wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str , Path ], path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path ))","title":"Module arti.storage.local"},{"location":"reference/arti/storage/local/#classes","text":"","title":"Classes"},{"location":"reference/arti/storage/local/#localfile","text":"class LocalFile ( __pydantic_self__ , ** data : Any ) View Source class LocalFile ( Storage [ LocalFilePartition ] ) : # ` _DEFAULT_PATH_TEMPLATE ` and ` rooted_at ` ease testing , where we often want to just override # the tempdir , but keep the rest of the template . Eventually , we should introduce Resources and # implement a MockFS ( to be used in ` io . * ` ). _DEFAULT_PATH_TEMPLATE = str ( Path ( \"{graph_name}\" ) / \"{path_tags}\" / \"{names}\" / \"{partition_key_spec}\" / \"{input_fingerprint}\" / \"{name}{extension}\" ) path : str = str ( Path ( tempfile . gettempdir ()) / _DEFAULT_PATH_TEMPLATE ) def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition, ... ] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () ) @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path ))","title":"LocalFile"},{"location":"reference/arti/storage/local/#ancestors-in-mro","text":"arti.storage.Storage arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/local/#class-variables","text":"Config key_value_sep partition_name_component_sep segment_sep storage_partition_type","title":"Class variables"},{"location":"reference/arti/storage/local/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/storage/local/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/local/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/local/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/local/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/local/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/local/#rooted_at","text":"def rooted_at ( root : 'Union[str, Path]' , path : 'Optional[str]' = None ) -> 'LocalFile' View Source @classmethod def rooted_at ( cls , root : Union [ str, Path ] , path : Optional [ str ] = None ) -> LocalFile : path = path if path is not None else cls . _DEFAULT_PATH_TEMPLATE return cls ( path = str ( Path ( root ) / path ))","title":"rooted_at"},{"location":"reference/arti/storage/local/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/local/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/local/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/local/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/local/#validate_format","text":"def validate_format ( format : arti . formats . Format ) -> arti . formats . Format View Source @validator ( \"format\" ) @classmethod def validate_format ( cls , format : Format ) -> Format : return format","title":"validate_format"},{"location":"reference/arti/storage/local/#validate_type","text":"def validate_type ( type_ : arti . types . Type ) -> arti . types . Type View Source @validator ( \"type\" ) @classmethod def validate_type ( cls , type_ : Type ) -> Type : # TODO : Check support for the types and partitioning on the specified field ( s ). return type_","title":"validate_type"},{"location":"reference/arti/storage/local/#instance-variables","text":"fingerprint includes_input_fingerprint_template key_types","title":"Instance variables"},{"location":"reference/arti/storage/local/#methods","text":"","title":"Methods"},{"location":"reference/arti/storage/local/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/local/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/local/#discover_partitions","text":"def discover_partitions ( self , input_fingerprints : 'InputFingerprints' = frozendict ({}) ) -> 'tuple[LocalFilePartition, ...]' View Source def discover_partitions ( self , input_fingerprints : InputFingerprints = InputFingerprints () ) -> tuple [ LocalFilePartition , ...] : wildcard = spec_to_wildcard ( self . path , self . key_types ) paths = set ( glob ( wildcard )) path_metadata = parse_spec ( paths , spec = self . path , key_types = self . key_types , input_fingerprints = input_fingerprints ) return tuple ( self . generate_partition ( input_fingerprint = input_fingerprint , keys = keys ) for path , ( input_fingerprint , keys ) in path_metadata . items () )","title":"discover_partitions"},{"location":"reference/arti/storage/local/#generate_partition","text":"def generate_partition ( self , keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] = frozendict ({}), input_fingerprint : arti . fingerprints . Fingerprint = Fingerprint ( key = None ), with_content_fingerprint : bool = True ) -> ~ _StoragePartition View Source def generate_partition( self, keys: CompositeKey = CompositeKey(), input_fingerprint: Fingerprint = Fingerprint.empty(), with_content_fingerprint: bool = True, ) -> _StoragePartition: self._check_keys(self.key_types, keys) format_kwargs = dict[Any, Any](keys) if input_fingerprint.is_empty: if self.includes_input_fingerprint_template: raise ValueError(f\"{self} requires an input_fingerprint, but none was provided\") else: if not self.includes_input_fingerprint_template: raise ValueError(f\"{self} does not specify a {{ input_fingerprint }} template\") format_kwargs[\"input_fingerprint\"] = str(input_fingerprint.key) field_values = { name: ( strip_partition_indexes(original).format(**format_kwargs) if lenient_issubclass(type(original := getattr(self, name)), str) else original ) for name in self.__fields__ if name in self.storage_partition_type.__fields__ } partition = self.storage_partition_type( input_fingerprint=input_fingerprint, keys=keys, **field_values ) if with_content_fingerprint: partition = partition.with_content_fingerprint() return partition","title":"generate_partition"},{"location":"reference/arti/storage/local/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/local/#resolve_templates","text":"def resolve_templates ( self : '_Storage' , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ arti . fingerprints . Fingerprint ] = None , names : Optional [ tuple [ str , ... ]] = None , path_tags : Optional [ arti . internal . utils . frozendict [ str , str ]] = None ) -> '_Storage' View Source def resolve_templates ( self : \"_Storage\" , graph_name : Optional [ str ] = None , input_fingerprint : Optional [ Fingerprint ] = None , names : Optional [ tuple[str, ... ] ] = None , path_tags : Optional [ frozendict[str, str ] ] = None , ) -> \"_Storage\" : values = {} if graph_name is not None : values [ \"graph_name\" ] = graph_name if input_fingerprint is not None : input_fingerprint_key = str ( input_fingerprint . key ) if input_fingerprint . is_empty : input_fingerprint_key = \"\" values [ \"input_fingerprint\" ] = input_fingerprint_key if names is not None : values [ \"name\" ] = names [ -1 ] if names else \"\" values [ \"names\" ] = self . segment_sep . join ( names ) if path_tags is not None : values [ \"path_tags\" ] = self . segment_sep . join ( f \"{tag}{self.key_value_sep}{value}\" for tag , value in path_tags . items () ) if self . format is not None : values [ \"extension\" ] = self . format . extension if self . type is not None : key_component_specs = { f \"{name}{self.partition_name_component_sep}{component_name}\" : f \"{{{name}.{component_spec}}}\" for name , pk in self . key_types . items () for component_name , component_spec in pk . default_key_components . items () } values [ \"partition_key_spec\" ] = self . segment_sep . join ( f \"{name}{self.key_value_sep}{spec}\" for name , spec in key_component_specs . items () ) return self . copy ( update = { name : new for name , original in self . _format_fields . items () # Avoid \"setting\" the value if not updated to reduce pydantic repr verbosity ( which # only shows \"set\" fields by default ). if ( new : = self . _resolve_field ( name , original , values )) != original } )","title":"resolve_templates"},{"location":"reference/arti/storage/local/#localfilepartition","text":"class LocalFilePartition ( __pydantic_self__ , ** data : Any ) View Source class LocalFilePartition ( StoragePartition ): path: str def compute_content_fingerprint ( self , buffer_size: int = 1024 * 1024 ) -> Fingerprint: with open ( self . path , mode = \"rb\" ) as f: sha = hashlib . sha256 () data = f . read ( buffer_size ) while len ( data ) > 0 : sha . update ( data ) data = f . read ( buffer_size ) return Fingerprint . from_string ( sha . hexdigest ())","title":"LocalFilePartition"},{"location":"reference/arti/storage/local/#ancestors-in-mro_1","text":"arti.storage.StoragePartition arti.storage._StorageMixin arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/storage/local/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/storage/local/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/storage/local/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/storage/local/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/storage/local/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/storage/local/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/storage/local/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/storage/local/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/storage/local/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/storage/local/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/storage/local/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/storage/local/#validate_keys","text":"def validate_keys ( keys : arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ], values : dict [ str , typing . Any ] ) -> arti . internal . utils . frozendict [ str , arti . partitions . PartitionKey ] View Source @validator ( \"keys\" ) @classmethod def validate_keys ( cls , keys : CompositeKey , values : dict [ str, Any ] ) -> CompositeKey : if \"type\" in values : cls . _check_keys ( PartitionKey . types_from ( values [ \"type\" ] ), keys ) return keys","title":"validate_keys"},{"location":"reference/arti/storage/local/#instance-variables_1","text":"fingerprint key_types","title":"Instance variables"},{"location":"reference/arti/storage/local/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/storage/local/#compute_content_fingerprint","text":"def compute_content_fingerprint ( self , buffer_size : 'int' = 1048576 ) -> 'Fingerprint' View Source def compute_content_fingerprint ( self , buffer_size : int = 1024 * 1024 ) -> Fingerprint : with open ( self . path , mode = \"rb\" ) as f : sha = hashlib . sha256 () data = f . read ( buffer_size ) while len ( data ) > 0 : sha . update ( data ) data = f . read ( buffer_size ) return Fingerprint . from_string ( sha . hexdigest ())","title":"compute_content_fingerprint"},{"location":"reference/arti/storage/local/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/storage/local/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/storage/local/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/storage/local/#with_content_fingerprint","text":"def with_content_fingerprint ( self : '_StoragePartition' , keep_existing : bool = True ) -> '_StoragePartition' View Source def with_content_fingerprint ( self : \"_StoragePartition\" , keep_existing : bool = True ) -> \"_StoragePartition\" : if keep_existing and not self . content_fingerprint . is_empty : return self return self . copy ( update = { \"content_fingerprint\" : self . compute_content_fingerprint ()})","title":"with_content_fingerprint"},{"location":"reference/arti/types/","text":"Module arti.types None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterable , Iterator , Mapping from operator import attrgetter from typing import Any , ClassVar , Literal , Optional from pydantic import PrivateAttr , validator from arti.internal.models import Model from arti.internal.type_hints import lenient_issubclass from arti.internal.utils import class_name , frozendict , register DEFAULT_ANONYMOUS_NAME = \"anon\" _TimePrecision = Literal [ \"second\" , \"millisecond\" , \"microsecond\" , \"nanosecond\" ] class Type ( Model ): \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE: Exclude the description to minimize fingerprint changes (and thus rebuilds). _fingerprint_excludes_ = frozenset ([ \"description\" ]) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ class _NamedMixin ( Model ): name : str = DEFAULT_ANONYMOUS_NAME @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : type_ = super () . _pydantic_type_system_post_field_conversion_hook_ ( type_ , name = name , required = required ) if \"name\" not in type_ . __fields_set__ : type_ = type_ . copy ( update = { \"name\" : name }) return type_ @property @abstractmethod def _default_friendly_key ( self ) -> str : raise NotImplementedError () @property def friendly_key ( self ) -> str : return self . _default_friendly_key if self . name == DEFAULT_ANONYMOUS_NAME else self . name ######################## # Core Artigraph Types # ######################## class _Numeric ( Type ): pass class _Float ( _Numeric ): pass class _Int ( _Numeric ): pass class Binary ( Type ): byte_size : Optional [ int ] class Boolean ( Type ): pass class Date ( Type ): pass class DateTime ( Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" precision : _TimePrecision class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \" class Float16 ( _Float ): pass class Float32 ( _Float ): pass class Float64 ( _Float ): pass class Geography ( Type ): format : Optional [ str ] # \"WKB\", \"WKT\", etc srid : Optional [ str ] class Int8 ( _Int ): pass class Int16 ( _Int ): pass class Int32 ( _Int ): pass class Int64 ( _Int ): pass class List ( Type ): element : Type @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class Collection ( _NamedMixin , List ): \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str , ... ] = () cluster_by : tuple [ str , ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( element := values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ): raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown := requested - known : raise ValueError ( f \"unknown field(s): { unknown } \" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( partition_by := values . get ( \"partition_by\" )) is None : return cluster_by if overlapping := set ( cluster_by ) & set ( partition_by ): raise ValueError ( f \"clustering fields overlap with partition fields: { overlapping } \" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" @property def fields ( self ) -> frozendict [ str , Type ]: \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type: ignore # We want the standard AttributeError @property def is_partitioned ( self ) -> bool : return bool ( self . partition_fields ) @property def partition_fields ( self ) -> frozendict [ str , Type ]: if not isinstance ( self . element , Struct ): return frozendict () return frozendict ({ name : self . element . fields [ name ] for name in self . partition_by }) class Map ( Type ): key : Type value : Type @property def friendly_key ( self ) -> str : return f \" { self . key . friendly_key } To { self . value . friendly_key } \" class Null ( Type ): pass class Set ( Type ): element : Type @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class String ( Type ): pass class Struct ( _NamedMixin , Type ): fields : frozendict [ str , Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom { self . _class_key_ } \" # :shrug: class Time ( Type ): precision : _TimePrecision class Timestamp ( Type ): \"\"\"UTC timestamp with configurable precision.\"\"\" precision : _TimePrecision @property def friendly_key ( self ) -> str : return f \" { self . precision . title () }{ self . _class_key_ } \" class UInt8 ( _Int ): pass class UInt16 ( _Int ): pass class UInt32 ( _Int ): pass class UInt64 ( _Int ): pass ############################## # Type conversion interfaces # ############################## class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type [ Type ]] # The internal Artigraph Type system : ClassVar [ Any ] # The external system's type priority : ClassVar [ int ] = 0 # Set the priority of this mapping. Higher is better. @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : raise NotImplementedError () # _ScalarClassTypeAdapter can be used for scalars defined as python types (eg: int or str for the # python TypeSystem). class _ScalarClassTypeAdapter ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : return cls . system @classmethod def generate ( cls , * , artigraph : type [ Type ], system : Any , priority : int = 0 , type_system : \"TypeSystem\" , name : Optional [ str ] = None , ) -> type [ TypeAdapter ]: \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \" { type_system . key }{ artigraph . __name__ } \" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) class TypeSystem ( Model ): key : str _adapter_by_key : dict [ str , type [ TypeAdapter ]] = PrivateAttr ( default_factory = dict ) def register_adapter ( self , adapter : type [ TypeAdapter ]) -> type [ TypeAdapter ]: return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> Iterator [ type [ TypeAdapter ]]: return reversed ( sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ))) def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ]) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ): return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No { self } adapter for system type: { type_ } .\" ) def to_system ( self , type_ : Type , * , hints : dict [ str , Any ]) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ): return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No { self } adapter for Artigraph type: { type_ } .\" ) Sub-modules arti.types.pydantic arti.types.python Variables DEFAULT_ANONYMOUS_NAME Classes Binary class Binary ( __pydantic_self__ , ** data : Any ) View Source class Binary ( Type ) : byte_size : Optional [ int ] Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Boolean class Boolean ( __pydantic_self__ , ** data : Any ) View Source class Boolean ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Collection class Collection ( __pydantic_self__ , ** data : Any ) View Source class Collection ( _NamedMixin , List ) : \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str, ... ] = () cluster_by : tuple [ str, ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( element : = values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ) : raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown : = requested - known : raise ValueError ( f \"unknown field(s): {unknown}\" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( partition_by : = values . get ( \"partition_by\" )) is None : return cluster_by if overlapping : = set ( cluster_by ) & set ( partition_by ) : raise ValueError ( f \"clustering fields overlap with partition fields: {overlapping}\" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" @property def fields ( self ) -> frozendict [ str, Type ] : \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type : ignore # We want the standard AttributeError @property def is_partitioned ( self ) -> bool : return bool ( self . partition_fields ) @property def partition_fields ( self ) -> frozendict [ str, Type ] : if not isinstance ( self . element , Struct ) : return frozendict () return frozendict ( { name : self . element . fields [ name ] for name in self . partition_by } ) Ancestors (in MRO) arti.types._NamedMixin arti.types.List arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fields Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. fingerprint friendly_key is_partitioned partition_fields Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Date class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . DateTime class DateTime ( __pydantic_self__ , ** data : Any ) View Source class DateTime ( Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" precision: _TimePrecision Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Enum class Enum ( __pydantic_self__ , ** data : Any ) View Source class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \" Ancestors (in MRO) arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float16 class Float16 ( __pydantic_self__ , ** data : Any ) View Source class Float16 ( _Float ): pass Ancestors (in MRO) arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float32 class Float32 ( __pydantic_self__ , ** data : Any ) View Source class Float32 ( _Float ): pass Ancestors (in MRO) arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float64 class Float64 ( __pydantic_self__ , ** data : Any ) View Source class Float64 ( _Float ): pass Ancestors (in MRO) arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Geography class Geography ( __pydantic_self__ , ** data : Any ) View Source class Geography ( Type ) : format : Optional [ str ] # \"WKB\" , \"WKT\" , etc srid : Optional [ str ] Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int16 class Int16 ( __pydantic_self__ , ** data : Any ) View Source class Int16 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int32 class Int32 ( __pydantic_self__ , ** data : Any ) View Source class Int32 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int64 class Int64 ( __pydantic_self__ , ** data : Any ) View Source class Int64 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int8 class Int8 ( __pydantic_self__ , ** data : Any ) View Source class Int8 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . List class List ( __pydantic_self__ , ** data : Any ) View Source class List ( Type ) : element : Type @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.types.Collection Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Map class Map ( __pydantic_self__ , ** data : Any ) View Source class Map ( Type ) : key : Type value : Type @property def friendly_key ( self ) -> str : return f \"{self.key.friendly_key}To{self.value.friendly_key}\" Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Null class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Set class Set ( __pydantic_self__ , ** data : Any ) View Source class Set ( Type ) : element : Type @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . String class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Type ): pass Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Struct class Struct ( __pydantic_self__ , ** data : Any ) View Source class Struct ( _NamedMixin , Type ) : fields : frozendict [ str, Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom{self._class_key_}\" # : shrug : Ancestors (in MRO) arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Time class Time ( __pydantic_self__ , ** data : Any ) View Source class Time ( Type ): precision: _TimePrecision Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Timestamp class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( Type ) : \"\"\"UTC timestamp with configurable precision.\"\"\" precision : _TimePrecision @property def friendly_key ( self ) -> str : return f \"{self.precision.title()}{self._class_key_}\" Ancestors (in MRO) arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Type class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . TypeAdapter class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError () Descendants arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.pydantic.BaseModelAdapter Class variables key priority Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError () to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError () TypeSystem class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str _adapter_by_key : dict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = dict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> Iterator [ type[TypeAdapter ] ]: return reversed ( sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ))) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . register_adapter def register_adapter ( self , adapter : type [ arti . types . TypeAdapter ] ) -> type [ arti . types . TypeAdapter ] View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) to_artigraph def to_artigraph ( self , type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ]) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" ) to_system def to_system ( self , type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source def to_system ( self , type_ : Type , * , hints : dict [ str , Any ]) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" ) UInt16 class UInt16 ( __pydantic_self__ , ** data : Any ) View Source class UInt16 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . UInt32 class UInt32 ( __pydantic_self__ , ** data : Any ) View Source class UInt32 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . UInt64 class UInt64 ( __pydantic_self__ , ** data : Any ) View Source class UInt64 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . UInt8 class UInt8 ( __pydantic_self__ , ** data : Any ) View Source class UInt8 ( _Int ): pass Ancestors (in MRO) arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/types/#module-artitypes","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from abc import abstractmethod from collections.abc import Iterable , Iterator , Mapping from operator import attrgetter from typing import Any , ClassVar , Literal , Optional from pydantic import PrivateAttr , validator from arti.internal.models import Model from arti.internal.type_hints import lenient_issubclass from arti.internal.utils import class_name , frozendict , register DEFAULT_ANONYMOUS_NAME = \"anon\" _TimePrecision = Literal [ \"second\" , \"millisecond\" , \"microsecond\" , \"nanosecond\" ] class Type ( Model ): \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE: Exclude the description to minimize fingerprint changes (and thus rebuilds). _fingerprint_excludes_ = frozenset ([ \"description\" ]) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_ class _NamedMixin ( Model ): name : str = DEFAULT_ANONYMOUS_NAME @classmethod def _pydantic_type_system_post_field_conversion_hook_ ( cls , type_ : Type , * , name : str , required : bool ) -> Type : type_ = super () . _pydantic_type_system_post_field_conversion_hook_ ( type_ , name = name , required = required ) if \"name\" not in type_ . __fields_set__ : type_ = type_ . copy ( update = { \"name\" : name }) return type_ @property @abstractmethod def _default_friendly_key ( self ) -> str : raise NotImplementedError () @property def friendly_key ( self ) -> str : return self . _default_friendly_key if self . name == DEFAULT_ANONYMOUS_NAME else self . name ######################## # Core Artigraph Types # ######################## class _Numeric ( Type ): pass class _Float ( _Numeric ): pass class _Int ( _Numeric ): pass class Binary ( Type ): byte_size : Optional [ int ] class Boolean ( Type ): pass class Date ( Type ): pass class DateTime ( Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" precision : _TimePrecision class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \" class Float16 ( _Float ): pass class Float32 ( _Float ): pass class Float64 ( _Float ): pass class Geography ( Type ): format : Optional [ str ] # \"WKB\", \"WKT\", etc srid : Optional [ str ] class Int8 ( _Int ): pass class Int16 ( _Int ): pass class Int32 ( _Int ): pass class Int64 ( _Int ): pass class List ( Type ): element : Type @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class Collection ( _NamedMixin , List ): \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str , ... ] = () cluster_by : tuple [ str , ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( element := values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ): raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown := requested - known : raise ValueError ( f \"unknown field(s): { unknown } \" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str , ... ], values : dict [ str , Any ] ) -> tuple [ str , ... ]: if ( partition_by := values . get ( \"partition_by\" )) is None : return cluster_by if overlapping := set ( cluster_by ) & set ( partition_by ): raise ValueError ( f \"clustering fields overlap with partition fields: { overlapping } \" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" @property def fields ( self ) -> frozendict [ str , Type ]: \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type: ignore # We want the standard AttributeError @property def is_partitioned ( self ) -> bool : return bool ( self . partition_fields ) @property def partition_fields ( self ) -> frozendict [ str , Type ]: if not isinstance ( self . element , Struct ): return frozendict () return frozendict ({ name : self . element . fields [ name ] for name in self . partition_by }) class Map ( Type ): key : Type value : Type @property def friendly_key ( self ) -> str : return f \" { self . key . friendly_key } To { self . value . friendly_key } \" class Null ( Type ): pass class Set ( Type ): element : Type @property def friendly_key ( self ) -> str : return f \" { self . element . friendly_key }{ self . _class_key_ } \" class String ( Type ): pass class Struct ( _NamedMixin , Type ): fields : frozendict [ str , Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom { self . _class_key_ } \" # :shrug: class Time ( Type ): precision : _TimePrecision class Timestamp ( Type ): \"\"\"UTC timestamp with configurable precision.\"\"\" precision : _TimePrecision @property def friendly_key ( self ) -> str : return f \" { self . precision . title () }{ self . _class_key_ } \" class UInt8 ( _Int ): pass class UInt16 ( _Int ): pass class UInt32 ( _Int ): pass class UInt64 ( _Int ): pass ############################## # Type conversion interfaces # ############################## class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type [ Type ]] # The internal Artigraph Type system : ClassVar [ Any ] # The external system's type priority : ClassVar [ int ] = 0 # Set the priority of this mapping. Higher is better. @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : raise NotImplementedError () # _ScalarClassTypeAdapter can be used for scalars defined as python types (eg: int or str for the # python TypeSystem). class _ScalarClassTypeAdapter ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return cls . artigraph () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : return cls . system @classmethod def generate ( cls , * , artigraph : type [ Type ], system : Any , priority : int = 0 , type_system : \"TypeSystem\" , name : Optional [ str ] = None , ) -> type [ TypeAdapter ]: \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \" { type_system . key }{ artigraph . __name__ } \" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority }, ) ) class TypeSystem ( Model ): key : str _adapter_by_key : dict [ str , type [ TypeAdapter ]] = PrivateAttr ( default_factory = dict ) def register_adapter ( self , adapter : type [ TypeAdapter ]) -> type [ TypeAdapter ]: return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> Iterator [ type [ TypeAdapter ]]: return reversed ( sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ))) def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ]) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ): return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No { self } adapter for system type: { type_ } .\" ) def to_system ( self , type_ : Type , * , hints : dict [ str , Any ]) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ): return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No { self } adapter for Artigraph type: { type_ } .\" )","title":"Module arti.types"},{"location":"reference/arti/types/#sub-modules","text":"arti.types.pydantic arti.types.python","title":"Sub-modules"},{"location":"reference/arti/types/#variables","text":"DEFAULT_ANONYMOUS_NAME","title":"Variables"},{"location":"reference/arti/types/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/#binary","text":"class Binary ( __pydantic_self__ , ** data : Any ) View Source class Binary ( Type ) : byte_size : Optional [ int ]","title":"Binary"},{"location":"reference/arti/types/#ancestors-in-mro","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods","text":"","title":"Methods"},{"location":"reference/arti/types/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#boolean","text":"class Boolean ( __pydantic_self__ , ** data : Any ) View Source class Boolean ( Type ): pass","title":"Boolean"},{"location":"reference/arti/types/#ancestors-in-mro_1","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_1","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#collection","text":"class Collection ( __pydantic_self__ , ** data : Any ) View Source class Collection ( _NamedMixin , List ) : \"\"\"A collection of elements with partition and cluster metadata. Collections should not be nested in other types. \"\"\" partition_by : tuple [ str, ... ] = () cluster_by : tuple [ str, ... ] = () @validator ( \"partition_by\" , \"cluster_by\" ) @classmethod def _validate_field_ref ( cls , references : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( element : = values . get ( \"element\" )) is None : return references if references and not isinstance ( element , Struct ) : raise ValueError ( \"requires element to be a Struct\" ) known , requested = set ( element . fields ), set ( references ) if unknown : = requested - known : raise ValueError ( f \"unknown field(s): {unknown}\" ) return references @validator ( \"cluster_by\" ) @classmethod def _validate_cluster_by ( cls , cluster_by : tuple [ str, ... ] , values : dict [ str, Any ] ) -> tuple [ str, ... ] : if ( partition_by : = values . get ( \"partition_by\" )) is None : return cluster_by if overlapping : = set ( cluster_by ) & set ( partition_by ) : raise ValueError ( f \"clustering fields overlap with partition fields: {overlapping}\" ) return cluster_by @property def _default_friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\" @property def fields ( self ) -> frozendict [ str, Type ] : \"\"\"Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. \"\"\" return self . element . fields # type : ignore # We want the standard AttributeError @property def is_partitioned ( self ) -> bool : return bool ( self . partition_fields ) @property def partition_fields ( self ) -> frozendict [ str, Type ] : if not isinstance ( self . element , Struct ) : return frozendict () return frozendict ( { name : self . element . fields [ name ] for name in self . partition_by } )","title":"Collection"},{"location":"reference/arti/types/#ancestors-in-mro_2","text":"arti.types._NamedMixin arti.types.List arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_2","text":"fields Shorthand accessor to access Struct element fields. If the element is not a Struct, an AttributeError will be raised. fingerprint friendly_key is_partitioned partition_fields","title":"Instance variables"},{"location":"reference/arti/types/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_2","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#date","text":"class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( Type ): pass","title":"Date"},{"location":"reference/arti/types/#ancestors-in-mro_3","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_3","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_3","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#datetime","text":"class DateTime ( __pydantic_self__ , ** data : Any ) View Source class DateTime ( Type ): \"\"\"A Date and Time as shown on a calendar and clock, independent of timezone.\"\"\" precision: _TimePrecision","title":"DateTime"},{"location":"reference/arti/types/#ancestors-in-mro_4","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_4","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_4","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_4","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#enum","text":"class Enum ( __pydantic_self__ , ** data : Any ) View Source class Enum ( _NamedMixin , Type ): type : Type items : frozenset [ Any ] @validator ( \"items\" , pre = True ) @classmethod def _cast_values ( cls , items : Any ) -> Any : if isinstance ( items , Iterable ) and not isinstance ( items , Mapping ): return frozenset ( items ) return items @validator ( \"items\" ) @classmethod def _validate_values ( cls , items : frozenset [ Any ], values : dict [ str , Any ]) -> frozenset [ Any ]: from arti.types.python import python_type_system if len ( items ) == 0 : raise ValueError ( \"cannot be empty.\" ) # `type` will be missing if it doesn't pass validation. if ( arti_type := values . get ( \"type\" )) is None : return items py_type = python_type_system . to_system ( arti_type , hints = {}) mismatched_items = [ item for item in items if not lenient_issubclass ( type ( item ), py_type )] if mismatched_items : raise ValueError ( f \"incompatible { arti_type } ( { py_type } ) item(s): { mismatched_items } \" ) return items @property def _default_friendly_key ( self ) -> str : return f \" { self . type . friendly_key }{ self . _class_key_ } \"","title":"Enum"},{"location":"reference/arti/types/#ancestors-in-mro_5","text":"arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_5","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_5","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_5","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#float16","text":"class Float16 ( __pydantic_self__ , ** data : Any ) View Source class Float16 ( _Float ): pass","title":"Float16"},{"location":"reference/arti/types/#ancestors-in-mro_6","text":"arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_6","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_6","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_6","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#float32","text":"class Float32 ( __pydantic_self__ , ** data : Any ) View Source class Float32 ( _Float ): pass","title":"Float32"},{"location":"reference/arti/types/#ancestors-in-mro_7","text":"arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_7","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_7","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_7","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_7","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_7","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_7","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_7","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_7","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_7","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_7","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_7","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_7","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_7","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_7","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#float64","text":"class Float64 ( __pydantic_self__ , ** data : Any ) View Source class Float64 ( _Float ): pass","title":"Float64"},{"location":"reference/arti/types/#ancestors-in-mro_8","text":"arti.types._Float arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_8","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_8","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_8","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_8","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_8","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_8","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_8","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_8","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_8","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_8","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_8","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_8","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_8","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_8","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#geography","text":"class Geography ( __pydantic_self__ , ** data : Any ) View Source class Geography ( Type ) : format : Optional [ str ] # \"WKB\" , \"WKT\" , etc srid : Optional [ str ]","title":"Geography"},{"location":"reference/arti/types/#ancestors-in-mro_9","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_9","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_9","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_9","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_9","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_9","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_9","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_9","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_9","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_9","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_9","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_9","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_9","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_9","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_9","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_9","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int16","text":"class Int16 ( __pydantic_self__ , ** data : Any ) View Source class Int16 ( _Int ): pass","title":"Int16"},{"location":"reference/arti/types/#ancestors-in-mro_10","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_10","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_10","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_10","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_10","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_10","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_10","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_10","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_10","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_10","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_10","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_10","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_10","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_10","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_10","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_10","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_10","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int32","text":"class Int32 ( __pydantic_self__ , ** data : Any ) View Source class Int32 ( _Int ): pass","title":"Int32"},{"location":"reference/arti/types/#ancestors-in-mro_11","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_11","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_11","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_11","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_11","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_11","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_11","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_11","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_11","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_11","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_11","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_11","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_11","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_11","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_11","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_11","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_11","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int64","text":"class Int64 ( __pydantic_self__ , ** data : Any ) View Source class Int64 ( _Int ): pass","title":"Int64"},{"location":"reference/arti/types/#ancestors-in-mro_12","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_12","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_12","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_12","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_12","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_12","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_12","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_12","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_12","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_12","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_12","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_12","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_12","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_12","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_12","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_12","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_12","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#int8","text":"class Int8 ( __pydantic_self__ , ** data : Any ) View Source class Int8 ( _Int ): pass","title":"Int8"},{"location":"reference/arti/types/#ancestors-in-mro_13","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_13","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_13","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_13","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_13","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_13","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_13","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_13","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_13","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_13","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_13","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_13","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_13","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_13","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_13","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_13","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_13","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#list","text":"class List ( __pydantic_self__ , ** data : Any ) View Source class List ( Type ) : element : Type @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\"","title":"List"},{"location":"reference/arti/types/#ancestors-in-mro_14","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#descendants","text":"arti.types.Collection","title":"Descendants"},{"location":"reference/arti/types/#class-variables_14","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_14","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_14","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_14","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_14","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_14","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_14","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_14","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_14","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_14","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_14","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_14","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_14","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_14","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_14","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_14","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#map","text":"class Map ( __pydantic_self__ , ** data : Any ) View Source class Map ( Type ) : key : Type value : Type @property def friendly_key ( self ) -> str : return f \"{self.key.friendly_key}To{self.value.friendly_key}\"","title":"Map"},{"location":"reference/arti/types/#ancestors-in-mro_15","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_15","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_15","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_15","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_15","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_15","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_15","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_15","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_15","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_15","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_15","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_15","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_15","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_15","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_15","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_15","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_15","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#null","text":"class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( Type ): pass","title":"Null"},{"location":"reference/arti/types/#ancestors-in-mro_16","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_16","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_16","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_16","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_16","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_16","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_16","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_16","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_16","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_16","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_16","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_16","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_16","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_16","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_16","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_16","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_16","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#set","text":"class Set ( __pydantic_self__ , ** data : Any ) View Source class Set ( Type ) : element : Type @property def friendly_key ( self ) -> str : return f \"{self.element.friendly_key}{self._class_key_}\"","title":"Set"},{"location":"reference/arti/types/#ancestors-in-mro_17","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_17","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_17","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_17","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_17","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_17","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_17","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_17","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_17","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_17","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_17","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_17","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_17","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_17","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_17","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_17","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_17","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#string","text":"class String ( __pydantic_self__ , ** data : Any ) View Source class String ( Type ): pass","title":"String"},{"location":"reference/arti/types/#ancestors-in-mro_18","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_18","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_18","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_18","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_18","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_18","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_18","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_18","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_18","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_18","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_18","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_18","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_18","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_18","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_18","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_18","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_18","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#struct","text":"class Struct ( __pydantic_self__ , ** data : Any ) View Source class Struct ( _NamedMixin , Type ) : fields : frozendict [ str, Type ] @property def _default_friendly_key ( self ) -> str : return f \"Custom{self._class_key_}\" # : shrug :","title":"Struct"},{"location":"reference/arti/types/#ancestors-in-mro_19","text":"arti.types._NamedMixin arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_19","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_19","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_19","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_19","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_19","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_19","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_19","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_19","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_19","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_19","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_19","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_19","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_19","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_19","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_19","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_19","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#time","text":"class Time ( __pydantic_self__ , ** data : Any ) View Source class Time ( Type ): precision: _TimePrecision","title":"Time"},{"location":"reference/arti/types/#ancestors-in-mro_20","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_20","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_20","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_20","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_20","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_20","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_20","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_20","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_20","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_20","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_20","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_20","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_20","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_20","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_20","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_20","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_20","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#timestamp","text":"class Timestamp ( __pydantic_self__ , ** data : Any ) View Source class Timestamp ( Type ) : \"\"\"UTC timestamp with configurable precision.\"\"\" precision : _TimePrecision @property def friendly_key ( self ) -> str : return f \"{self.precision.title()}{self._class_key_}\"","title":"Timestamp"},{"location":"reference/arti/types/#ancestors-in-mro_21","text":"arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_21","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_21","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_21","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_21","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_21","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_21","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_21","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_21","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_21","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_21","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_21","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_21","text":"fingerprint friendly_key","title":"Instance variables"},{"location":"reference/arti/types/#methods_21","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_21","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_21","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_21","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#type","text":"class Type ( __pydantic_self__ , ** data : Any ) View Source class Type ( Model ) : \"\"\"Type represents a data type.\"\"\" _abstract_ = True # NOTE : Exclude the description to minimize fingerprint changes ( and thus rebuilds ). _fingerprint_excludes_ = frozenset ( [ \"description\" ] ) description : Optional [ str ] nullable : bool = False @property def friendly_key ( self ) -> str : \"\"\"A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string. \"\"\" return self . _class_key_","title":"Type"},{"location":"reference/arti/types/#ancestors-in-mro_22","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#descendants_1","text":"arti.types._Numeric arti.types.Binary arti.types.Boolean arti.types.Date arti.types.DateTime arti.types.Enum arti.types.Geography arti.types.List arti.types.Map arti.types.Null arti.types.Set arti.types.String arti.types.Struct arti.types.Time arti.types.Timestamp","title":"Descendants"},{"location":"reference/arti/types/#class-variables_22","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_22","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_22","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_22","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_22","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_22","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_22","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_22","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_22","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_22","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_22","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_22","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_22","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_22","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_22","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_22","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#typeadapter","text":"class TypeAdapter ( / , * args , ** kwargs ) View Source class TypeAdapter : \"\"\"TypeAdapter maps between Artigraph types and a foreign type system.\"\"\" key : ClassVar [ str ] = class_name () artigraph : ClassVar [ type[Type ] ] # The internal Artigraph Type system : ClassVar [ Any ] # The external system ' s type priority : ClassVar [ int ] = 0 # Set the priority of this mapping . Higher is better . @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError () @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError () @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError ()","title":"TypeAdapter"},{"location":"reference/arti/types/#descendants_2","text":"arti.types._ScalarClassTypeAdapter arti.types.python.PyValueContainer arti.types.python.PyLiteral arti.types.python.PyMap arti.types.python.PyOptional arti.types.python.PyStruct arti.types.pydantic.BaseModelAdapter","title":"Descendants"},{"location":"reference/arti/types/#class-variables_23","text":"key priority","title":"Class variables"},{"location":"reference/arti/types/#static-methods_23","text":"","title":"Static methods"},{"location":"reference/arti/types/#matches_artigraph","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/#matches_system","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : raise NotImplementedError ()","title":"matches_system"},{"location":"reference/arti/types/#to_artigraph","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : raise NotImplementedError ()","title":"to_artigraph"},{"location":"reference/arti/types/#to_system","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : raise NotImplementedError ()","title":"to_system"},{"location":"reference/arti/types/#typesystem","text":"class TypeSystem ( __pydantic_self__ , ** data : Any ) View Source class TypeSystem ( Model ) : key : str _adapter_by_key : dict [ str, type[TypeAdapter ] ] = PrivateAttr ( default_factory = dict ) def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter ) @property def _priority_sorted_adapters ( self ) -> Iterator [ type[TypeAdapter ] ]: return reversed ( sorted ( self . _adapter_by_key . values (), key = attrgetter ( \"priority\" ))) def to_artigraph ( self , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" ) def to_system ( self , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" )","title":"TypeSystem"},{"location":"reference/arti/types/#ancestors-in-mro_23","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_24","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_24","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_23","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_23","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_23","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_23","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_23","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_23","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_23","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_23","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_23","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_23","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/types/#methods_23","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_23","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_23","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_23","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#register_adapter","text":"def register_adapter ( self , adapter : type [ arti . types . TypeAdapter ] ) -> type [ arti . types . TypeAdapter ] View Source def register_adapter ( self , adapter : type [ TypeAdapter ] ) -> type [ TypeAdapter ] : return register ( self . _adapter_by_key , adapter . key , adapter )","title":"register_adapter"},{"location":"reference/arti/types/#to_artigraph_1","text":"def to_artigraph ( self , type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source def to_artigraph ( self , type_ : Any , * , hints : dict [ str , Any ]) -> Type : for adapter in self . _priority_sorted_adapters : if adapter . matches_system ( type_ , hints = hints ) : return adapter . to_artigraph ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for system type: {type_}.\" )","title":"to_artigraph"},{"location":"reference/arti/types/#to_system_1","text":"def to_system ( self , type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source def to_system ( self , type_ : Type , * , hints : dict [ str , Any ]) -> Any : for adapter in self . _priority_sorted_adapters : if adapter . matches_artigraph ( type_ , hints = hints ) : return adapter . to_system ( type_ , hints = hints ) raise NotImplementedError ( f \"No {self} adapter for Artigraph type: {type_}.\" )","title":"to_system"},{"location":"reference/arti/types/#uint16","text":"class UInt16 ( __pydantic_self__ , ** data : Any ) View Source class UInt16 ( _Int ): pass","title":"UInt16"},{"location":"reference/arti/types/#ancestors-in-mro_24","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_25","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_25","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_24","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_24","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_24","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_24","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_24","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_24","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_24","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_24","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_24","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_24","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_24","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_24","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_24","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_24","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#uint32","text":"class UInt32 ( __pydantic_self__ , ** data : Any ) View Source class UInt32 ( _Int ): pass","title":"UInt32"},{"location":"reference/arti/types/#ancestors-in-mro_25","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_26","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_26","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_25","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_25","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_25","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_25","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_25","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_25","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_25","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_25","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_25","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_25","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_25","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_25","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_25","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_25","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#uint64","text":"class UInt64 ( __pydantic_self__ , ** data : Any ) View Source class UInt64 ( _Int ): pass","title":"UInt64"},{"location":"reference/arti/types/#ancestors-in-mro_26","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_27","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_27","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_26","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_26","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_26","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_26","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_26","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_26","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_26","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_26","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_26","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_26","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_26","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_26","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_26","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_26","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/#uint8","text":"class UInt8 ( __pydantic_self__ , ** data : Any ) View Source class UInt8 ( _Int ): pass","title":"UInt8"},{"location":"reference/arti/types/#ancestors-in-mro_27","text":"arti.types._Int arti.types._Numeric arti.types.Type arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/#class-variables_28","text":"Config","title":"Class variables"},{"location":"reference/arti/types/#static-methods_28","text":"","title":"Static methods"},{"location":"reference/arti/types/#construct_27","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/types/#from_orm_27","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/types/#parse_file_27","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/types/#parse_obj_27","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/types/#parse_raw_27","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/types/#schema_27","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/types/#schema_json_27","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/types/#update_forward_refs_27","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/types/#validate_27","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/types/#instance-variables_27","text":"fingerprint friendly_key A human-readable class-name like key representing this Type. The key doesn't have to be unique, just a best effort, meaningful string.","title":"Instance variables"},{"location":"reference/arti/types/#methods_27","text":"","title":"Methods"},{"location":"reference/arti/types/#copy_27","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/types/#dict_27","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/types/#json_27","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/types/pydantic/","text":"Module arti.types.pydantic None None View Source from typing import Any , Protocol from pydantic import BaseModel from pydantic.fields import ModelField from pydantic.fields import UndefinedType as _PydanticUndefinedType from arti.internal.type_hints import lenient_issubclass from arti.types import Struct , Type , TypeAdapter , TypeSystem , _ScalarClassTypeAdapter from arti.types.python import python_type_system pydantic_type_system = TypeSystem ( key = \"pydantic\" ) class _PostFieldConversionHook ( Protocol ): def __call__ ( self , type_ : Type , * , name : str , required : bool ) -> Type : raise NotImplementedError () def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ): return type_ . _pydantic_type_system_post_field_conversion_hook_ # type: ignore return lambda type_ , * , name , required : type_ @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ): artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str , Any ]) -> Type : subtype = python_type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ], * , hints : dict [ str , Any ]) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints ) for field in type_ . __fields__ . values () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> type [ BaseModel ]: assert isinstance ( type_ , Struct ) return type ( f \" { type_ . name } \" , ( BaseModel ,), { \"__annotations__\" : { k : ( pydantic_type_system . to_system ( v , hints = hints ) if isinstance ( v , Struct ) else python_type_system . to_system ( v , hints = hints ) ) for k , v in type_ . fields . items () } }, ) # Extend the python_type_system to handle BaseModel. This simplifies conversion of nested models @python_type_system . register_adapter class _PythonBaseModelAdapter ( _ScalarClassTypeAdapter ): artigraph = Struct system = BaseModel priority = int ( 1e8 ) # Beneath the Optional Adapter @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Avoid converting a python type to a BaseModel unless explicit annotated. return super () . matches_artigraph ( type_ , hints = hints ) and hints . get ( f \" { pydantic_type_system . key } .is_model\" , False ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return BaseModelAdapter . to_artigraph ( type_ , hints = hints ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return BaseModelAdapter . matches_system ( type_ , hints = hints ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : return BaseModelAdapter . to_system ( type_ , hints = hints ) Variables pydantic_type_system Functions get_post_field_conversion_hook def get_post_field_conversion_hook ( type_ : Any ) -> arti . types . pydantic . _PostFieldConversionHook View Source def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ) : return type_ . _pydantic_type_system_post_field_conversion_hook_ # type : ignore return lambda type_ , * , name , required : type_ Classes BaseModelAdapter class BaseModelAdapter ( / , * args , ** kwargs ) View Source @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ) : artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str, Any ] ) -> Type : subtype = python_type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints ) for field in type_ . __fields__ . values () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : ( pydantic_type_system . to_system ( v , hints = hints ) if isinstance ( v , Struct ) else python_type_system . to_system ( v , hints = hints ) ) for k , v in type_ . fields . items () } } , ) Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : type [ pydantic . main . BaseModel ], * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints ) for field in type_ . __fields__ . values () } , ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> type [ pydantic . main . BaseModel ] View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : ( pydantic_type_system . to_system ( v , hints = hints ) if isinstance ( v , Struct ) else python_type_system . to_system ( v , hints = hints ) ) for k , v in type_ . fields . items () } } , )","title":"Pydantic"},{"location":"reference/arti/types/pydantic/#module-artitypespydantic","text":"None None View Source from typing import Any , Protocol from pydantic import BaseModel from pydantic.fields import ModelField from pydantic.fields import UndefinedType as _PydanticUndefinedType from arti.internal.type_hints import lenient_issubclass from arti.types import Struct , Type , TypeAdapter , TypeSystem , _ScalarClassTypeAdapter from arti.types.python import python_type_system pydantic_type_system = TypeSystem ( key = \"pydantic\" ) class _PostFieldConversionHook ( Protocol ): def __call__ ( self , type_ : Type , * , name : str , required : bool ) -> Type : raise NotImplementedError () def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ): return type_ . _pydantic_type_system_post_field_conversion_hook_ # type: ignore return lambda type_ , * , name , required : type_ @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ): artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str , Any ]) -> Type : subtype = python_type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ], * , hints : dict [ str , Any ]) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints ) for field in type_ . __fields__ . values () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> type [ BaseModel ]: assert isinstance ( type_ , Struct ) return type ( f \" { type_ . name } \" , ( BaseModel ,), { \"__annotations__\" : { k : ( pydantic_type_system . to_system ( v , hints = hints ) if isinstance ( v , Struct ) else python_type_system . to_system ( v , hints = hints ) ) for k , v in type_ . fields . items () } }, ) # Extend the python_type_system to handle BaseModel. This simplifies conversion of nested models @python_type_system . register_adapter class _PythonBaseModelAdapter ( _ScalarClassTypeAdapter ): artigraph = Struct system = BaseModel priority = int ( 1e8 ) # Beneath the Optional Adapter @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : # Avoid converting a python type to a BaseModel unless explicit annotated. return super () . matches_artigraph ( type_ , hints = hints ) and hints . get ( f \" { pydantic_type_system . key } .is_model\" , False ) @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return BaseModelAdapter . to_artigraph ( type_ , hints = hints ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return BaseModelAdapter . matches_system ( type_ , hints = hints ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : return BaseModelAdapter . to_system ( type_ , hints = hints )","title":"Module arti.types.pydantic"},{"location":"reference/arti/types/pydantic/#variables","text":"pydantic_type_system","title":"Variables"},{"location":"reference/arti/types/pydantic/#functions","text":"","title":"Functions"},{"location":"reference/arti/types/pydantic/#get_post_field_conversion_hook","text":"def get_post_field_conversion_hook ( type_ : Any ) -> arti . types . pydantic . _PostFieldConversionHook View Source def get_post_field_conversion_hook ( type_ : Any ) -> _PostFieldConversionHook : if hasattr ( type_ , \"_pydantic_type_system_post_field_conversion_hook_\" ) : return type_ . _pydantic_type_system_post_field_conversion_hook_ # type : ignore return lambda type_ , * , name , required : type_","title":"get_post_field_conversion_hook"},{"location":"reference/arti/types/pydantic/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/pydantic/#basemodeladapter","text":"class BaseModelAdapter ( / , * args , ** kwargs ) View Source @pydantic_type_system . register_adapter class BaseModelAdapter ( TypeAdapter ) : artigraph = Struct system = BaseModel @staticmethod def _field_to_artigraph ( field : ModelField , * , hints : dict [ str, Any ] ) -> Type : subtype = python_type_system . to_artigraph ( field . outer_type_ , hints = hints ) return get_post_field_conversion_hook ( subtype )( subtype , name = field . name , required = ( True if isinstance ( field . required , _PydanticUndefinedType ) else field . required ), ) @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints ) for field in type_ . __fields__ . values () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : ( pydantic_type_system . to_system ( v , hints = hints ) if isinstance ( v , Struct ) else python_type_system . to_system ( v , hints = hints ) ) for k , v in type_ . fields . items () } } , )","title":"BaseModelAdapter"},{"location":"reference/arti/types/pydantic/#ancestors-in-mro","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/pydantic/#class-variables","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/pydantic/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/pydantic/#matches_artigraph","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/pydantic/#matches_system","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/pydantic/#to_artigraph","text":"def to_artigraph ( type_ : type [ pydantic . main . BaseModel ], * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : type [ BaseModel ] , * , hints : dict [ str, Any ] ) -> Type : return Struct ( name = type_ . __name__ , fields = { field . name : cls . _field_to_artigraph ( field , hints = hints ) for field in type_ . __fields__ . values () } , )","title":"to_artigraph"},{"location":"reference/arti/types/pydantic/#to_system","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> type [ pydantic . main . BaseModel ] View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> type [ BaseModel ] : assert isinstance ( type_ , Struct ) return type ( f \"{type_.name}\" , ( BaseModel ,), { \"__annotations__\" : { k : ( pydantic_type_system . to_system ( v , hints = hints ) if isinstance ( v , Struct ) else python_type_system . to_system ( v , hints = hints ) ) for k , v in type_ . fields . items () } } , )","title":"to_system"},{"location":"reference/arti/types/python/","text":"Module arti.types.python None None View Source import datetime from collections.abc import Mapping from functools import partial from itertools import chain from typing import Any , Literal , Optional , TypedDict , Union , get_args , get_origin , get_type_hints import arti.types from arti.internal.type_hints import ( NoneType , is_optional_hint , is_typeddict , is_union , lenient_issubclass , ) from arti.types import Type , TypeAdapter , TypeSystem , _ScalarClassTypeAdapter python_type_system = TypeSystem ( key = \"python\" ) _generate = partial ( _ScalarClassTypeAdapter . generate , type_system = python_type_system ) _generate ( artigraph = arti . types . Binary , system = bytes ) # NOTE: issubclass(bool, int) is True, so set higher priority _generate ( artigraph = arti . types . Boolean , system = bool , priority = int ( 1e9 )) _generate ( artigraph = arti . types . Date , system = datetime . date ) _generate ( artigraph = arti . types . Null , system = NoneType ) _generate ( artigraph = arti . types . String , system = str ) for _precision in ( 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Float { _precision } \" ), system = float , priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Int { _precision } \" ), system = int , priority = _precision , ) @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ): artigraph = arti . types . Timestamp system = datetime . datetime @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return cls . artigraph ( precision = \"microsecond\" ) class PyValueContainer ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system . to_system ( type_ . element , hints = hints ), # type: ignore ] @python_type_system . register_adapter class PyList ( PyValueContainer ): artigraph = arti . types . List system = list priority = 1 # NOTE: PyTuple only covers sequences (eg: tuple[int, ...]), not structure (eg: tuple[int, str]). @python_type_system . register_adapter class PyTuple ( PyValueContainer ): artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 and args [ 1 ] is ... return super () . to_artigraph ( origin [ args [ 0 ]], hints = hints ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : if super () . matches_system ( type_ , hints = hints ): args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : ret = super () . to_system ( type_ , hints = hints ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args [ 0 ], ... ] @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ): artigraph = arti . types . Set system = frozenset @python_type_system . register_adapter class PySet ( PyValueContainer ): artigraph = arti . types . Set system = set priority = 1 # Set above frozenset @python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals := [ sub for sub in items if not get_origin ( sub ) is Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: { non_literals } \" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: { type_ } \" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = python_type_system . to_artigraph ( py_type , hints = hints ), items = items , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )] @python_type_system . register_adapter class PyMap ( TypeAdapter ): artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = python_type_system . to_artigraph ( key , hints = hints ), value = python_type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system . to_system ( type_ . key , hints = hints ), python_type_system . to_system ( type_ . value , hints = hints ), ] # type: ignore @python_type_system . register_adapter class PyOptional ( TypeAdapter ): artigraph = arti . types . Type # Check against isinstance *and* .nullable system = Optional # Set very high priority to intercept other matching arti.types.Types/py Union in order to set .nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return super () . matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : # Optional is represented as a Union; strip out NoneType before dispatching type_ = Union [ tuple ( subtype for subtype in get_args ( type_ ) if subtype is not NoneType )] return python_type_system . to_artigraph ( type_ , hints = hints ) . copy ( update = { \"nullable\" : True }) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : return cls . system [ python_type_system . to_system ( type_ . copy ( update = { \"nullable\" : False }), hints = hints ) ] @python_type_system . register_adapter class PyStruct ( TypeAdapter ): artigraph = arti . types . Struct system = TypedDict # TODO: Support and inspect TypedDict's '__optional_keys__', '__required_keys__', '__total__' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : python_type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ) . items () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # NOTE: This check is probably a little shaky, particularly across python versions. Consider # using the typing_inspect package. return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( type_ . name , { field_name : python_type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () }, ) # type: ignore Variables python_type_system Classes PyDatetime class PyDatetime ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ) : artigraph = arti . types . Timestamp system = datetime . datetime @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return cls . artigraph ( precision = \"microsecond\" ) Ancestors (in MRO) arti.types._ScalarClassTypeAdapter arti.types.TypeAdapter Class variables artigraph key priority system Static methods generate def generate ( * , artigraph : type [ arti . types . Type ], system : Any , priority : int = 0 , type_system : 'TypeSystem' , name : Optional [ str ] = None ) -> type [ arti . types . TypeAdapter ] Generate a _ScalarClassTypeAdapter subclass for the scalar system type. View Source @classmethod def generate ( cls , * , artigraph : type [ Type ] , system : Any , priority : int = 0 , type_system : \"TypeSystem\" , name : Optional [ str ] = None , ) -> type [ TypeAdapter ] : \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \"{type_system.key}{artigraph.__name__}\" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority } , ) ) matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return cls . artigraph ( precision = \"microsecond\" ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : return cls . system PyFrozenset class PyFrozenset ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ) : artigraph = arti . types . Set system = frozenset Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ] PyList class PyList ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyList ( PyValueContainer ) : artigraph = arti . types . List system = list priority = 1 Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ] PyLiteral class PyLiteral ( / , * args , ** kwargs ) View Source @ python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if not get_origin ( sub ) is Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = python_type_system . to_artigraph ( py_type , hints = hints ), items = items , ) @ classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @ classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )] Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don 't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python' s Optional is also represented as a Union , but we handle that with the # high priority PyOptional . origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if not get_origin ( sub ) is Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = python_type_system . to_artigraph ( py_type , hints = hints ), items = items , ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple(type_.items) ] PyMap class PyMap ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyMap ( TypeAdapter ) : artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = python_type_system . to_artigraph ( key , hints = hints ), value = python_type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.key, hints=hints), python_type_system.to_system(type_.value, hints=hints), ] # type : ignore Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = python_type_system . to_artigraph ( key , hints = hints ), value = python_type_system . to_artigraph ( value , hints = hints ), ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.key, hints=hints), python_type_system.to_system(type_.value, hints=hints), ] # type : ignore PyOptional class PyOptional ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyOptional ( TypeAdapter ) : artigraph = arti . types . Type # Check against isinstance * and * . nullable system = Optional # Set very high priority to intercept other matching arti . types . Types / py Union in order to set . nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return python_type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : return cls . system [ python_type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ] Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return python_type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : return cls . system [ python_type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ] PySet class PySet ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PySet ( PyValueContainer ) : artigraph = arti . types . Set system = set priority = 1 # Set above frozenset Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ] PyStruct class PyStruct ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyStruct ( TypeAdapter ) : artigraph = arti . types . Struct system = TypedDict # TODO : Support and inspect TypedDict 's ' __optional_keys__ ', ' __required_keys__ ', ' __total__ ' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : python_type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( type_ . name , { field_name : python_type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , ) # type : ignore Ancestors (in MRO) arti.types.TypeAdapter Class variables artigraph key priority Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : python_type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( type_ . name , { field_name : python_type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , ) # type : ignore Methods system def system ( typename , fields = None , / , * , total = True , ** kwargs ) A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. Usage:: class Point2D ( TypedDict ): x : int y: int label: str a: Point2D = { 'x' : 1 , 'y' : 2 , 'label' : 'good' } # OK b: Point2D = { 'z' : 3 , 'label' : 'bad' } # Fails type check assert Point2D ( x = 1 , y = 2 , label = 'first' ) == dict ( x = 1 , y = 2 , label = 'first' ) The type info can be accessed via the Point2D. annotations dict, and the Point2D. required_keys and Point2D. optional_keys frozensets. TypedDict supports two additional equivalent forms:: Point2D = TypedDict('Point2D', x=int, y=int, label=str) Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str}) By default, all keys must be present in a TypedDict. It is possible to override this by specifying totality. Usage:: class point2D ( TypedDict , total = False ): x : int y: int This means that a point2D TypedDict can have any of the keys omitted.A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. The class syntax is only supported in Python 3.6+, while two other syntax forms work for Python 2.7 and 3.2+ View Source def TypedDict ( typename , fields = None , / , * , total = True , ** kwargs ) : \"\"\" A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys , where each key is associated with a value of a consistent type . This expectation is not checked at runtime but is only enforced by type checkers . Usage :: class Point2D ( TypedDict ) : x : int y : int label : str a : Point2D = { ' x ' : 1 , ' y ' : 2 , ' label ' : ' good ' } # OK b : Point2D = { ' z ' : 3 , ' label ' : ' bad ' } # Fails type check assert Point2D ( x = 1 , y = 2 , label = ' first ' ) == dict ( x = 1 , y = 2 , label = ' first ' ) The type info can be accessed via the Point2D . __annotations__ dict , and the Point2D . __required_keys__ and Point2D . __optional_keys__ frozensets . TypedDict supports two additional equivalent forms :: Point2D = TypedDict ( ' Point2D ' , x = int , y = int , label = str ) Point2D = TypedDict ( ' Point2D ' , { ' x ' : int , ' y ' : int , ' label ' : str } ) By default , all keys must be present in a TypedDict . It is possible to override this by specifying totality . Usage :: class point2D ( TypedDict , total = False ) : x : int y : int This means that a point2D TypedDict can have any of the keys omitted . A type checker is only expected to support a literal False or True as the value of the total argument . True is the default , and makes all items defined in the class body be required . The class syntax is only supported in Python 3 . 6 + , while two other syntax forms work for Python 2 . 7 and 3 . 2 + \"\"\" if fields is None : fields = kwargs elif kwargs : raise TypeError ( \" TypedDict takes either a dict or keyword arguments, \" \" but not both \" ) ns = { ' __annotations__ ' : dict ( fields ) } try : # Setting correct module is necessary to make typed dict classes pickleable . ns [ ' __module__ ' ] = sys . _getframe ( 1 ) . f_globals . get ( ' __name__ ' , ' __main__ ' ) except ( AttributeError , ValueError ) : pass return _TypedDictMeta ( typename , () , ns , total = total ) PyTuple class PyTuple ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyTuple ( PyValueContainer ) : artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 and args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : ret = super (). to_system ( type_ , hints = hints ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ] Ancestors (in MRO) arti.types.python.PyValueContainer arti.types.TypeAdapter Class variables artigraph key priority system Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 and args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : ret = super (). to_system ( type_ , hints = hints ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ] PyValueContainer class PyValueContainer ( / , * args , ** kwargs ) View Source class PyValueContainer ( TypeAdapter ) : @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ] Ancestors (in MRO) arti.types.TypeAdapter Descendants arti.types.python.PyList arti.types.python.PyTuple arti.types.python.PyFrozenset arti.types.python.PySet Class variables key priority Static methods matches_artigraph def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph ) matches_system def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) to_artigraph def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) to_system def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ]","title":"Python"},{"location":"reference/arti/types/python/#module-artitypespython","text":"None None View Source import datetime from collections.abc import Mapping from functools import partial from itertools import chain from typing import Any , Literal , Optional , TypedDict , Union , get_args , get_origin , get_type_hints import arti.types from arti.internal.type_hints import ( NoneType , is_optional_hint , is_typeddict , is_union , lenient_issubclass , ) from arti.types import Type , TypeAdapter , TypeSystem , _ScalarClassTypeAdapter python_type_system = TypeSystem ( key = \"python\" ) _generate = partial ( _ScalarClassTypeAdapter . generate , type_system = python_type_system ) _generate ( artigraph = arti . types . Binary , system = bytes ) # NOTE: issubclass(bool, int) is True, so set higher priority _generate ( artigraph = arti . types . Boolean , system = bool , priority = int ( 1e9 )) _generate ( artigraph = arti . types . Date , system = datetime . date ) _generate ( artigraph = arti . types . Null , system = NoneType ) _generate ( artigraph = arti . types . String , system = str ) for _precision in ( 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Float { _precision } \" ), system = float , priority = _precision , ) for _precision in ( 8 , 16 , 32 , 64 ): _generate ( artigraph = getattr ( arti . types , f \"Int { _precision } \" ), system = int , priority = _precision , ) @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ): artigraph = arti . types . Timestamp system = datetime . datetime @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return cls . artigraph ( precision = \"microsecond\" ) class PyValueContainer ( TypeAdapter ): @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system . to_system ( type_ . element , hints = hints ), # type: ignore ] @python_type_system . register_adapter class PyList ( PyValueContainer ): artigraph = arti . types . List system = list priority = 1 # NOTE: PyTuple only covers sequences (eg: tuple[int, ...]), not structure (eg: tuple[int, str]). @python_type_system . register_adapter class PyTuple ( PyValueContainer ): artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 and args [ 1 ] is ... return super () . to_artigraph ( origin [ args [ 0 ]], hints = hints ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : if super () . matches_system ( type_ , hints = hints ): args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : ret = super () . to_system ( type_ , hints = hints ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args [ 0 ], ... ] @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ): artigraph = arti . types . Set system = frozenset @python_type_system . register_adapter class PySet ( PyValueContainer ): artigraph = arti . types . Set system = set priority = 1 # Set above frozenset @python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals := [ sub for sub in items if not get_origin ( sub ) is Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: { non_literals } \" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: { type_ } \" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = python_type_system . to_artigraph ( py_type , hints = hints ), items = items , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )] @python_type_system . register_adapter class PyMap ( TypeAdapter ): artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = python_type_system . to_artigraph ( key , hints = hints ), value = python_type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system . to_system ( type_ . key , hints = hints ), python_type_system . to_system ( type_ . value , hints = hints ), ] # type: ignore @python_type_system . register_adapter class PyOptional ( TypeAdapter ): artigraph = arti . types . Type # Check against isinstance *and* .nullable system = Optional # Set very high priority to intercept other matching arti.types.Types/py Union in order to set .nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> bool : return super () . matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : # Optional is represented as a Union; strip out NoneType before dispatching type_ = Union [ tuple ( subtype for subtype in get_args ( type_ ) if subtype is not NoneType )] return python_type_system . to_artigraph ( type_ , hints = hints ) . copy ( update = { \"nullable\" : True }) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : return cls . system [ python_type_system . to_system ( type_ . copy ( update = { \"nullable\" : False }), hints = hints ) ] @python_type_system . register_adapter class PyStruct ( TypeAdapter ): artigraph = arti . types . Struct system = TypedDict # TODO: Support and inspect TypedDict's '__optional_keys__', '__required_keys__', '__total__' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : python_type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ) . items () }, ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # NOTE: This check is probably a little shaky, particularly across python versions. Consider # using the typing_inspect package. return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( type_ . name , { field_name : python_type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () }, ) # type: ignore","title":"Module arti.types.python"},{"location":"reference/arti/types/python/#variables","text":"python_type_system","title":"Variables"},{"location":"reference/arti/types/python/#classes","text":"","title":"Classes"},{"location":"reference/arti/types/python/#pydatetime","text":"class PyDatetime ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyDatetime ( _ScalarClassTypeAdapter ) : artigraph = arti . types . Timestamp system = datetime . datetime @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return cls . artigraph ( precision = \"microsecond\" )","title":"PyDatetime"},{"location":"reference/arti/types/python/#ancestors-in-mro","text":"arti.types._ScalarClassTypeAdapter arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#generate","text":"def generate ( * , artigraph : type [ arti . types . Type ], system : Any , priority : int = 0 , type_system : 'TypeSystem' , name : Optional [ str ] = None ) -> type [ arti . types . TypeAdapter ] Generate a _ScalarClassTypeAdapter subclass for the scalar system type. View Source @classmethod def generate ( cls , * , artigraph : type [ Type ] , system : Any , priority : int = 0 , type_system : \"TypeSystem\" , name : Optional [ str ] = None , ) -> type [ TypeAdapter ] : \"\"\"Generate a _ScalarClassTypeAdapter subclass for the scalar system type.\"\"\" name = name or f \"{type_system.key}{artigraph.__name__}\" return type_system . register_adapter ( type ( name , ( cls ,), { \"artigraph\" : artigraph , \"system\" : system , \"priority\" : priority } , ) )","title":"generate"},{"location":"reference/arti/types/python/#matches_artigraph","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( type_ , cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return cls . artigraph ( precision = \"microsecond\" )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : return cls . system","title":"to_system"},{"location":"reference/arti/types/python/#pyfrozenset","text":"class PyFrozenset ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyFrozenset ( PyValueContainer ) : artigraph = arti . types . Set system = frozenset","title":"PyFrozenset"},{"location":"reference/arti/types/python/#ancestors-in-mro_1","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_1","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_1","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_1","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_1","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_1","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ]","title":"to_system"},{"location":"reference/arti/types/python/#pylist","text":"class PyList ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyList ( PyValueContainer ) : artigraph = arti . types . List system = list priority = 1","title":"PyList"},{"location":"reference/arti/types/python/#ancestors-in-mro_2","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_2","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_2","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_2","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_2","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_2","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ]","title":"to_system"},{"location":"reference/arti/types/python/#pyliteral","text":"class PyLiteral ( / , * args , ** kwargs ) View Source @ python_type_system . register_adapter class PyLiteral ( TypeAdapter ): artigraph = arti . types . Enum system = Literal @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if not get_origin ( sub ) is Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = python_type_system . to_artigraph ( py_type , hints = hints ), items = items , ) @ classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> bool : # We don't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python's Optional is also represented as a Union, but we handle that with the # high priority PyOptional. origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) ) @ classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str , Any ]) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple ( type_ . items )]","title":"PyLiteral"},{"location":"reference/arti/types/python/#ancestors-in-mro_3","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_3","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_3","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_3","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # We don 't (currently) support arbitrary Unions, but can map Union[Literal[1], Literal[2]] # to an Enum. Python' s Optional is also represented as a Union , but we handle that with the # high priority PyOptional . origin , items = get_origin ( type_ ), get_args ( type_ ) return origin is Literal or ( is_union ( origin ) and all ( get_origin ( sub ) is Literal for sub in items ) )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_3","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @ classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str , Any ]) -> Type : origin , items = get_origin ( type_ ), get_args ( type_ ) if is_union ( origin ): assert not is_optional_hint ( type_ ) # Should be handled by PyOptional # We only support Enums currently, so all subtypes must be Literal if non_literals : = [ sub for sub in items if not get_origin ( sub ) is Literal ]: raise NotImplementedError ( f \"Only Union[Literal[...], ...] (enums) are currently supported, got invalid subtypes: {non_literals}\" ) # Flatten Union[Literal[1], Literal[1,2,3]] origin , items = Literal , tuple ( chain . from_iterable ( get_args ( sub ) for sub in items )) assert origin is Literal assert isinstance ( items , tuple ) if len ( items ) == 0 : raise NotImplementedError ( f \"Invalid Literal with no values: {type_}\" ) py_type , * other_types = ( type ( v ) for v in items ) if not all ( t is py_type for t in other_types ): raise ValueError ( \"All Literals must be the same type, got: {(py_type, *other_types)}\" ) return cls . artigraph ( type = python_type_system . to_artigraph ( py_type , hints = hints ), items = items , )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_3","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ tuple(type_.items) ]","title":"to_system"},{"location":"reference/arti/types/python/#pymap","text":"class PyMap ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyMap ( TypeAdapter ) : artigraph = arti . types . Map system = dict @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = python_type_system . to_artigraph ( key , hints = hints ), value = python_type_system . to_artigraph ( value , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping )) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.key, hints=hints), python_type_system.to_system(type_.value, hints=hints), ] # type : ignore","title":"PyMap"},{"location":"reference/arti/types/python/#ancestors-in-mro_4","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_4","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_4","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_4","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), ( cls . system , Mapping ))","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_4","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : key , value = get_args ( type_ ) return cls . artigraph ( key = python_type_system . to_artigraph ( key , hints = hints ), value = python_type_system . to_artigraph ( value , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_4","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.key, hints=hints), python_type_system.to_system(type_.value, hints=hints), ] # type : ignore","title":"to_system"},{"location":"reference/arti/types/python/#pyoptional","text":"class PyOptional ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyOptional ( TypeAdapter ) : artigraph = arti . types . Type # Check against isinstance * and * . nullable system = Optional # Set very high priority to intercept other matching arti . types . Types / py Union in order to set . nullable priority = int ( 1e9 ) @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return python_type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : return cls . system [ python_type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ]","title":"PyOptional"},{"location":"reference/arti/types/python/#ancestors-in-mro_5","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_5","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_5","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return super (). matches_artigraph ( type_ , hints = hints ) and type_ . nullable","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_5","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return is_optional_hint ( type_ )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_5","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : # Optional is represented as a Union ; strip out NoneType before dispatching type_ = Union [ tuple(subtype for subtype in get_args(type_) if subtype is not NoneType) ] return python_type_system . to_artigraph ( type_ , hints = hints ). copy ( update = { \"nullable\" : True } )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_5","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : return cls . system [ python_type_system.to_system(type_.copy(update={\"nullable\": False}), hints=hints) ]","title":"to_system"},{"location":"reference/arti/types/python/#pyset","text":"class PySet ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PySet ( PyValueContainer ) : artigraph = arti . types . Set system = set priority = 1 # Set above frozenset","title":"PySet"},{"location":"reference/arti/types/python/#ancestors-in-mro_6","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_6","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_6","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_6","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_6","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_6","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ]","title":"to_system"},{"location":"reference/arti/types/python/#pystruct","text":"class PyStruct ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyStruct ( TypeAdapter ) : artigraph = arti . types . Struct system = TypedDict # TODO : Support and inspect TypedDict 's ' __optional_keys__ ', ' __required_keys__ ', ' __total__ ' @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : python_type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( type_ . name , { field_name : python_type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , ) # type : ignore","title":"PyStruct"},{"location":"reference/arti/types/python/#ancestors-in-mro_7","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_7","text":"artigraph key priority","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_7","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_7","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : # NOTE : This check is probably a little shaky , particularly across python versions . Consider # using the typing_inspect package . return is_typeddict ( type_ )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_7","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : return arti . types . Struct ( name = type_ . __name__ , fields = { field_name : python_type_system . to_artigraph ( field_type , hints = hints ) for field_name , field_type in get_type_hints ( type_ ). items () } , )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_7","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return TypedDict ( type_ . name , { field_name : python_type_system . to_system ( field_type , hints = hints ) for field_name , field_type in type_ . fields . items () } , ) # type : ignore","title":"to_system"},{"location":"reference/arti/types/python/#methods","text":"","title":"Methods"},{"location":"reference/arti/types/python/#system","text":"def system ( typename , fields = None , / , * , total = True , ** kwargs ) A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. Usage:: class Point2D ( TypedDict ): x : int y: int label: str a: Point2D = { 'x' : 1 , 'y' : 2 , 'label' : 'good' } # OK b: Point2D = { 'z' : 3 , 'label' : 'bad' } # Fails type check assert Point2D ( x = 1 , y = 2 , label = 'first' ) == dict ( x = 1 , y = 2 , label = 'first' ) The type info can be accessed via the Point2D. annotations dict, and the Point2D. required_keys and Point2D. optional_keys frozensets. TypedDict supports two additional equivalent forms:: Point2D = TypedDict('Point2D', x=int, y=int, label=str) Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str}) By default, all keys must be present in a TypedDict. It is possible to override this by specifying totality. Usage:: class point2D ( TypedDict , total = False ): x : int y: int This means that a point2D TypedDict can have any of the keys omitted.A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. The class syntax is only supported in Python 3.6+, while two other syntax forms work for Python 2.7 and 3.2+ View Source def TypedDict ( typename , fields = None , / , * , total = True , ** kwargs ) : \"\"\" A simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances to have a certain set of keys , where each key is associated with a value of a consistent type . This expectation is not checked at runtime but is only enforced by type checkers . Usage :: class Point2D ( TypedDict ) : x : int y : int label : str a : Point2D = { ' x ' : 1 , ' y ' : 2 , ' label ' : ' good ' } # OK b : Point2D = { ' z ' : 3 , ' label ' : ' bad ' } # Fails type check assert Point2D ( x = 1 , y = 2 , label = ' first ' ) == dict ( x = 1 , y = 2 , label = ' first ' ) The type info can be accessed via the Point2D . __annotations__ dict , and the Point2D . __required_keys__ and Point2D . __optional_keys__ frozensets . TypedDict supports two additional equivalent forms :: Point2D = TypedDict ( ' Point2D ' , x = int , y = int , label = str ) Point2D = TypedDict ( ' Point2D ' , { ' x ' : int , ' y ' : int , ' label ' : str } ) By default , all keys must be present in a TypedDict . It is possible to override this by specifying totality . Usage :: class point2D ( TypedDict , total = False ) : x : int y : int This means that a point2D TypedDict can have any of the keys omitted . A type checker is only expected to support a literal False or True as the value of the total argument . True is the default , and makes all items defined in the class body be required . The class syntax is only supported in Python 3 . 6 + , while two other syntax forms work for Python 2 . 7 and 3 . 2 + \"\"\" if fields is None : fields = kwargs elif kwargs : raise TypeError ( \" TypedDict takes either a dict or keyword arguments, \" \" but not both \" ) ns = { ' __annotations__ ' : dict ( fields ) } try : # Setting correct module is necessary to make typed dict classes pickleable . ns [ ' __module__ ' ] = sys . _getframe ( 1 ) . f_globals . get ( ' __name__ ' , ' __main__ ' ) except ( AttributeError , ValueError ) : pass return _TypedDictMeta ( typename , () , ns , total = total )","title":"system"},{"location":"reference/arti/types/python/#pytuple","text":"class PyTuple ( / , * args , ** kwargs ) View Source @python_type_system . register_adapter class PyTuple ( PyValueContainer ) : artigraph = arti . types . List system = tuple @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 and args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : ret = super (). to_system ( type_ , hints = hints ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ]","title":"PyTuple"},{"location":"reference/arti/types/python/#ancestors-in-mro_8","text":"arti.types.python.PyValueContainer arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#class-variables_8","text":"artigraph key priority system","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_8","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_8","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : if super (). matches_system ( type_ , hints = hints ) : args = get_args ( type_ ) if len ( args ) == 2 and args [ 1 ] is ... : return True return False","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_8","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : origin , args = get_origin ( type_ ), get_args ( type_ ) assert origin is not None assert len ( args ) == 2 and args [ 1 ] is ... return super (). to_artigraph ( origin [ args[0 ] ] , hints = hints )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_8","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : ret = super (). to_system ( type_ , hints = hints ) origin , args = get_origin ( ret ), get_args ( ret ) assert origin is not None assert len ( args ) == 1 return origin [ args[0 ] , ... ]","title":"to_system"},{"location":"reference/arti/types/python/#pyvaluecontainer","text":"class PyValueContainer ( / , * args , ** kwargs ) View Source class PyValueContainer ( TypeAdapter ) : @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), ) @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system ) @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ]","title":"PyValueContainer"},{"location":"reference/arti/types/python/#ancestors-in-mro_9","text":"arti.types.TypeAdapter","title":"Ancestors (in MRO)"},{"location":"reference/arti/types/python/#descendants","text":"arti.types.python.PyList arti.types.python.PyTuple arti.types.python.PyFrozenset arti.types.python.PySet","title":"Descendants"},{"location":"reference/arti/types/python/#class-variables_9","text":"key priority","title":"Class variables"},{"location":"reference/arti/types/python/#static-methods_9","text":"","title":"Static methods"},{"location":"reference/arti/types/python/#matches_artigraph_9","text":"def matches_artigraph ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_artigraph ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> bool : return isinstance ( type_ , cls . artigraph )","title":"matches_artigraph"},{"location":"reference/arti/types/python/#matches_system_9","text":"def matches_system ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> bool View Source @classmethod def matches_system ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> bool : return lenient_issubclass ( get_origin ( type_ ), cls . system )","title":"matches_system"},{"location":"reference/arti/types/python/#to_artigraph_9","text":"def to_artigraph ( type_ : Any , * , hints : dict [ str , typing . Any ] ) -> arti . types . Type View Source @classmethod def to_artigraph ( cls , type_ : Any , * , hints : dict [ str, Any ] ) -> Type : ( element ,) = get_args ( type_ ) return cls . artigraph ( element = python_type_system . to_artigraph ( element , hints = hints ), )","title":"to_artigraph"},{"location":"reference/arti/types/python/#to_system_9","text":"def to_system ( type_ : arti . types . Type , * , hints : dict [ str , typing . Any ] ) -> Any View Source @classmethod def to_system ( cls , type_ : Type , * , hints : dict [ str, Any ] ) -> Any : assert isinstance ( type_ , cls . artigraph ) return cls . system [ python_type_system.to_system(type_.element, hints=hints), # type: ignore ]","title":"to_system"},{"location":"reference/arti/views/","text":"Module arti.views None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import Annotated , Any , ClassVar , Optional , get_args , get_origin from arti.internal.models import Model from arti.internal.type_hints import lenient_issubclass from arti.internal.utils import import_submodules , register from arti.types import Type , TypeSystem class View ( Model ): \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : \"ClassVar[dict[type, type[View]]]\" = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type. Higher is better. python_type : ClassVar [ type ] type_system : ClassVar [ TypeSystem ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ]: import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type [ View ]] = [ hint for hint in hints if lenient_issubclass ( hint , View )] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \" { annotation } cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {}) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ): raise ValueError ( f \" { python_type } cannot be used to represent { arti } \" ) Sub-modules arti.views.python Classes View class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : \"ClassVar[dict[type, type[View]]]\" = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ type ] type_system : ClassVar [ TypeSystem ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) Ancestors (in MRO) arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.views.python.PythonBuiltin Class variables Config priority Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Index"},{"location":"reference/arti/views/#module-artiviews","text":"None None View Source __path__ = __import__ ( \"pkgutil\" ) . extend_path ( __path__ , __name__ ) from typing import Annotated , Any , ClassVar , Optional , get_args , get_origin from arti.internal.models import Model from arti.internal.type_hints import lenient_issubclass from arti.internal.utils import import_submodules , register from arti.types import Type , TypeSystem class View ( Model ): \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : \"ClassVar[dict[type, type[View]]]\" = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type. Higher is better. python_type : ClassVar [ type ] type_system : ClassVar [ TypeSystem ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super () . __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ]: import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type [ View ]] = [ hint for hint in hints if lenient_issubclass ( hint , View )] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \" { annotation } cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {}) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ): raise ValueError ( f \" { python_type } cannot be used to represent { arti } \" )","title":"Module arti.views"},{"location":"reference/arti/views/#sub-modules","text":"arti.views.python","title":"Sub-modules"},{"location":"reference/arti/views/#classes","text":"","title":"Classes"},{"location":"reference/arti/views/#view","text":"class View ( __pydantic_self__ , ** data : Any ) View Source class View ( Model ) : \"\"\"View represents the in-memory representation of the artifact. Examples include pandas.DataFrame, dask.DataFrame, a BigQuery table. \"\"\" _abstract_ = True _by_python_type_ : \"ClassVar[dict[type, type[View]]]\" = {} priority : ClassVar [ int ] = 0 # Set priority of this view for its python_type . Higher is better . python_type : ClassVar [ type ] type_system : ClassVar [ TypeSystem ] @classmethod def __init_subclass__ ( cls , ** kwargs : Any ) -> None : super (). __init_subclass__ ( ** kwargs ) if not cls . _abstract_ : register ( cls . _by_python_type_ , cls . python_type , cls , lambda x : x . priority ) @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"View"},{"location":"reference/arti/views/#ancestors-in-mro","text":"arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/#descendants","text":"arti.views.python.PythonBuiltin","title":"Descendants"},{"location":"reference/arti/views/#class-variables","text":"Config priority","title":"Class variables"},{"location":"reference/arti/views/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/views/#check_type_similarity","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/#get_class_for","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/#methods","text":"","title":"Methods"},{"location":"reference/arti/views/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/","text":"Module arti.views.python None None View Source from datetime import date , datetime from arti.internal.type_hints import NoneType from arti.types.python import python_type_system from arti.views import View class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system class Date ( PythonBuiltin ): python_type = date class Datetime ( PythonBuiltin ): python_type = datetime class Dict ( PythonBuiltin ): python_type = dict class Float ( PythonBuiltin ): python_type = float class Int ( PythonBuiltin ): python_type = int class List ( PythonBuiltin ): python_type = list class Null ( PythonBuiltin ): python_type = NoneType class Str ( PythonBuiltin ): python_type = str Classes Date class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( PythonBuiltin ): python_type = date Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Datetime class Datetime ( __pydantic_self__ , ** data : Any ) View Source class Datetime ( PythonBuiltin ): python_type = datetime Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Dict class Dict ( __pydantic_self__ , ** data : Any ) View Source class Dict ( PythonBuiltin ): python_type = dict Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Float class Float ( __pydantic_self__ , ** data : Any ) View Source class Float ( PythonBuiltin ): python_type = float Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Int class Int ( __pydantic_self__ , ** data : Any ) View Source class Int ( PythonBuiltin ): python_type = int Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . List class List ( __pydantic_self__ , ** data : Any ) View Source class List ( PythonBuiltin ): python_type = list Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Null class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( PythonBuiltin ): python_type = NoneType Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . PythonBuiltin class PythonBuiltin ( __pydantic_self__ , ** data : Any ) View Source class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system Ancestors (in MRO) arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Descendants arti.views.python.Date arti.views.python.Datetime arti.views.python.Dict arti.views.python.Float arti.views.python.Int arti.views.python.List arti.views.python.Null arti.views.python.Str Class variables Config priority type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Str class Str ( __pydantic_self__ , ** data : Any ) View Source class Str ( PythonBuiltin ): python_type = str Ancestors (in MRO) arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation Class variables Config priority python_type type_system Static methods check_type_similarity def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" ) construct def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' get_class_for def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fingerprint Methods copy def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Python"},{"location":"reference/arti/views/python/#module-artiviewspython","text":"None None View Source from datetime import date , datetime from arti.internal.type_hints import NoneType from arti.types.python import python_type_system from arti.views import View class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system class Date ( PythonBuiltin ): python_type = date class Datetime ( PythonBuiltin ): python_type = datetime class Dict ( PythonBuiltin ): python_type = dict class Float ( PythonBuiltin ): python_type = float class Int ( PythonBuiltin ): python_type = int class List ( PythonBuiltin ): python_type = list class Null ( PythonBuiltin ): python_type = NoneType class Str ( PythonBuiltin ): python_type = str","title":"Module arti.views.python"},{"location":"reference/arti/views/python/#classes","text":"","title":"Classes"},{"location":"reference/arti/views/python/#date","text":"class Date ( __pydantic_self__ , ** data : Any ) View Source class Date ( PythonBuiltin ): python_type = date","title":"Date"},{"location":"reference/arti/views/python/#ancestors-in-mro","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#datetime","text":"class Datetime ( __pydantic_self__ , ** data : Any ) View Source class Datetime ( PythonBuiltin ): python_type = datetime","title":"Datetime"},{"location":"reference/arti/views/python/#ancestors-in-mro_1","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_1","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_1","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_1","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_1","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_1","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_1","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_1","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#dict_2","text":"class Dict ( __pydantic_self__ , ** data : Any ) View Source class Dict ( PythonBuiltin ): python_type = dict","title":"Dict"},{"location":"reference/arti/views/python/#ancestors-in-mro_2","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_2","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_2","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_2","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_2","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_2","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_2","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_2","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_3","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#float","text":"class Float ( __pydantic_self__ , ** data : Any ) View Source class Float ( PythonBuiltin ): python_type = float","title":"Float"},{"location":"reference/arti/views/python/#ancestors-in-mro_3","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_3","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_3","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_3","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_3","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_3","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_3","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_3","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_3","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_3","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_3","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_3","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_3","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_3","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_3","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_3","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_4","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_3","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#int","text":"class Int ( __pydantic_self__ , ** data : Any ) View Source class Int ( PythonBuiltin ): python_type = int","title":"Int"},{"location":"reference/arti/views/python/#ancestors-in-mro_4","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_4","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_4","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_4","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_4","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_4","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_4","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_4","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_4","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_4","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_4","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_4","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_4","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_4","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_4","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_4","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_5","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_4","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#list","text":"class List ( __pydantic_self__ , ** data : Any ) View Source class List ( PythonBuiltin ): python_type = list","title":"List"},{"location":"reference/arti/views/python/#ancestors-in-mro_5","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_5","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_5","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_5","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_5","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_5","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_5","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_5","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_5","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_5","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_5","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_5","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_5","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_5","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_5","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_5","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_6","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_5","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#null","text":"class Null ( __pydantic_self__ , ** data : Any ) View Source class Null ( PythonBuiltin ): python_type = NoneType","title":"Null"},{"location":"reference/arti/views/python/#ancestors-in-mro_6","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_6","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_6","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_6","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_6","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_6","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_6","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_6","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_6","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_6","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_6","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_6","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_6","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_6","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_6","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_6","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_7","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_6","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#pythonbuiltin","text":"class PythonBuiltin ( __pydantic_self__ , ** data : Any ) View Source class PythonBuiltin ( View ): _abstract_ = True type_system = python_type_system","title":"PythonBuiltin"},{"location":"reference/arti/views/python/#ancestors-in-mro_7","text":"arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#descendants","text":"arti.views.python.Date arti.views.python.Datetime arti.views.python.Dict arti.views.python.Float arti.views.python.Int arti.views.python.List arti.views.python.Null arti.views.python.Str","title":"Descendants"},{"location":"reference/arti/views/python/#class-variables_7","text":"Config priority type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_7","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_7","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_7","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_7","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_7","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_7","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_7","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_7","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_7","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_7","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_7","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_7","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_7","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_7","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_7","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_8","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_7","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/arti/views/python/#str","text":"class Str ( __pydantic_self__ , ** data : Any ) View Source class Str ( PythonBuiltin ): python_type = str","title":"Str"},{"location":"reference/arti/views/python/#ancestors-in-mro_8","text":"arti.views.python.PythonBuiltin arti.views.View arti.internal.models.Model pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/arti/views/python/#class-variables_8","text":"Config priority python_type type_system","title":"Class variables"},{"location":"reference/arti/views/python/#static-methods_8","text":"","title":"Static methods"},{"location":"reference/arti/views/python/#check_type_similarity_8","text":"def check_type_similarity ( * , arti : arti . types . Type , python_type : type ) -> None View Source @classmethod def check_type_similarity ( cls , * , arti : Type , python_type : type ) -> None : system_type = cls . type_system . to_system ( arti , hints = {} ) if not ( lenient_issubclass ( system_type , python_type ) or lenient_issubclass ( type ( system_type ), python_type ) ) : raise ValueError ( f \"{python_type} cannot be used to represent {arti}\" )","title":"check_type_similarity"},{"location":"reference/arti/views/python/#construct_8","text":"def construct ( _fields_set : Optional [ ForwardRef ( 'SetStr' )] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/arti/views/python/#from_orm_8","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/arti/views/python/#get_class_for_8","text":"def get_class_for ( annotation : Any , * , validation_type : Optional [ arti . types . Type ] = None ) -> type [ 'View' ] View Source @classmethod def get_class_for ( cls , annotation : Any , * , validation_type : Optional [ Type ] = None ) -> type [ \"View\" ] : import_submodules ( __path__ , __name__ ) view = None origin , args = get_origin ( annotation ), get_args ( annotation ) if origin is Annotated : annotation , * hints = args views : list [ type[View ] ] = [ hint for hint in hints if lenient_issubclass(hint, View) ] if len ( views ) == 0 : return cls . get_class_for ( annotation , validation_type = validation_type ) if len ( views ) == 1 : view = views [ 0 ] else : raise ValueError ( \"multiple Views set\" ) if view is None : if origin is None : origin = annotation if origin not in cls . _by_python_type_ : raise ValueError ( f \"{annotation} cannot be matched to a View, try setting one explicitly (eg: `Annotated[pd.DataFrame, MyArtifact, PandasDataFrame]`)\" ) view = cls . _by_python_type_ [ origin ] if validation_type is not None : view . check_type_similarity ( arti = validation_type , python_type = annotation ) return view","title":"get_class_for"},{"location":"reference/arti/views/python/#parse_file_8","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/arti/views/python/#parse_obj_8","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/arti/views/python/#parse_raw_8","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/arti/views/python/#schema_8","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/arti/views/python/#schema_json_8","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/arti/views/python/#update_forward_refs_8","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/arti/views/python/#validate_8","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/arti/views/python/#instance-variables_8","text":"fingerprint","title":"Instance variables"},{"location":"reference/arti/views/python/#methods_8","text":"","title":"Methods"},{"location":"reference/arti/views/python/#copy_8","text":"def copy ( self : ~ _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> ~ _Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance View Source def copy ( self : _Model , * , deep : bool = False , validate : bool = True , ** kwargs : Any ) -> _Model : copy = super (). copy ( deep = deep , ** kwargs ) if validate : # NOTE: We set exclude_unset=False so that all existing defaulted fields are reused (as # is normal `.copy` behavior). # # To reduce `repr` noise, we'll reset .__fields_set__ to those of the pre-validation copy # (which includes those originally set + updated). fields_set = copy . __fields_set__ copy = copy . validate ( dict ( copy . _iter ( to_dict = False , by_alias = False , exclude_unset = False )) ) # Use object.__setattr__ to bypass frozen model assignment errors object . __setattr__ ( copy , \"__fields_set__\" , set ( fields_set )) # Copy over the private attributes, which are missing after validation (since we're only # passing the fields). for name in self . __private_attributes__ : if ( value := getattr ( self , name , Undefined )) is not Undefined : if deep : value = deepcopy ( value ) object . __setattr__ ( copy , name , value ) return copy","title":"copy"},{"location":"reference/arti/views/python/#dict_9","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/arti/views/python/#json_8","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Optional [ Callable [[ Any ], Any ]] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"}]}